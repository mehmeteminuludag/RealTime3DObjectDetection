{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f44ce32d",
      "metadata": {
        "id": "f44ce32d"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import json\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "HFHhIGESGWmk",
      "metadata": {
        "id": "HFHhIGESGWmk"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a895497",
      "metadata": {
        "id": "5a895497"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path,transform=None):\n",
        "        classes = [\n",
        "  \"person\",\n",
        "  \"rider\",\n",
        "  \"car\",\n",
        "  \"truck\",\n",
        "  \"bus\",\n",
        "  \"train\",\n",
        "  \"motor\",\n",
        "  \"bike\",\n",
        "  \"traffic light\",\n",
        "  \"traffic sign\"\n",
        "]\n",
        "        self.transform = transform\n",
        "        self.data=[]\n",
        "        for file_name in sorted(os.listdir(data_path)):\n",
        "            img_path = data_path+\"/\"+file_name\n",
        "            label_pth = img_path.replace(\"images\",\"labels\").replace(\".jpg\",\".json\")\n",
        "            labels=[]\n",
        "            with open(label_pth, \"r\") as label_file:\n",
        "                label = json.load(label_file)\n",
        "                objects = label[\"frames\"][0][\"objects\"]\n",
        "                for obj in objects:\n",
        "                    if \"box2d\" in obj:\n",
        "                        category = obj[\"category\"]\n",
        "                        category_num = classes.index(category)\n",
        "                        box = obj[\"box2d\"]\n",
        "                        x1, y1 = box[\"x1\"], box[\"y1\"]\n",
        "                        x2, y2 = box[\"x2\"], box[\"y2\"]\n",
        "                        labels.append([category_num,x1,y1,x2,y2])\n",
        "            self.data.append([img_path,labels])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, labels = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "class Transform(Dataset):\n",
        "    def __init__(self, base_dataset, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (image_1, image_2), labels = self.base_dataset[idx]  # (image_1, image_2) ve labels al\n",
        "        if self.transform:\n",
        "            image_1 = self.transform(image_1)  # İlk kareye transform uygula\n",
        "            image_2 = self.transform(image_2)  # İkinci kareye transform uygula\n",
        "        return (image_1, image_2), labels\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a772ca3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.classes = [\n",
        "            \"person\", \"rider\", \"car\", \"truck\", \"bus\",\n",
        "            \"train\", \"motor\", \"bike\", \"traffic light\", \"traffic sign\"\n",
        "        ]\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        \n",
        "        # Get sorted list of image files\n",
        "        file_names = sorted(os.listdir(data_path))\n",
        "        \n",
        "        # Create pairs of consecutive frames\n",
        "        for i in range(len(file_names) - 1):  # -1 to ensure we have pairs\n",
        "            img_path_1 = os.path.join(data_path, file_names[i])\n",
        "            img_path_2 = os.path.join(data_path, file_names[i + 1])\n",
        "            \n",
        "            # Load labels for both frames\n",
        "            label_path_1 = img_path_1.replace(\"images\", \"labels\").replace(\".jpg\", \".json\")\n",
        "            label_path_2 = img_path_2.replace(\"images\", \"labels\").replace(\".jpg\", \".json\")\n",
        "            \n",
        "            labels_1 = []\n",
        "            labels_2 = []\n",
        "            \n",
        "            # Load labels for first frame\n",
        "            with open(label_path_1, \"r\") as label_file:\n",
        "                label = json.load(label_file)\n",
        "                objects = label[\"frames\"][0][\"objects\"]\n",
        "                for obj in objects:\n",
        "                    if \"box2d\" in obj:\n",
        "                        category = obj[\"category\"]\n",
        "                        category_num = self.classes.index(category)\n",
        "                        box = obj[\"box2d\"]\n",
        "                        x1, y1 = box[\"x1\"], box[\"y1\"]\n",
        "                        x2, y2 = box[\"x2\"], box[\"y2\"]\n",
        "                        labels_1.append([category_num, x1, y1, x2, y2])\n",
        "            \n",
        "            # Load labels for second frame\n",
        "            with open(label_path_2, \"r\") as label_file:\n",
        "                label = json.load(label_file)\n",
        "                objects = label[\"frames\"][0][\"objects\"]\n",
        "                for obj in objects:\n",
        "                    if \"box2d\" in obj:\n",
        "                        category = obj[\"category\"]\n",
        "                        category_num = self.classes.index(category)\n",
        "                        box = obj[\"box2d\"]\n",
        "                        x1, y1 = box[\"x1\"], box[\"y1\"]\n",
        "                        x2, y2 = box[\"x2\"], box[\"y2\"]\n",
        "                        labels_2.append([category_num, x1, y1, x2, y2])\n",
        "            \n",
        "            self.data.append([(img_path_1, img_path_2), (labels_1, labels_2)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (img_path_1, img_path_2), (labels_1, labels_2) = self.data[idx]\n",
        "        image_1 = Image.open(img_path_1).convert(\"RGB\")\n",
        "        image_2 = Image.open(img_path_2).convert(\"RGB\")\n",
        "        \n",
        "        if self.transform:\n",
        "            image_1 = self.transform(image_1)\n",
        "            image_2 = self.transform(image_2)\n",
        "        \n",
        "        return (image_1, image_2), (labels_1, labels_2)\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Separate images and labels for both frames\n",
        "    images_1 = [item[0][0] for item in batch]  # First frame\n",
        "    images_2 = [item[0][1] for item in batch]  # Second frame\n",
        "    labels_1 = [item[1][0] for item in batch]  # Labels for first frame\n",
        "    labels_2 = [item[1][1] for item in batch]  # Labels for second frame\n",
        "\n",
        "    # Stack images to create [B, T, C, H, W]\n",
        "    images_1 = torch.stack(images_1, dim=0)  # [B, C, H, W]\n",
        "    images_2 = torch.stack(images_2, dim=0)  # [B, C, H, W]\n",
        "    images = torch.stack([images_1, images_2], dim=1)  # [B, T=2, C, H, W]\n",
        "\n",
        "    return images, (labels_1, labels_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f04dbc5a",
      "metadata": {
        "id": "f04dbc5a"
      },
      "outputs": [],
      "source": [
        "#data_path = \"/content/drive/MyDrive/BDD100k/images\" #colab\n",
        "data_path = \"C:/Users/Mehmet/Desktop/BDD100k/images\" #lcoal\n",
        "train_path = data_path+\"/train\"\n",
        "validation_path = data_path+\"/val\"\n",
        "test_path = data_path+\"/test\"\n",
        "\n",
        "image_size=300\n",
        "batch_size=1\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#train_dataset = Transform(CustomDataset(train_path),transform)\n",
        "#test_dataset  = Transform(CustomDataset(test_path),transform)\n",
        "val_dataset   = Transform(CustomDataset(validation_path),transform)\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "be09f37a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        x_out = self.conv1(x_cat)\n",
        "        attention_map = self.sigmoid(x_out)\n",
        "        return x * attention_map\n",
        "\n",
        "class EncoderBackBone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderBackBone,self).__init__()\n",
        "        efficient = models.efficientnet_b3()\n",
        "        self.features = efficient.features\n",
        "        self.SAttention=SpatialAttention()\n",
        "    def forward(self,x):         # x B,T,C,H,W\n",
        "        x_t0 = x[:, 0, :, :, :]  # İlk kare: [B, C, H, W]\n",
        "        x_t1 = x[:, 1, :, :, :]\n",
        "        outs = []\n",
        "        x_t0_out = x_t0\n",
        "        x_t1_out = x_t1\n",
        "        # t0 için özellikler\n",
        "        for i, block in enumerate(self.features):\n",
        "            x_t0_out = block(x_t0_out)\n",
        "            x_t1_out = block(x_t1_out)\n",
        "            if i > 2:  # C3’ten sonrası için Spatial Attention\n",
        "                x_t0_out = x_t0_out * self.SAttention(x_t0_out)\n",
        "                x_t1_out = x_t1_out * self.SAttention(x_t1_out)\n",
        "            if i in [3, 5, 7]:  #object detection tutarlılık için sadece t0 frame C3, C4, C5 ları ekleniyor \n",
        "                \n",
        "                out = F.interpolate(x_t0_out, size=256, mode='bilinear', align_corners=False)\n",
        "                outs.append(out)\n",
        "        x_out = torch.cat([x_t0_out, x_t1_out], dim=1)\n",
        "          \n",
        "        return [x_out, outs]   #torch.Size([32, 3072, 8, 8])      3,32,48,32,32\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                 stride, padding, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return self.swish(x)\n",
        "\n",
        "class BiFPNBlock(nn.Module):\n",
        "    def __init__(self, channels, epsilon=1e-4):\n",
        "        super(BiFPNBlock, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.channels = channels\n",
        "\n",
        "        # Convolution layers for each level\n",
        "        self.conv_p3 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p4 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p5 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p6 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p7 = DepthwiseSeparableConv(channels, channels)\n",
        "\n",
        "        # Weight parameters for feature fusion\n",
        "        self.w1 = nn.Parameter(torch.ones(2))\n",
        "        self.w2 = nn.Parameter(torch.ones(2))\n",
        "        self.w3 = nn.Parameter(torch.ones(2))\n",
        "        self.w4 = nn.Parameter(torch.ones(2))\n",
        "        self.w5 = nn.Parameter(torch.ones(3))\n",
        "        self.w6 = nn.Parameter(torch.ones(3))\n",
        "        self.w7 = nn.Parameter(torch.ones(3))\n",
        "        self.w8 = nn.Parameter(torch.ones(2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        P3, P4, P5, P6, P7 = inputs\n",
        "\n",
        "        \n",
        "        # Bottom-up pathway\n",
        "        w1 = F.relu(self.w1)\n",
        "        P6_td = (w1[0] * P6 + w1[1] * self.up_sampling(P7, P6.shape[-2:])) / (w1.sum() + self.epsilon)\n",
        "        P6_td = self.conv_p6(P6_td)\n",
        "\n",
        "        w2 = F.relu(self.w2)\n",
        "        P5_td = (w2[0] * P5 + w2[1] * self.up_sampling(P6_td, P5.shape[-2:])) / (w2.sum() + self.epsilon)\n",
        "        P5_td = self.conv_p5(P5_td)\n",
        "\n",
        "        w3 = F.relu(self.w3)\n",
        "        P4_td = (w3[0] * P4 + w3[1] * self.up_sampling(P5_td, P4.shape[-2:])) / (w3.sum() + self.epsilon)\n",
        "        P4_td = self.conv_p4(P4_td)\n",
        "\n",
        "        # Top-down pathway\n",
        "        w4 = F.relu(self.w4)\n",
        "        P3_out = (w4[0] * P3 + w4[1] * self.up_sampling(P4_td, P3.shape[-2:])) / (w4.sum() + self.epsilon)\n",
        "        P3_out = self.conv_p3(P3_out)\n",
        "\n",
        "        \n",
        "\n",
        "        w5 = F.relu(self.w5)\n",
        "        P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * self.down_sampling(P3_out, P4.shape[-2:])) / (w5.sum() + self.epsilon)\n",
        "        P4_out = self.conv_p4(P4_out)\n",
        "\n",
        "        w6 = F.relu(self.w6)\n",
        "        P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * self.down_sampling(P4_out, P5.shape[-2:])) / (w6.sum() + self.epsilon)\n",
        "        P5_out = self.conv_p5(P5_out)\n",
        "\n",
        "        w7 = F.relu(self.w7)\n",
        "        P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * self.down_sampling(P5_out, P6.shape[-2:])) / (w7.sum() + self.epsilon)\n",
        "        P6_out = self.conv_p6(P6_out)\n",
        "\n",
        "        w8 = F.relu(self.w8)\n",
        "        P7_out = (w8[0] * P7 + w8[1] * self.down_sampling(P6_out, P7.shape[-2:])) / (w8.sum() + self.epsilon)\n",
        "        P7_out = self.conv_p7(P7_out)\n",
        "\n",
        "        return [P3_out, P4_out, P5_out, P6_out, P7_out]\n",
        "\n",
        "    def up_sampling(self, x, target_size):\n",
        "        return F.interpolate(x, size=target_size, mode='nearest')\n",
        "\n",
        "    def down_sampling(self, x, target_size):\n",
        "        if x.shape[-2:] == target_size:\n",
        "            return x\n",
        "        stride = x.shape[-1] // target_size[-1]\n",
        "        kernel_size = stride\n",
        "        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels=256, num_blocks=3):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Input projection layers\n",
        "        self.input_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, out_channels, 1, bias=False)\n",
        "            for in_ch in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # Additional P6 and P7 layers\n",
        "        self.p6_conv = nn.Conv2d(in_channels_list[-1], out_channels, 3, stride=2, padding=1)\n",
        "        self.p7_conv = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN blocks\n",
        "        self.bifpn_blocks = nn.ModuleList([\n",
        "            BiFPNBlock(out_channels) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Project input features\n",
        "        features = []\n",
        "        for i, feat in enumerate(inputs):\n",
        "            features.append(self.input_convs[i](feat))\n",
        "\n",
        "        # Create P6 and P7\n",
        "        P6 = self.p6_conv(inputs[-1])\n",
        "        P7 = self.p7_conv(P6)\n",
        "\n",
        "        # Initial feature list\n",
        "        pyramid_features = features + [P6, P7]\n",
        "\n",
        "        # Apply BiFPN blocks\n",
        "        for block in self.bifpn_blocks:\n",
        "            pyramid_features = block(pyramid_features)\n",
        "\n",
        "        return pyramid_features\n",
        "\n",
        "\n",
        "  \n",
        "class BiFPNDepthMap(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BiFPNDepthMap, self).__init__()\n",
        "        \n",
        "        self.conv1x1 = nn.Conv2d(256, 1, kernel_size=1)\n",
        "        \n",
        "    def forward(self, bifpn_features):\n",
        "        H_max = max(feat.shape[2] for feat in bifpn_features)  \n",
        "        W_max = max(feat.shape[3] for feat in bifpn_features)  \n",
        "\n",
        "        depth_maps = []\n",
        "        for feature in bifpn_features:\n",
        "            depth = self.conv1x1(feature)\n",
        "            upsampled = F.interpolate(depth, size=(H_max, W_max), mode='bilinear', align_corners=False)  # [B, 1, H_max, W_max]\n",
        "            depth_maps.append(upsampled)\n",
        "\n",
        "        dep_map = torch.cat(depth_maps, dim=1)\n",
        "        dep_map = torch.mean(dep_map, dim=1, keepdim=True)  \n",
        "\n",
        "        return dep_map\n",
        "\n",
        "class PoseNetwork(nn.Module):\n",
        "    def __init__(self, backbone_channels=384, bifpn_channels=128):  # EfficientNet-B3 C5 channels + BiFPN features\n",
        "        super(PoseNetwork, self).__init__()\n",
        "        \n",
        "        # Backbone features processing\n",
        "        self.backbone_conv = nn.Sequential(\n",
        "            nn.Conv2d(backbone_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # BiFPN + Backbone fusion\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),  # Backbone + BiFPN features\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        # Pose prediction: 6 DOF\n",
        "        self.pose_head = nn.Linear(128, 6)\n",
        "        \n",
        "    def forward(self, backbone_features, bifpn_downsampled):\n",
        "        backbone_feat = self.backbone_conv(backbone_features)  # [B, 128]\n",
        "        depth_feat = self.depth_conv(bifpn_downsampled)\n",
        "        # Fuse backbone and BiFPN features\n",
        "        fused = torch.cat([backbone_feat, depth_feat], dim=1)  # [B, 256]\n",
        "        fused = self.fusion_fc(fused)  # [B, 128]\n",
        "        \n",
        "        # Pose prediction\n",
        "        pose = self.pose_head(fused)  # [B, 6]\n",
        "        \n",
        "        return pose\n",
        "\n",
        "class SharedMultiScaleModel(nn.Module):\n",
        "    \"\"\"Detection and Depth heads\"\"\"\n",
        "    def __init__(self, num_classes, in_channels=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Shared processing\n",
        "        self.shared_conv = nn.Conv2d(in_channels, 128, 3, padding=1)\n",
        "        \n",
        "        # Task heads\n",
        "        self.depth_head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, num_classes, 1)\n",
        "        )\n",
        "        \n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Conv2d(64, 4, 1)\n",
        "        )\n",
        "        \n",
        "        # Learnable fusion weights\n",
        "        self.depth_weights = nn.Parameter(torch.ones(5))\n",
        "        self.cls_weights = nn.Parameter(torch.ones(5))\n",
        "        self.reg_weights = nn.Parameter(torch.ones(5))\n",
        "        \n",
        "    def weighted_fusion(self, features, weights, target_size):\n",
        "        weights = F.softmax(weights, dim=0)\n",
        "        \n",
        "        fused = None\n",
        "        for feat, weight in zip(features, weights):\n",
        "            if feat.shape[2:] != target_size:\n",
        "                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
        "            \n",
        "            if fused is None:\n",
        "                fused = weight * feat\n",
        "            else:\n",
        "                fused += weight * feat\n",
        "                \n",
        "        return fused\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Shared processing\n",
        "        processed_features = [self.shared_conv(feat) for feat in x]\n",
        "        \n",
        "        # Predictions for each level\n",
        "        depth_preds = [self.depth_head(feat) for feat in processed_features]\n",
        "        cls_preds = [self.cls_head(feat) for feat in processed_features]  \n",
        "        reg_preds = [self.reg_head(feat) for feat in processed_features]\n",
        "        \n",
        "        # Target size (P3)\n",
        "        target_size = depth_preds[0].shape[2:]\n",
        "        \n",
        "        # Weighted fusion\n",
        "        final_depth = self.weighted_fusion(depth_preds, self.depth_weights, target_size)\n",
        "        final_cls = self.weighted_fusion(cls_preds, self.cls_weights, target_size)\n",
        "        final_reg = self.weighted_fusion(reg_preds, self.reg_weights, target_size)\n",
        "        \n",
        "        return {\n",
        "            'depth': final_depth,\n",
        "            'classification': final_cls,\n",
        "            'regression': final_reg\n",
        "        }\n",
        "\n",
        "class BackwardWarping(nn.Module):\n",
        "    \"\"\"\n",
        "    Backward warping using predicted depth and pose\n",
        "    Makaledeki yaklaşıma göre: önceki frame'den current frame'i synthesize et\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BackwardWarping, self).__init__()\n",
        "        \n",
        "    def forward(self, img_prev, depth_curr, pose, K):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_prev: Previous frame [B, 3, H, W]\n",
        "            depth_curr: Current depth [B, 1, H, W] \n",
        "            pose: Camera pose [B, 6] (tx, ty, tz, rx, ry, rz)\n",
        "            K: Camera intrinsic matrix [B, 3, 3]\n",
        "        Returns:\n",
        "            warped_img: Synthesized current frame [B, 3, H, W]\n",
        "            valid_mask: Valid pixel mask [B, 1, H, W]\n",
        "        \"\"\"\n",
        "        B, _, H, W = img_prev.shape\n",
        "        device = img_prev.device\n",
        "        \n",
        "        # Depth'i original image size'a resize et\n",
        "        if depth_curr.shape[2:] != (H, W):\n",
        "            depth_curr = F.interpolate(depth_curr, size=(H, W), mode='bilinear', align_corners=False)\n",
        "        \n",
        "        # Create pixel coordinates\n",
        "        i, j = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
        "        ones = torch.ones_like(i)\n",
        "        pixel_coords = torch.stack([j, i, ones], dim=0).float().to(device)\n",
        "        pixel_coords = pixel_coords.unsqueeze(0).repeat(B, 1, 1, 1)\n",
        "        \n",
        "        # Convert pose to transformation matrix\n",
        "        T = self.pose_to_matrix(pose)\n",
        "        \n",
        "        # Inverse camera intrinsics\n",
        "        K_inv = torch.inverse(K)\n",
        "        \n",
        "        # Back-project to 3D (current frame coordinates)\n",
        "        cam_coords = torch.matmul(K_inv.unsqueeze(-1).unsqueeze(-1), \n",
        "                                pixel_coords.unsqueeze(-1))\n",
        "        cam_coords = cam_coords.squeeze(-1) * depth_curr\n",
        "        \n",
        "        # Add homogeneous coordinate\n",
        "        ones = torch.ones(B, 1, H, W).to(device)\n",
        "        cam_coords_hom = torch.cat([cam_coords, ones], dim=1)\n",
        "        \n",
        "        # Transform to previous camera frame coordinates\n",
        "        cam_coords_prev = torch.matmul(T.unsqueeze(-1).unsqueeze(-1), \n",
        "                                     cam_coords_hom.unsqueeze(-1))\n",
        "        cam_coords_prev = cam_coords_prev.squeeze(-1)[:, :3]\n",
        "        \n",
        "        # Project back to 2D (previous frame pixel coordinates)\n",
        "        pixel_coords_prev = torch.matmul(K.unsqueeze(-1).unsqueeze(-1), \n",
        "                                       cam_coords_prev.unsqueeze(-1))\n",
        "        pixel_coords_prev = pixel_coords_prev.squeeze(-1)\n",
        "        \n",
        "        # Normalize by depth (perspective division)\n",
        "        Z = pixel_coords_prev[:, 2:3]\n",
        "        pixel_coords_prev = pixel_coords_prev[:, :2] / (Z + 1e-7)\n",
        "        \n",
        "        # Create valid mask (pixels that are within image bounds)\n",
        "        valid_mask = ((pixel_coords_prev[:, 0:1] >= 0) & \n",
        "                     (pixel_coords_prev[:, 0:1] < W) &\n",
        "                     (pixel_coords_prev[:, 1:2] >= 0) & \n",
        "                     (pixel_coords_prev[:, 1:2] < H) &\n",
        "                     (Z > 0)).float()\n",
        "        \n",
        "        # Normalize coordinates to [-1, 1] for grid_sample\n",
        "        pixel_coords_prev[:, 0] = 2 * pixel_coords_prev[:, 0] / (W - 1) - 1\n",
        "        pixel_coords_prev[:, 1] = 2 * pixel_coords_prev[:, 1] / (H - 1) - 1\n",
        "        \n",
        "        # Rearrange for grid_sample [B, H, W, 2]\n",
        "        grid = pixel_coords_prev.permute(0, 2, 3, 1)\n",
        "        \n",
        "        # Warp the previous image to synthesize current view\n",
        "        warped_img = F.grid_sample(img_prev, grid, mode='bilinear', \n",
        "                                 padding_mode='zeros', align_corners=True)\n",
        "        \n",
        "        # Apply valid mask\n",
        "        warped_img = warped_img * valid_mask\n",
        "        \n",
        "        return warped_img, valid_mask\n",
        "    \n",
        "    def pose_to_matrix(self, pose):\n",
        "        \"\"\"Convert 6DOF pose to 4x4 transformation matrix\"\"\"\n",
        "        B = pose.shape[0]\n",
        "        device = pose.device\n",
        "        \n",
        "        # Translation\n",
        "        t = pose[:, :3]\n",
        "        \n",
        "        # Rotation (axis-angle)\n",
        "        r = pose[:, 3:]\n",
        "        \n",
        "        # Convert axis-angle to rotation matrix\n",
        "        angle = torch.norm(r, dim=1, keepdim=True)\n",
        "        axis = r / (angle + 1e-7)\n",
        "        \n",
        "        cos_angle = torch.cos(angle)\n",
        "        sin_angle = torch.sin(angle)\n",
        "        \n",
        "        # Rodrigues' formula\n",
        "        K = torch.zeros(B, 3, 3).to(device)\n",
        "        K[:, 0, 1] = -axis[:, 2]\n",
        "        K[:, 0, 2] = axis[:, 1]\n",
        "        K[:, 1, 0] = axis[:, 2]\n",
        "        K[:, 1, 2] = -axis[:, 0]\n",
        "        K[:, 2, 0] = -axis[:, 1]\n",
        "        K[:, 2, 1] = axis[:, 0]\n",
        "        \n",
        "        I = torch.eye(3).unsqueeze(0).repeat(B, 1, 1).to(device)\n",
        "        R = I + sin_angle.unsqueeze(-1) * K + (1 - cos_angle).unsqueeze(-1) * torch.matmul(K, K)\n",
        "        \n",
        "        # Create 4x4 transformation matrix\n",
        "        T = torch.zeros(B, 4, 4).to(device)\n",
        "        T[:, :3, :3] = R\n",
        "        T[:, :3, 3] = t\n",
        "        T[:, 3, 3] = 1\n",
        "        \n",
        "        return T\n",
        "    \n",
        "    def pose_to_matrix(self, pose):\n",
        "        \"\"\"Convert 6DOF pose to 4x4 transformation matrix\"\"\"\n",
        "        B = pose.shape[0]\n",
        "        device = pose.device\n",
        "        \n",
        "        # Translation\n",
        "        t = pose[:, :3]\n",
        "        \n",
        "        # Rotation (axis-angle)\n",
        "        r = pose[:, 3:]\n",
        "        \n",
        "        # Convert axis-angle to rotation matrix\n",
        "        angle = torch.norm(r, dim=1, keepdim=True)\n",
        "        axis = r / (angle + 1e-7)\n",
        "        \n",
        "        cos_angle = torch.cos(angle)\n",
        "        sin_angle = torch.sin(angle)\n",
        "        \n",
        "        # Rodrigues' formula\n",
        "        K = torch.zeros(B, 3, 3).to(device)\n",
        "        K[:, 0, 1] = -axis[:, 2]\n",
        "        K[:, 0, 2] = axis[:, 1]\n",
        "        K[:, 1, 0] = axis[:, 2]\n",
        "        K[:, 1, 2] = -axis[:, 0]\n",
        "        K[:, 2, 0] = -axis[:, 1]\n",
        "        K[:, 2, 1] = axis[:, 0]\n",
        "        \n",
        "        I = torch.eye(3).unsqueeze(0).repeat(B, 1, 1).to(device)\n",
        "        R = I + sin_angle.unsqueeze(-1) * K + (1 - cos_angle).unsqueeze(-1) * torch.matmul(K, K)\n",
        "        \n",
        "        # Create 4x4 transformation matrix\n",
        "        T = torch.zeros(B, 4, 4).to(device)\n",
        "        T[:, :3, :3] = R\n",
        "        T[:, :3, 3] = t\n",
        "        T[:, 3, 3] = 1\n",
        "        \n",
        "        return T\n",
        "\n",
        "class CompleteMultiTaskModel(nn.Module):\n",
        "    \"\"\"Complete model following your specified architecture\"\"\"\n",
        "    def __init__(self, num_classes=10, bifpn_channels=256, bifpn_blocks=3):\n",
        "        super(CompleteMultiTaskModel, self).__init__()\n",
        "        \n",
        "        # Backbone encoder\n",
        "        self.encoder = EncoderBackBone()\n",
        "        \n",
        "        # EfficientNet-B3 channel dimensions (approximate values)\n",
        "        # Block 3: ~48, Block 5: ~136, Block 7: ~384\n",
        "        in_channels_list = [48, 136, 384]\n",
        "        \n",
        "        # BiFPN for multi-scale features\n",
        "        self.bifpn = BiFPN(in_channels_list, bifpn_channels, bifpn_blocks)\n",
        "        \n",
        "        # BiFPN downsampling for pose network\n",
        "        self.bifpn_downsampling = BiFPNDepthMap(bifpn_channels, 128)\n",
        "        \n",
        "        # Pose network (uses backbone + downsampled BiFPN features)\n",
        "        self.pose_network = PoseNetwork(backbone_channels=384, bifpn_channels=128)\n",
        "        \n",
        "        # Detection and Depth heads\n",
        "        self.detection_head = SharedMultiScaleModel(num_classes, bifpn_channels)\n",
        "        \n",
        "        # Backward warping\n",
        "        self.backward_warping = BackwardWarping()\n",
        "        \n",
        "    def forward(self, img_curr, img_prev=None, K=None):\n",
        "        \n",
        "        backbone_features = self.encoder(img_curr, forPose=False)  # [C3, C4, C5]\n",
        "        bifpn_features = self.bifpn(backbone_features)  # [P3, P4, P5, P6, P7]\n",
        "        \n",
        "\n",
        "        depth_map0 = self.bifpn_downsampling(bifpn_features)  # [B, 128]\n",
        "        \n",
        "\n",
        "        pose = self.pose_network(backbone_features, depth_map0)\n",
        "\n",
        "\n",
        "\n",
        "        detection_outputs = self.detection_head(bifpn_features)\n",
        "        \n",
        "        # Outputs\n",
        "        outputs = {\n",
        "            'depth': detection_outputs['depth'],\n",
        "            'classification': detection_outputs['classification'],\n",
        "            'regression': detection_outputs['regression'],\n",
        "            'pose': pose\n",
        "        }\n",
        "        \n",
        "        # 6. Backward warping (if previous frame provided)\n",
        "        if img_prev is not None and K is not None:\n",
        "            warped_img = self.backward_warping(img_prev, detection_outputs['depth'], pose, K)\n",
        "            outputs['warped_img'] = warped_img\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f33ce1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfSupervisedLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Makaledeki self-supervised loss functions\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.15, gamma=0.001):\n",
        "        super(SelfSupervisedLoss, self).__init__()\n",
        "        self.alpha = alpha  # SSIM loss weight (τ in paper)\n",
        "        self.gamma = gamma  # smoothness loss weight (γ in paper)\n",
        "        \n",
        "    def photometric_loss(self, img_curr, img_warped, valid_mask=None):\n",
        "        \"\"\"L1 photometric loss between current and warped image\"\"\"\n",
        "        diff = torch.abs(img_curr - img_warped)\n",
        "        if valid_mask is not None:\n",
        "            diff = diff * valid_mask\n",
        "            loss = diff.sum() / (valid_mask.sum() + 1e-7)\n",
        "        else:\n",
        "            loss = diff.mean()\n",
        "        return loss\n",
        "    \n",
        "    def ssim_loss(self, img_curr, img_warped, valid_mask=None):\n",
        "        \"\"\"SSIM loss between current and warped image\"\"\"\n",
        "        def ssim(x, y):\n",
        "            C1 = 0.01**2\n",
        "            C2 = 0.03**2\n",
        "            \n",
        "            mu_x = F.avg_pool2d(x, 3, 1, 1)\n",
        "            mu_y = F.avg_pool2d(y, 3, 1, 1)\n",
        "            \n",
        "            sigma_x = F.avg_pool2d(x**2, 3, 1, 1) - mu_x**2\n",
        "            sigma_y = F.avg_pool2d(y**2, 3, 1, 1) - mu_y**2\n",
        "            sigma_xy = F.avg_pool2d(x*y, 3, 1, 1) - mu_x*mu_y\n",
        "            \n",
        "            num = (2*mu_x*mu_y + C1) * (2*sigma_xy + C2)\n",
        "            den = (mu_x**2 + mu_y**2 + C1) * (sigma_x + sigma_y + C2)\n",
        "            \n",
        "            return num / den\n",
        "        \n",
        "        ssim_map = ssim(img_curr, img_warped)\n",
        "        if valid_mask is not None:\n",
        "            # Valid mask'i SSIM boyutuna resize et\n",
        "            valid_resized = F.interpolate(valid_mask, size=ssim_map.shape[2:], \n",
        "                                        mode='bilinear', align_corners=False)\n",
        "            ssim_map = ssim_map * valid_resized\n",
        "            loss = (1 - ssim_map).sum() / (valid_resized.sum() + 1e-7)\n",
        "        else:\n",
        "            loss = (1 - ssim_map).mean()\n",
        "        return loss\n",
        "    \n",
        "    def smoothness_loss(self, depth, img):\n",
        "        \"\"\"Edge-aware smoothness loss for depth\"\"\"\n",
        "        def gradient(x):\n",
        "            h_grad = torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])\n",
        "            v_grad = torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :])\n",
        "            return h_grad, v_grad\n",
        "        \n",
        "        # Normalize depth\n",
        "        depth_mean = depth.mean(dim=[1, 2, 3], keepdim=True)\n",
        "        depth_norm = depth / (depth_mean + 1e-7)\n",
        "        \n",
        "        # Depth gradients\n",
        "        depth_grad_h, depth_grad_v = gradient(depth_norm)\n",
        "        \n",
        "        # Image gradients (for edge-aware weighting)\n",
        "        img_grad_h, img_grad_v = gradient(img)\n",
        "        img_grad_h = img_grad_h.mean(dim=1, keepdim=True)\n",
        "        img_grad_v = img_grad_v.mean(dim=1, keepdim=True)\n",
        "        \n",
        "        # Edge-aware smoothness\n",
        "        smooth_h = depth_grad_h * torch.exp(-img_grad_h)\n",
        "        smooth_v = depth_grad_v * torch.exp(-img_grad_v)\n",
        "        \n",
        "        return smooth_h.mean() + smooth_v.mean()\n",
        "    \n",
        "    def forward(self, img_curr, img_warped, depth, valid_mask=None):\n",
        "        \"\"\"\n",
        "        Combined self-supervised loss as in the paper\n",
        "        \"\"\"\n",
        "        # Photometric loss\n",
        "        L_ph = self.photometric_loss(img_curr, img_warped, valid_mask)\n",
        "        \n",
        "        # SSIM loss\n",
        "        L_ssim = self.ssim_loss(img_curr, img_warped, valid_mask)\n",
        "        \n",
        "        # Smoothness loss\n",
        "        L_smooth = self.smoothness_loss(depth, img_curr)\n",
        "        \n",
        "        # Combined loss (equation 6 in paper)\n",
        "        # μ is auto-mask, simplified here as valid_mask consideration\n",
        "        L_total = self.alpha * L_ph + (1 - self.alpha) * L_ssim + self.gamma * L_smooth\n",
        "        \n",
        "        return {\n",
        "            'total': L_total,\n",
        "            'photometric': L_ph,\n",
        "            'ssim': L_ssim,\n",
        "            'smoothness': L_smooth\n",
        "        }\n",
        "    \n",
        "def create_scaled_camera_matrix(original_height, original_width, \n",
        "                              target_height, target_width, batch_size=1):\n",
        "    \"\"\"\n",
        "    BDD100K orijinal boyuttan hedef boyuta ölçeklenmiş K matrisi\n",
        "    \"\"\"\n",
        "    # Orijinal BDD100K için automotive kamera parametreleri\n",
        "    fov_rad = np.radians(70)  # Automotive kameralar için tipik FOV\n",
        "    focal_length = original_width / (2.0 * np.tan(fov_rad / 2.0))\n",
        "    \n",
        "    # Orijinal K matrisi\n",
        "    K_original = torch.zeros(batch_size, 3, 3)\n",
        "    K_original[:, 0, 0] = focal_length\n",
        "    K_original[:, 1, 1] = focal_length\n",
        "    K_original[:, 0, 2] = original_width / 2\n",
        "    K_original[:, 1, 2] = original_height / 2\n",
        "    K_original[:, 2, 2] = 1.0\n",
        "    \n",
        "    # Ölçekleme faktörleri\n",
        "    scale_x = target_width / original_width\n",
        "    scale_y = target_height / original_height\n",
        "    \n",
        "    # K matrisini ölçekle\n",
        "    K_scaled = K_original.clone()\n",
        "    K_scaled[:, 0, 0] *= scale_x  # fx\n",
        "    K_scaled[:, 1, 1] *= scale_y  # fy\n",
        "    K_scaled[:, 0, 2] *= scale_x  # cx\n",
        "    K_scaled[:, 1, 2] *= scale_y  # cy\n",
        "    \n",
        "    return K_scaled\n",
        "\n",
        "def get_bdd100k_camera_matrix(target_height=300, target_width=300, batch_size=1):\n",
        "    \"\"\"BDD100K için önerilen K matrisi\"\"\"\n",
        "    return create_scaled_camera_matrix(720, 1280,  # BDD100K orijinal boyut\n",
        "                                     target_height, target_width, \n",
        "                                     batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f84188",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Poz bilgisi boyutu : torch.Size([1, 6])\n",
            "Depth map boyutu : torch.Size([1, 1, 256, 256])\n",
            "1.Frame boyutu : torch.Size([1, 1, 3, 300, 300])\n"
          ]
        }
      ],
      "source": [
        "efficient_backbone = EncoderBackBone()\n",
        "bifpn = BiFPN([48,136,384],256,3)\n",
        "bifpn_depthmap=BiFPNDepthMap()\n",
        "pose_network = PoseNetwork(backbone_channels=3072,bifpn_channels=256)\n",
        "backward_warping = BackwardWarping()\n",
        "pred_labels=[]\n",
        "K = get_bdd100k_camera_matrix(300, 300, 1).cuda()\n",
        "for images, (labels_1, labels_2) in val_loader:\n",
        "    \n",
        "    x = efficient_backbone(images)\n",
        "    bifpn_out=bifpn(x[1])\n",
        "    depthmap_out=bifpn_depthmap(bifpn_out)\n",
        "    pose_network_out = pose_network(x[0],depthmap_out)\n",
        "    print(\"Poz bilgisi boyutu : \"+str(pose_network_out.shape))\n",
        "    print(\"Depth map boyutu : \"+str(depthmap_out.shape))\n",
        "    print(\"1.Frame boyutu : \"+str(images[:,:1:,:].shape))\n",
        "    print(\"K matris boyutu : \"+str(K.shape()))\n",
        "    syhntez_depth_map = backward_warping(images[:,:1:,:],depthmap_out,pose_network_out,K)\n",
        "    print(syhntez_depth_map.shape)\n",
        "    pred_labels.append(pose_network_out)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69a7750",
      "metadata": {},
      "outputs": [],
      "source": [
        "#            img_prev: Previous frame [B, 3, H, W]\n",
        "#            depth_curr: Current depth [B, 1, H, W]\n",
        "#            pose: Camera pose [B, 6] (tx, ty, tz, rx, ry, rz)\n",
        "#            K: Camera intrinsic matrix [B, 3, 3]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PytTorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
