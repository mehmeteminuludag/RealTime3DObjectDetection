{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44ce32d",
      "metadata": {
        "id": "f44ce32d"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import json\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HFHhIGESGWmk"
      },
      "id": "HFHhIGESGWmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a895497",
      "metadata": {
        "id": "5a895497"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path,transform=None):\n",
        "        classes = [\n",
        "  \"person\",\n",
        "  \"rider\",\n",
        "  \"car\",\n",
        "  \"truck\",\n",
        "  \"bus\",\n",
        "  \"train\",\n",
        "  \"motor\",\n",
        "  \"bike\",\n",
        "  \"traffic light\",\n",
        "  \"traffic sign\"\n",
        "]\n",
        "        self.transform = transform\n",
        "        self.data=[]\n",
        "        for file_name in os.listdir(data_path):\n",
        "            img_path = data_path+\"/\"+file_name\n",
        "            label_pth = img_path.replace(\"images\",\"labels\").replace(\".jpg\",\".json\")\n",
        "            labels=[]\n",
        "            with open(label_pth, \"r\") as label_file:\n",
        "                label = json.load(label_file)\n",
        "                objects = label[\"frames\"][0][\"objects\"]\n",
        "                for obj in objects:\n",
        "                    if \"box2d\" in obj:\n",
        "                        category = obj[\"category\"]\n",
        "                        category_num = classes.index(category)\n",
        "                        box = obj[\"box2d\"]\n",
        "                        x1, y1 = box[\"x1\"], box[\"y1\"]\n",
        "                        x2, y2 = box[\"x2\"], box[\"y2\"]\n",
        "                        labels.append([category_num,x1,y1,x2,y2])\n",
        "            self.data.append([img_path,labels])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, labels = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "class Transform(Dataset):\n",
        "    def __init__(self, base_dataset, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, labels = self.base_dataset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, labels\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04dbc5a",
      "metadata": {
        "id": "f04dbc5a"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/BDD100k/images\"\n",
        "train_path = data_path+\"/train\"\n",
        "validation_path = data_path+\"/val\"\n",
        "test_path = data_path+\"/test\"\n",
        "\n",
        "image_size=256\n",
        "batch_size=32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = Transform(CustomDataset(train_path),transform)\n",
        "test_dataset  = Transform(CustomDataset(test_path),transform)\n",
        "val_dataset   = Transform(CustomDataset(validation_path),transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0,collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78221ed",
      "metadata": {
        "id": "c78221ed"
      },
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, H, W]\n",
        "\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)  # [B, 2, H, W]\n",
        "\n",
        "        x_out = self.conv1(x_cat)  # [B, 1, H, W]\n",
        "        attention_map = self.sigmoid(x_out)\n",
        "\n",
        "        return x * attention_map\n",
        "\n",
        "\n",
        "class EncoderBackBone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderBackBone,self).__init__()\n",
        "        efficient = models.efficientnet_b3()\n",
        "        self.features = efficient.features\n",
        "        self.SAttention=SpatialAttention()\n",
        "    def forward(self,x,forPose): #Backbonedan çıkan veri BiFPN ve Pose hesaplaması için 2 ye ayrılacak BiFPN e 5-9.Bloklardan veri gidecek\n",
        "        outs = []                #Pose hesaplaması için sadece son çıktı yeterli\n",
        "        for i,block in enumerate(self.features):\n",
        "            x = block(x)\n",
        "            if i>2:\n",
        "                x = self.SAttention(x)\n",
        "            if i in [3, 5, 7] :\n",
        "                outs.append(x) # Burdaki veriler BiFPN e gönderilecek\n",
        "        if(forPose):\n",
        "            return outs\n",
        "        else :\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2122ef",
      "metadata": {
        "id": "1f2122ef"
      },
      "outputs": [],
      "source": [
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    \"\"\"Depthwise Separable Convolution used in BiFPN\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                 stride, padding, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.swish = nn.SiLU()  # Swish activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return self.swish(x)\n",
        "\n",
        "class BiFPNBlock(nn.Module):\n",
        "    \"\"\"Single BiFPN block with weighted feature fusion\"\"\"\n",
        "    def __init__(self, channels, epsilon=1e-4):\n",
        "        super(BiFPNBlock, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.channels = channels\n",
        "\n",
        "        # Convolution layers for each level\n",
        "        self.conv_p3 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p4 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p5 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p6 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p7 = DepthwiseSeparableConv(channels, channels)\n",
        "\n",
        "        # Weight parameters for feature fusion\n",
        "        # P6_td weights\n",
        "        self.w1 = nn.Parameter(torch.ones(2))\n",
        "        # P5_td weights\n",
        "        self.w2 = nn.Parameter(torch.ones(2))\n",
        "        # P4_td weights\n",
        "        self.w3 = nn.Parameter(torch.ones(2))\n",
        "        # P3_out weights\n",
        "        self.w4 = nn.Parameter(torch.ones(2))\n",
        "        # P4_out weights\n",
        "        self.w5 = nn.Parameter(torch.ones(3))\n",
        "        # P5_out weights\n",
        "        self.w6 = nn.Parameter(torch.ones(3))\n",
        "        # P6_out weights\n",
        "        self.w7 = nn.Parameter(torch.ones(3))\n",
        "        # P7_out weights\n",
        "        self.w8 = nn.Parameter(torch.ones(2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: [P3, P4, P5, P6, P7] feature maps\n",
        "        \"\"\"\n",
        "        P3, P4, P5, P6, P7 = inputs\n",
        "\n",
        "        # Top-down pathway\n",
        "        # P6_td = (w1[0] * P6 + w1[1] * resize(P7)) / (w1[0] + w1[1] + eps)\n",
        "        w1 = F.relu(self.w1)\n",
        "        P6_td = (w1[0] * P6 + w1[1] * self.up_sampling(P7, P6.shape[-2:])) / (w1.sum() + self.epsilon)\n",
        "        P6_td = self.conv_p6(P6_td)\n",
        "\n",
        "        # P5_td = (w2[0] * P5 + w2[1] * resize(P6_td)) / (w2[0] + w2[1] + eps)\n",
        "        w2 = F.relu(self.w2)\n",
        "        P5_td = (w2[0] * P5 + w2[1] * self.up_sampling(P6_td, P5.shape[-2:])) / (w2.sum() + self.epsilon)\n",
        "        P5_td = self.conv_p5(P5_td)\n",
        "\n",
        "        # P4_td = (w3[0] * P4 + w3[1] * resize(P5_td)) / (w3[0] + w3[1] + eps)\n",
        "        w3 = F.relu(self.w3)\n",
        "        P4_td = (w3[0] * P4 + w3[1] * self.up_sampling(P5_td, P4.shape[-2:])) / (w3.sum() + self.epsilon)\n",
        "        P4_td = self.conv_p4(P4_td)\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        # P3_out = (w4[0] * P3 + w4[1] * resize(P4_td)) / (w4[0] + w4[1] + eps)\n",
        "        w4 = F.relu(self.w4)\n",
        "        P3_out = (w4[0] * P3 + w4[1] * self.up_sampling(P4_td, P3.shape[-2:])) / (w4.sum() + self.epsilon)\n",
        "        P3_out = self.conv_p3(P3_out)\n",
        "\n",
        "        # P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * resize(P3_out)) / (w5[0] + w5[1] + w5[2] + eps)\n",
        "        w5 = F.relu(self.w5)\n",
        "        P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * self.down_sampling(P3_out, P4.shape[-2:])) / (w5.sum() + self.epsilon)\n",
        "        P4_out = self.conv_p4(P4_out)\n",
        "\n",
        "        # P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * resize(P4_out)) / (w6[0] + w6[1] + w6[2] + eps)\n",
        "        w6 = F.relu(self.w6)\n",
        "        P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * self.down_sampling(P4_out, P5.shape[-2:])) / (w6.sum() + self.epsilon)\n",
        "        P5_out = self.conv_p5(P5_out)\n",
        "\n",
        "        # P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * resize(P5_out)) / (w7[0] + w7[1] + w7[2] + eps)\n",
        "        w7 = F.relu(self.w7)\n",
        "        P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * self.down_sampling(P5_out, P6.shape[-2:])) / (w7.sum() + self.epsilon)\n",
        "        P6_out = self.conv_p6(P6_out)\n",
        "\n",
        "        # P7_out = (w8[0] * P7 + w8[1] * resize(P6_out)) / (w8[0] + w8[1] + eps)\n",
        "        w8 = F.relu(self.w8)\n",
        "        P7_out = (w8[0] * P7 + w8[1] * self.down_sampling(P6_out, P7.shape[-2:])) / (w8.sum() + self.epsilon)\n",
        "        P7_out = self.conv_p7(P7_out)\n",
        "\n",
        "        return [P3_out, P4_out, P5_out, P6_out, P7_out]\n",
        "\n",
        "    def up_sampling(self, x, target_size):\n",
        "        \"\"\"Upsampling with nearest neighbor interpolation\"\"\"\n",
        "        return F.interpolate(x, size=target_size, mode='nearest')\n",
        "\n",
        "    def down_sampling(self, x, target_size):\n",
        "        \"\"\"Downsampling with max pooling\"\"\"\n",
        "        if x.shape[-2:] == target_size:\n",
        "            return x\n",
        "\n",
        "        # Calculate stride for max pooling\n",
        "        stride = x.shape[-1] // target_size[-1]\n",
        "        kernel_size = stride\n",
        "\n",
        "        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    \"\"\"Complete BiFPN module with multiple blocks\"\"\"\n",
        "    def __init__(self, in_channels_list, out_channels=256, num_blocks=3):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Input projection layers to match channel dimensions\n",
        "        self.input_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, out_channels, 1, bias=False)\n",
        "            for in_ch in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # Additional P6 and P7 layers\n",
        "        self.p6_conv = nn.Conv2d(in_channels_list[-1], out_channels, 3, stride=2, padding=1)\n",
        "        self.p7_conv = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN blocks\n",
        "        self.bifpn_blocks = nn.ModuleList([\n",
        "            BiFPNBlock(out_channels) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: List of feature maps from backbone [C3, C4, C5]\n",
        "        returns: List of enhanced feature maps [P3, P4, P5, P6, P7]\n",
        "        \"\"\"\n",
        "        # Project input features to same channel dimension\n",
        "        features = []\n",
        "        for i, feat in enumerate(inputs):\n",
        "            features.append(self.input_convs[i](feat))\n",
        "\n",
        "        # Create P6 and P7\n",
        "        P6 = self.p6_conv(inputs[-1])  # P6 from C5\n",
        "        P7 = self.p7_conv(P6)          # P7 from P6\n",
        "\n",
        "        # Initial feature list [P3, P4, P5, P6, P7]\n",
        "        pyramid_features = features + [P6, P7]\n",
        "\n",
        "        # Apply BiFPN blocks\n",
        "        for block in self.bifpn_blocks:\n",
        "            pyramid_features = block(pyramid_features)\n",
        "\n",
        "        return pyramid_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f0dd93",
      "metadata": {
        "id": "99f0dd93"
      },
      "outputs": [],
      "source": [
        "efficient_backbone = EncoderBackBone()\n",
        "biFPN = BiFPN([48,136,384],256,3)\n",
        "pred_labels=[]\n",
        "for i, data in enumerate(val_loader):\n",
        "    images,labels=data\n",
        "    x = efficient_backbone(images,forPose=True)\n",
        "    out = biFPN(x)\n",
        "    pred_labels.append(out)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PytTorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}