{"cells":[{"cell_type":"code","execution_count":1,"id":"HFHhIGESGWmk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18743,"status":"ok","timestamp":1754403770178,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"HFHhIGESGWmk","outputId":"392fc104-b6dd-4112-c984-5eaa33f97b37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"id":"94mNEv4-4hiR","metadata":{"executionInfo":{"elapsed":681401,"status":"ok","timestamp":1754404474362,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"94mNEv4-4hiR"},"outputs":[],"source":["src = \"/content/drive/MyDrive/kitti2012\"\n","dst = \"/content\"\n","os.makedirs(dst, exist_ok=True)\n","\n","!cp -r \"$src\" \"$dst\""]},{"cell_type":"code","execution_count":1,"id":"f44ce32d","metadata":{"executionInfo":{"elapsed":2949,"status":"ok","timestamp":1754419535314,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"f44ce32d"},"outputs":[],"source":["import os\n","import cv2\n","import json\n","import torch\n","import shutil\n","import logging\n","import numpy as np\n","import torchvision\n","import torch.optim as optim\n","import torchvision.ops as ops\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch import nn\n","from tqdm import tqdm\n","from PIL import Image\n","from collections import Counter\n","from torchsummary import summary\n","from typing import Dict, Any, Optional\n","from torchvision import transforms , models\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":2,"id":"jhdeI9tm_43Y","metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1754419535352,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"jhdeI9tm_43Y"},"outputs":[],"source":["class KITTI_Dataset(Dataset):\n","    def __init__(self, data_path, transform=None, mode='train'):\n","        self.data_path = data_path\n","        self.transform = transform\n","        self.mode = mode  # 'train', 'val', veya 'test'\n","        self.classes = ['Car', 'Van', 'Truck', 'Pedestrian', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n","        self.class_map = {cls: idx for idx, cls in enumerate(self.classes)}\n","        self.data = []\n","\n","        # KITTI görsel boyutları - normalizasyon için\n","        self.img_width = 1242\n","        self.img_height = 375\n","        self.max_depth = 80.0  # KITTI max derinlik\n","\n","        image_dir = os.path.join(data_path, 'training', 'colored_0')\n","        label_dir = os.path.join(data_path, 'training', 'label_2')\n","        disp_dir = os.path.join(data_path, 'training', 'disp_noc')\n","\n","        file_names = os.listdir(image_dir)\n","        for fname in file_names:\n","            if fname.endswith('.png'):\n","                scene_id = fname.split('_')[0]\n","                img_path = os.path.join(image_dir, fname)\n","                label_path = os.path.join(label_dir, f'{scene_id}.txt')\n","                disp_path = os.path.join(disp_dir, f'{scene_id}_10.png')\n","                if os.path.exists(label_path) and os.path.exists(disp_path):\n","                    self.data.append((img_path, label_path, disp_path))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def convert_labels(self, label_file):\n","        \"\"\"KITTI label_2 etiketlerini normalize ederek [category_num, x, y, w, h] formatına dönüştürür.\"\"\"\n","        labels = []\n","        with open(label_file, 'r') as f:\n","            for line in f:\n","                parts = line.strip().split()\n","                category = parts[0]\n","                if category in self.class_map :\n","                    x1, y1, x2, y2 = map(float, parts[4:8])\n","\n","                    # Merkez koordinat ve boyutları hesapla\n","                    x_center = (x1 + x2) / 2\n","                    y_center = (y1 + y2) / 2\n","                    width = x2 - x1\n","                    height = y2 - y1\n","\n","                    # [0,1] aralığına normalize et\n","                    x_norm = x_center / self.img_width\n","                    y_norm = y_center / self.img_height\n","                    w_norm = width / self.img_width\n","                    h_norm = height / self.img_height\n","\n","                    category_num = self.class_map[category]\n","                    labels.append([category_num, x_norm, y_norm, w_norm, h_norm])\n","        return labels\n","\n","    def load_disparity(self, disp_path):\n","        \"\"\"Disparite haritasını yükler.\"\"\"\n","        disp_map = cv2.imread(disp_path, cv2.IMREAD_UNCHANGED) / 256.0\n","        return disp_map\n","\n","    def calculate_depth(self, disp_map):\n","        \"\"\"Disparite haritasından normalize edilmiş derinlik hesaplar.\"\"\"\n","        baseline = 0.54\n","        focal_length = 721.5377\n","        depth = (baseline * focal_length) / (disp_map + 1e-6)\n","        depth = np.clip(depth, 0, self.max_depth)\n","\n","        # [0,1] aralığına normalize et\n","        normalized_depth = depth / self.max_depth\n","        return normalized_depth\n","\n","    def get_depth_at_box(self, depth_map, x, y, w, h):\n","        \"\"\"Bounding box merkezindeki normalize edilmiş derinlik değerini döndürür.\"\"\"\n","        # Normalize edilmiş koordinatları piksel koordinatlarına çevir\n","        x_pixel = int(x * self.img_width)\n","        y_pixel = int(y * self.img_height)\n","\n","        # Sınır kontrolü\n","        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n","        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n","\n","        if depth_map[y_pixel, x_pixel] == 0:\n","            return np.nan\n","        return depth_map[y_pixel, x_pixel]\n","\n","    def get_disparity_at_box(self, disp_map, x, y, w, h):\n","        \"\"\"Bounding box merkezindeki normalize edilmiş disparite değerini döndürür.\"\"\"\n","        # Normalize edilmiş koordinatları piksel koordinatlarına çevir\n","        x_pixel = int(x * self.img_width)\n","        y_pixel = int(y * self.img_height)\n","\n","        # Sınır kontrolü\n","        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n","        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n","\n","        if disp_map[y_pixel, x_pixel] == 0:\n","            return np.nan\n","\n","        # Disparite değerini de normalize et (max disparite ~300 civarı)\n","        max_disparity = 300.0\n","        normalized_disparity = disp_map[y_pixel, x_pixel] / max_disparity\n","        return np.clip(normalized_disparity, 0, 1)\n","\n","    def __getitem__(self, idx):\n","        img_path, label_path, disp_path = self.data[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        labels = self.convert_labels(label_path)  # Artık normalize edilmiş koordinatlar\n","        disp_map = self.load_disparity(disp_path)\n","\n","        if self.mode == 'train':\n","            # Eğitim: Normalize edilmiş disparite ile etiket döndür\n","            labels_with_disparity = []\n","            for label in labels:\n","                category_num, x, y, w, h = label\n","                disparity = self.get_disparity_at_box(disp_map, x, y, w, h)\n","                labels_with_disparity.append([category_num, x, y, w, h, disparity])\n","            output_labels = labels_with_disparity\n","        else:\n","            # Doğrulama/Test: Normalize edilmiş derinlik ile etiket döndür\n","            depth_map = self.calculate_depth(disp_map)\n","            labels_with_depth = []\n","            for label in labels:\n","                category_num, x, y, w, h = label\n","                depth = self.get_depth_at_box(depth_map, x, y, w, h)\n","                labels_with_depth.append([category_num, x, y, w, h, depth])\n","            output_labels = labels_with_depth\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, output_labels\n","def kitti_collate_fn(batch):\n","    images = [item[0] for item in batch]\n","    labels = [item[1] for item in batch]\n","    images = torch.stack(images, dim=0)  # [B, C, H, W]\n","    return images, labels\n","\n","# Transform tanımla\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"]},{"cell_type":"code","execution_count":3,"id":"f04dbc5a","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1754419535354,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"f04dbc5a"},"outputs":[],"source":["data_path = \"/content/kitti2012\" #colab\n","#data_path = \"C:/Users/Mehmet/Desktop/kitti2012\" #lcoal\n","test_path = data_path+\"/testing\"\n","\n","image_size=300\n","batch_size=2\n","\n","transform = transforms.Compose([\n","    transforms.Resize((image_size, image_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_val_dataset = KITTI_Dataset(data_path=data_path, transform=transform, mode='train')\n","train_size = int(0.8 * len(train_val_dataset))  # ~155 sahne\n","val_size = len(train_val_dataset) - train_size  # ~39 sahne\n","train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n","val_dataset.dataset.mode = 'val'\n","#test_dataset  = KITTI_Dataset(test_path,transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n","#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)"]},{"cell_type":"code","source":["print(f\"Veri seti boyutu: {len(train_dataset)} örnek\")\n","cls_labels=[]\n","depth_maps = []\n","bboxes = []\n","for i in range(len(train_dataset)):\n","  for j in range(len(train_dataset[i][1])):\n","    cls_labels.append(train_dataset[i][1][j][0])\n","    depth_maps.append(train_dataset[i][1][j][5])\n","    bboxes.append(train_dataset[i][1][j][1:4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfTUquQgbec4","executionInfo":{"status":"ok","timestamp":1754419778143,"user_tz":-180,"elapsed":229031,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"}},"outputId":"b7087c5f-a9d7-4372-c774-36c1cf6f8863"},"id":"cfTUquQgbec4","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Veri seti boyutu: 310 örnek\n"]}]},{"cell_type":"code","source":["cls_distribution = Counter(cls_labels)\n","sorted_cls_distribution = sorted(cls_distribution.items(), key=lambda x: x[0])\n","\n","class_weights = torch.tensor([i[1]for i in sorted_cls_distribution],\n","                             dtype=torch.float32).to(device)\n","labels, values = zip(*sorted_cls_distribution)\n","\n","plt.figure(figsize=(5, 5))\n","plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n","plt.title('Sınıf Dağılımı')\n","plt.axis('equal')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"6VTAN7XJa4_g","executionInfo":{"status":"ok","timestamp":1754419778478,"user_tz":-180,"elapsed":348,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"}},"outputId":"87a5054e-5016-4d79-a963-27d88f2c2ddb"},"id":"6VTAN7XJa4_g","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":6,"id":"be09f37a","metadata":{"executionInfo":{"elapsed":169,"status":"ok","timestamp":1754419778647,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"be09f37a"},"outputs":[],"source":["class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","        padding = (kernel_size - 1) // 2\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x_cat = torch.cat([avg_out, max_out], dim=1)\n","        x_out = self.conv1(x_cat)\n","        attention_map = self.sigmoid(x_out)\n","        return x * attention_map\n","\n","class EncoderBackBone(nn.Module):\n","    def __init__(self,İsPretreained=True):\n","        super(EncoderBackBone,self).__init__()\n","        efficient = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n","        self.features = efficient.features\n","        self.SAttention = SpatialAttention()\n","\n","    def forward(self, x):         # B,C,H,W\n","\n","        outs = []\n","\n","        # Her iki frame için özellikler\n","        for i, block in enumerate(self.features):\n","            x = block(x)\n","            if i > 2:  # C3'ten sonrası için Spatial Attention\n","                x = x * self.SAttention(x)\n","            if i in [3,5,7]:\n","                out = F.interpolate(x, size=256, mode='bilinear', align_corners=False)\n","                outs.append(out)\n","        return outs\n","\n","class DepthwiseSeparableConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        super(DepthwiseSeparableConv, self).__init__()\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n","                                 stride, padding, groups=in_channels, bias=False)\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.swish = nn.SiLU()\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        x = self.bn(x)\n","        return self.swish(x)\n","\n","class BiFPNBlock(nn.Module):\n","    def __init__(self, channels, epsilon=1e-4):\n","        super(BiFPNBlock, self).__init__()\n","        self.epsilon = epsilon\n","        self.channels = channels\n","\n","        # Convolution layers for each level\n","        self.conv_p3 = DepthwiseSeparableConv(channels, channels)\n","        self.conv_p4 = DepthwiseSeparableConv(channels, channels)\n","        self.conv_p5 = DepthwiseSeparableConv(channels, channels)\n","        self.conv_p6 = DepthwiseSeparableConv(channels, channels)\n","        self.conv_p7 = DepthwiseSeparableConv(channels, channels)\n","\n","        # Weight parameters for feature fusion\n","        self.w1 = nn.Parameter(torch.ones(2))\n","        self.w2 = nn.Parameter(torch.ones(2))\n","        self.w3 = nn.Parameter(torch.ones(2))\n","        self.w4 = nn.Parameter(torch.ones(2))\n","        self.w5 = nn.Parameter(torch.ones(3))\n","        self.w6 = nn.Parameter(torch.ones(3))\n","        self.w7 = nn.Parameter(torch.ones(3))\n","        self.w8 = nn.Parameter(torch.ones(2))\n","\n","    def forward(self, inputs):\n","        P3, P4, P5, P6, P7 = inputs\n","\n","        # Bottom-up pathway\n","        w1 = F.relu(self.w1)\n","        P6_td = (w1[0] * P6 + w1[1] * self.up_sampling(P7, P6.shape[-2:])) / (w1.sum() + self.epsilon)\n","        P6_td = self.conv_p6(P6_td)\n","\n","        w2 = F.relu(self.w2)\n","        P5_td = (w2[0] * P5 + w2[1] * self.up_sampling(P6_td, P5.shape[-2:])) / (w2.sum() + self.epsilon)\n","        P5_td = self.conv_p5(P5_td)\n","\n","        w3 = F.relu(self.w3)\n","        P4_td = (w3[0] * P4 + w3[1] * self.up_sampling(P5_td, P4.shape[-2:])) / (w3.sum() + self.epsilon)\n","        P4_td = self.conv_p4(P4_td)\n","\n","        # Top-down pathway\n","        w4 = F.relu(self.w4)\n","        P3_out = (w4[0] * P3 + w4[1] * self.up_sampling(P4_td, P3.shape[-2:])) / (w4.sum() + self.epsilon)\n","        P3_out = self.conv_p3(P3_out)\n","\n","        w5 = F.relu(self.w5)\n","        P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * self.down_sampling(P3_out, P4.shape[-2:])) / (w5.sum() + self.epsilon)\n","        P4_out = self.conv_p4(P4_out)\n","\n","        w6 = F.relu(self.w6)\n","        P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * self.down_sampling(P4_out, P5.shape[-2:])) / (w6.sum() + self.epsilon)\n","        P5_out = self.conv_p5(P5_out)\n","\n","        w7 = F.relu(self.w7)\n","        P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * self.down_sampling(P5_out, P6.shape[-2:])) / (w7.sum() + self.epsilon)\n","        P6_out = self.conv_p6(P6_out)\n","\n","        w8 = F.relu(self.w8)\n","        P7_out = (w8[0] * P7 + w8[1] * self.down_sampling(P6_out, P7.shape[-2:])) / (w8.sum() + self.epsilon)\n","        P7_out = self.conv_p7(P7_out)\n","\n","        return [P3_out, P4_out, P5_out, P6_out, P7_out]\n","\n","    def up_sampling(self, x, target_size):\n","        return F.interpolate(x, size=target_size, mode='nearest')\n","\n","    def down_sampling(self, x, target_size):\n","        if x.shape[-2:] == target_size:\n","            return x\n","        stride = x.shape[-1] // target_size[-1]\n","        kernel_size = stride\n","        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)\n","\n","class BiFPN(nn.Module):\n","    def __init__(self, in_channels_list, out_channels=256, num_blocks=3):\n","        super(BiFPN, self).__init__()\n","        self.out_channels = out_channels\n","        self.num_blocks = num_blocks\n","\n","        # Input projection layers\n","        self.input_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, out_channels, 1, bias=False)\n","            for in_ch in in_channels_list\n","        ])\n","\n","        # Additional P6 and P7 layers\n","        self.p6_conv = nn.Conv2d(in_channels_list[-1], out_channels, 3, stride=2, padding=1)\n","        self.p7_conv = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n","\n","        # BiFPN blocks\n","        self.bifpn_blocks = nn.ModuleList([\n","            BiFPNBlock(out_channels) for _ in range(num_blocks)\n","        ])\n","\n","    def forward(self, inputs):\n","        # Project input features\n","        features = []\n","        for i, feat in enumerate(inputs):\n","            features.append(self.input_convs[i](feat))\n","\n","        # Create P6 and P7\n","        P6 = self.p6_conv(inputs[-1])\n","        P7 = self.p7_conv(P6)\n","\n","        # Initial feature list\n","        pyramid_features = features + [P6, P7]\n","\n","        # Apply BiFPN blocks\n","        for block in self.bifpn_blocks:\n","            pyramid_features = block(pyramid_features)\n","\n","        return pyramid_features\n","\n","class NNConv3UpBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        return x\n","\n","class FusionBlock(nn.Module):\n","    def __init__(self, fusion_type='add'):\n","        super().__init__()\n","        self.fusion_type = fusion_type\n","\n","    def forward(self, high_level, low_level):\n","        if self.fusion_type == 'add':\n","            return high_level + low_level\n","        elif self.fusion_type == 'concat':\n","            return torch.cat([high_level, low_level], dim=1)\n","\n","class PredictionDecoder(nn.Module):\n","    def __init__(self, in_channels, out_channels=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels//2, out_channels, kernel_size=3, padding=1)\n","        self.leaky_relu = nn.LeakyReLU(0.1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.leaky_relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        return x\n","\n","class RTMonoDepthDecoder(nn.Module):\n","    def __init__(self, encoder_channels=[48, 136, 384], decoder_channels=[256, 128, 64, 32]):\n","        super().__init__()\n","\n","        # Upsampling blocks\n","        self.upconv2 = NNConv3UpBlock(encoder_channels[2], decoder_channels[0])  # F3 -> D2\n","        self.upconv1 = NNConv3UpBlock(decoder_channels[0], decoder_channels[1])  # After fusion -> D1\n","        self.upconv0 = NNConv3UpBlock(decoder_channels[1], decoder_channels[2])  # After fusion -> D0\n","\n","        # Projection layers to match dimensions for fusion\n","        self.proj2 = nn.Conv2d(encoder_channels[1], decoder_channels[0], 1)  # F2 -> D2 channels\n","        self.proj1 = nn.Conv2d(encoder_channels[0], decoder_channels[1], 1)  # F1 -> D1 channels\n","\n","        # Fusion blocks\n","        self.fusion1 = FusionBlock('add')\n","        self.fusion0 = FusionBlock('concat')\n","\n","        # Prediction decoders at each scale\n","        self.decoder2 = PredictionDecoder(decoder_channels[0])\n","        self.decoder1 = PredictionDecoder(decoder_channels[1])\n","        # After concat: up1_resized (128) + f1_proj (128) = 256 channels\n","        self.decoder0 = PredictionDecoder(decoder_channels[1] + decoder_channels[1])\n","\n","    def forward(self, features, inference_mode=False):\n","        f1, f2, f3 = features  # [low_res -> high_res]\n","        depth_maps = {}\n","\n","        # Level 2: Start from highest level feature\n","        up2 = self.upconv2(f3)\n","        if not inference_mode:\n","            depth_maps['depth_2'] = self.decoder2(up2)\n","\n","        # Level 1: Project F2 to match up2 channels and fuse\n","        f2_proj = self.proj2(f2)\n","        # Resize up2 to match f2 spatial dimensions\n","        up2_resized = F.interpolate(up2, size=f2_proj.shape[-2:], mode='bilinear', align_corners=False)\n","        fused1 = self.fusion1(up2_resized, f2_proj)\n","        up1 = self.upconv1(fused1)\n","        if not inference_mode:\n","            depth_maps['depth_1'] = self.decoder1(up1)\n","\n","        # Level 0: Project F1 to match up1 channels and fuse\n","        f1_proj = self.proj1(f1)\n","        # Resize up1 to match f1 spatial dimensions\n","        up1_resized = F.interpolate(up1, size=f1_proj.shape[-2:], mode='bilinear', align_corners=False)\n","        fused0 = self.fusion0(up1_resized, f1_proj)\n","        depth_maps['depth_0'] = self.decoder0(fused0)\n","\n","        return depth_maps\n","class DepthHead(nn.Module):\n","    def __init__(self, in_channels=256):\n","        super(DepthHead, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, 128, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 64, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 1, 1),\n","            nn.Sigmoid()\n","        )\n","        self.weights = nn.Parameter(torch.ones(5))\n","\n","    def weighted_fusion(self, features, weights, target_size):\n","        weights = F.softmax(weights, dim=0)\n","        fused = None\n","        for feat, weight in zip(features, weights):\n","            if feat.shape[2:] != target_size:\n","                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n","            if fused is None:\n","                fused = weight * feat\n","            else:\n","                fused += weight * feat\n","        return fused\n","\n","    def forward(self, features):\n","        processed = [self.conv(feat) for feat in features]\n","        target_size = processed[0].shape[2:]\n","        return self.weighted_fusion(processed, self.weights, target_size)\n","\n","class ClassificationHead(nn.Module):\n","    def __init__(self, in_channels=256, num_classes=10):\n","        super(ClassificationHead, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, 128, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 64, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, num_classes, 1)\n","        )\n","        self.weights = nn.Parameter(torch.ones(5))\n","\n","    def weighted_fusion(self, features, weights, target_size):\n","        weights = F.softmax(weights, dim=0)\n","        fused = None\n","        for feat, weight in zip(features, weights):\n","            if feat.shape[2:] != target_size:\n","                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n","            if fused is None:\n","                fused = weight * feat\n","            else:\n","                fused += weight * feat\n","        return fused\n","\n","    def forward(self, features):\n","        processed = [self.conv(feat) for feat in features]\n","        target_size = processed[0].shape[2:]\n","        return self.weighted_fusion(processed, self.weights, target_size)\n","\n","class RegressionHead(nn.Module):\n","    def __init__(self, in_channels=256):\n","        super(RegressionHead, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, 128, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 64, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 4, 1)\n","        )\n","        self.weights = nn.Parameter(torch.ones(5))\n","\n","    def weighted_fusion(self, features, weights, target_size):\n","        weights = F.softmax(weights, dim=0)\n","        fused = None\n","        for feat, weight in zip(features, weights):\n","            if feat.shape[2:] != target_size:\n","                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n","            if fused is None:\n","                fused = weight * feat\n","            else:\n","                fused += weight * feat\n","        return fused\n","\n","    def forward(self, features):\n","        processed = [self.conv(feat) for feat in features]\n","        target_size = processed[0].shape[2:]\n","        return self.weighted_fusion(processed, self.weights, target_size)\n","\n","class MultiTaskHeads(nn.Module):\n","    def __init__(self, num_classes=10, in_channels=256):\n","        super(MultiTaskHeads, self).__init__()\n","        self.depth_head = DepthHead(in_channels)\n","        self.cls_head = ClassificationHead(in_channels, num_classes)\n","        self.reg_head = RegressionHead(in_channels)\n","\n","    def forward(self, features):\n","        depth = self.depth_head(features)\n","        cls = self.cls_head(features)\n","        reg = self.reg_head(features)\n","        return {\n","            'depth': depth,\n","            'classification': cls,\n","            'regression': reg\n","        }\n","\n","class DetectionPostProcessor(nn.Module):\n","    \"\"\"Post-process detection outputs to final format [cls, x, y, w, h, depth]\"\"\"\n","    def __init__(self, num_classes=80, confidence_threshold=0.5, max_detections=100):\n","        super(DetectionPostProcessor, self).__init__()\n","        self.num_classes = num_classes\n","        self.confidence_threshold = confidence_threshold\n","        self.max_detections = max_detections\n","\n","    def forward(self, classification, regression, depth, return_raw=False):\n","        \"\"\"\n","        Args:\n","            classification: [B, num_classes, H, W] - class probabilities\n","            regression: [B, 4, H, W] - bbox coordinates (x, y, w, h)\n","            depth: [B, 1, H, W] - depth values\n","            return_raw: if True, return raw outputs without post-processing\n","\n","        Returns:\n","            if return_raw=True:\n","                raw_outputs: [B, 6+num_classes, H, W] - concatenated raw outputs\n","            else:\n","                detections: [B, max_detections, 6] - [cls_id, x, y, w, h, depth]\n","                scores: [B, max_detections] - confidence scores\n","        \"\"\"\n","        batch_size, _, height, width = classification.shape\n","\n","        if return_raw:\n","            # Return raw concatenated outputs for training\n","            # Format: [cls_probs(num_classes), x, y, w, h, depth]\n","            raw_outputs = torch.cat([\n","                classification,  # [B, num_classes, H, W]\n","                regression,      # [B, 4, H, W]\n","                depth           # [B, 1, H, W]\n","            ], dim=1)  # [B, num_classes+5, H, W]\n","            return raw_outputs\n","\n","        # Post-process for inference\n","        device = classification.device\n","\n","        # Apply softmax to classification\n","        cls_probs = F.softmax(classification, dim=1)  # [B, num_classes, H, W]\n","\n","        # Get max class probabilities and indices\n","        max_scores, class_ids = torch.max(cls_probs, dim=1, keepdim=True)  # [B, 1, H, W]\n","\n","        # Filter by confidence threshold\n","        valid_mask = (max_scores > self.confidence_threshold).squeeze(1)  # [B, H, W]\n","\n","        batch_detections = []\n","        batch_scores = []\n","\n","        for b in range(batch_size):\n","            # Get valid positions for this batch\n","            valid_positions = torch.nonzero(valid_mask[b], as_tuple=False)  # [N, 2] (y, x coordinates)\n","\n","            if len(valid_positions) == 0:\n","                # No valid detections\n","                detections = torch.zeros((self.max_detections, 6), device=device)\n","                scores = torch.zeros(self.max_detections, device=device)\n","            else:\n","                # Extract values at valid positions\n","                y_coords, x_coords = valid_positions[:, 0], valid_positions[:, 1]\n","\n","                # Get predictions at valid positions\n","                batch_class_ids = class_ids[b, 0, y_coords, x_coords]  # [N]\n","                batch_scores_raw = max_scores[b, 0, y_coords, x_coords]  # [N]\n","                batch_regression = regression[b, :, y_coords, x_coords].t()  # [N, 4]\n","                batch_depth = depth[b, 0, y_coords, x_coords]  # [N]\n","\n","                # Convert relative coordinates to absolute (if needed)\n","                # Assuming regression outputs are already in proper format\n","                bbox_x = batch_regression[:, 0]\n","                bbox_y = batch_regression[:, 1]\n","                bbox_w = batch_regression[:, 2]\n","                bbox_h = batch_regression[:, 3]\n","\n","                # Stack all predictions\n","                batch_detections_raw = torch.stack([\n","                    batch_class_ids.float(),  # class id\n","                    bbox_x,                   # x\n","                    bbox_y,                   # y\n","                    bbox_w,                   # w\n","                    bbox_h,                   # h\n","                    batch_depth               # depth\n","                ], dim=1)  # [N, 6]\n","\n","                # Sort by confidence and take top max_detections\n","                sorted_indices = torch.argsort(batch_scores_raw, descending=True)\n","                sorted_indices = sorted_indices[:self.max_detections]\n","\n","                # Pad if necessary\n","                num_detections = min(len(sorted_indices), self.max_detections)\n","                detections = torch.zeros((self.max_detections, 6), device=device)\n","                scores = torch.zeros(self.max_detections, device=device)\n","\n","                if num_detections > 0:\n","                    detections[:num_detections] = batch_detections_raw[sorted_indices[:num_detections]]\n","                    scores[:num_detections] = batch_scores_raw[sorted_indices[:num_detections]]\n","\n","            batch_detections.append(detections)\n","            batch_scores.append(scores)\n","\n","        # Stack batch results\n","        final_detections = torch.stack(batch_detections, dim=0)  # [B, max_detections, 6]\n","        final_scores = torch.stack(batch_scores, dim=0)          # [B, max_detections]\n","\n","        return final_detections, final_scores\n","\n","class CompleteMultiTaskModel(nn.Module):\n","    def __init__(self,İsPretreained=True, num_classes=10, bifpn_channels=256, bifpn_blocks=3, confidence_threshold=0.5, max_detections=100):\n","        super(CompleteMultiTaskModel, self).__init__()\n","        self.encoder = EncoderBackBone(İsPretreained)\n","        in_channels_list = [48, 136, 384]  # EfficientNet-B3 kanal boyutları\n","        self.bifpn = BiFPN(in_channels_list, bifpn_channels, bifpn_blocks)\n","        self.depth_decoder = RTMonoDepthDecoder()\n","        self.detection_heads = MultiTaskHeads(num_classes, bifpn_channels)\n","        self.post_processor = DetectionPostProcessor(num_classes, confidence_threshold, max_detections)\n","\n","    def forward(self, images, K=None, inference_mode=False, mode=\"train\"):\n","        # Backbone features\n","        backbone_features = self.encoder(images)  # outs\n","\n","        # Depth estimation\n","        depth_maps = self.depth_decoder(backbone_features, inference_mode)\n","\n","        # Object detection via BiFPN\n","        bifpn_features = self.bifpn(backbone_features)  # [P3, P4, P5, P6, P7]\n","        detection_outputs = self.detection_heads(bifpn_features)\n","\n","        if mode == \"train\":\n","            # For training: return raw outputs\n","            # Raw format: [cls_probs(num_classes), x, y, w, h, depth]\n","            raw_detections = self.post_processor(\n","                detection_outputs['classification'],\n","                detection_outputs['regression'],\n","                detection_outputs['depth']\n","            )\n","\n","            outputs = {\n","                'depth_maps': depth_maps,  # Multi-scale depth maps for depth loss\n","                'raw_detections': raw_detections,  # [B, num_classes+5, H, W] for detection loss\n","                'detection_depth': detection_outputs['depth'],  # Single scale detection depth\n","                'classification': detection_outputs['classification'],  # For individual losses if needed\n","                'regression': detection_outputs['regression']  # For individual losses if needed\n","            }\n","        else:\n","            # For inference: return post-processed detections\n","            final_detections, scores = self.post_processor(\n","                detection_outputs['classification'],\n","                detection_outputs['regression'],\n","                detection_outputs['depth']\n","            )\n","\n","            outputs = {\n","                'depth_maps': depth_maps,\n","                'detections': final_detections,  # [B, max_detections, 6] - [cls, x, y, w, h, depth]\n","                'scores': scores,  # [B, max_detections] - confidence scores\n","                'detection_depth': detection_outputs['depth']\n","            }\n","\n","        return outputs"]},{"cell_type":"code","execution_count":7,"id":"YEwibMmfXRU3","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1754419778671,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"YEwibMmfXRU3"},"outputs":[],"source":["class MultiTaskLoss(nn.Module):\n","    \"\"\"Multi-task loss with automatic weighting\"\"\"\n","    def __init__(self, num_tasks=4, loss_weights=None):\n","        super(MultiTaskLoss, self).__init__()\n","        self.num_tasks = num_tasks\n","\n","        # Loss functions for each task\n","        self.classification_loss = nn.CrossEntropyLoss(weight=class_weights)\n","        self.regression_loss = nn.MSELoss()\n","        self.depth_loss = nn.L1Loss()\n","        self.depth_map_loss = nn.L1Loss()\n","\n","        # Learnable loss weights (uncertainty weighting)\n","        if loss_weights is None:\n","            self.log_vars = nn.Parameter(torch.zeros(num_tasks))\n","        else:\n","            self.register_buffer('loss_weights', torch.tensor(loss_weights))\n","            self.log_vars = None\n","\n","    def forward(self, outputs, targets):\n","        \"\"\"\n","        outputs: model outputs dict with spatial outputs\n","        targets: ground truth dict with object-level targets from KITTI\n","        \"\"\"\n","        losses = {}\n","\n","        # Get batch size\n","        batch_size = targets['classification'].shape[0]\n","\n","        # Classification loss - need to handle spatial vs object-level mismatch\n","        if 'classification' in outputs and 'classification' in targets:\n","            # outputs['classification']: [B, 8, 256, 256] - spatial classification map\n","            # targets['classification']: [B, num_objects] - object class indices\n","\n","            # For now, use a simplified approach - average pool the spatial output\n","            cls_output = outputs['classification'].mean(dim=[2, 3])  # [B, 8]\n","\n","            # Create target for each batch - use first object's class (simplified)\n","            cls_targets = []\n","            for b in range(batch_size):\n","                if len(targets['classification'][b]) > 0:\n","                    # Use first valid object's class\n","                    valid_objects = targets['classification'][b][targets['classification'][b] >= 0]\n","                    if len(valid_objects) > 0:\n","                        cls_targets.append(valid_objects[0])\n","                    else:\n","                        cls_targets.append(torch.tensor(0))  # Default class\n","                else:\n","                    cls_targets.append(torch.tensor(0))  # Default class\n","\n","            cls_targets = torch.stack(cls_targets).to(cls_output.device)\n","            cls_loss = self.classification_loss(cls_output, cls_targets.long())\n","            losses['classification'] = cls_loss\n","\n","        # Regression loss - similar handling needed\n","        if 'regression' in outputs and 'regression' in targets:\n","            # outputs['regression']: [B, 4, 256, 256] - spatial regression map\n","            # targets['regression']: [B, num_objects, 4] - object bbox coordinates\n","\n","            reg_output = outputs['regression'].mean(dim=[2, 3])  # [B, 4]\n","\n","            # Use first object's bbox (simplified)\n","            reg_targets = []\n","            for b in range(batch_size):\n","                if len(targets['regression'][b]) > 0:\n","                    reg_targets.append(targets['regression'][b][0])  # First object\n","                else:\n","                    reg_targets.append(torch.zeros(4))  # Default bbox\n","\n","            reg_targets = torch.stack(reg_targets).to(reg_output.device)\n","            reg_loss = self.regression_loss(reg_output, reg_targets.float())\n","            losses['regression'] = reg_loss\n","\n","        # Detection depth loss\n","        if 'detection_depth' in outputs and 'depth' in targets:\n","            # outputs['detection_depth']: [B, 1, 256, 256] - spatial depth map\n","            # targets['depth']: [B, num_objects, 1] - object depth values\n","\n","            depth_output = outputs['detection_depth'].mean(dim=[2, 3])  # [B, 1]\n","\n","            # Use first object's depth (simplified)\n","            depth_targets = []\n","            for b in range(batch_size):\n","                if len(targets['depth'][b]) > 0:\n","                    # Filter out NaN values\n","                    valid_depths = targets['depth'][b][~torch.isnan(targets['depth'][b])]\n","                    if len(valid_depths) > 0:\n","                        depth_targets.append(valid_depths[0])\n","                    else:\n","                        depth_targets.append(torch.tensor([0.0]))\n","                else:\n","                    depth_targets.append(torch.tensor([0.0]))\n","\n","            depth_targets = torch.stack(depth_targets).to(depth_output.device)\n","\n","            # Only calculate loss for valid (non-NaN) targets\n","            valid_mask = ~torch.isnan(depth_targets)\n","            if valid_mask.any():\n","                depth_loss = self.depth_loss(depth_output[valid_mask], depth_targets[valid_mask].float())\n","            else:\n","                depth_loss = torch.tensor(0.0, device=depth_output.device)\n","\n","            losses['detection_depth'] = depth_loss\n","\n","        # Depth map loss (this is spatial to spatial, so direct comparison)\n","        if 'depth_maps' in outputs and 'depth_map' in targets:\n","            depth_map = outputs['depth_maps']['depth_2']  # [B, 1, 512, 512]\n","            target_depth_map = targets['depth_map']       # [B, 1, 512, 512]\n","\n","            depth_map_loss = self.depth_map_loss(depth_map, target_depth_map)\n","            losses['depth_map'] = depth_map_loss\n","\n","        # Combine losses\n","        if self.log_vars is not None:\n","            # Uncertainty weighting\n","            total_loss = 0\n","            for i, (task, loss) in enumerate(losses.items()):\n","                weight = torch.exp(-self.log_vars[i])\n","                total_loss += weight * loss + self.log_vars[i]\n","        else:\n","            # Fixed weighting\n","            total_loss = sum(self.loss_weights[i] * loss for i, loss in enumerate(losses.values()))\n","\n","        losses['total'] = total_loss\n","        return losses\n","\n","def train_model(model, train_loader, val_loader, num_epochs=100,\n","                learning_rate=1e-4, device='cuda', save_path='model_checkpoint.pth',\n","                log_interval=10, val_interval=5):\n","    \"\"\"\n","    Multi-task model training function\n","\n","    Args:\n","        model: The multi-task model\n","        train_loader: Training data loader\n","        val_loader: Validation data loader\n","        num_epochs: Number of training epochs\n","        learning_rate: Learning rate\n","        device: Device to train on\n","        save_path: Path to save best model\n","        log_interval: Logging interval\n","        val_interval: Validation interval\n","    \"\"\"\n","\n","    # Setup logging\n","    logging.basicConfig(level=logging.INFO)\n","    logger = logging.getLogger(__name__)\n","\n","    # Move model to device\n","    model = model.to(device)\n","\n","    # Loss function and optimizer\n","    criterion = MultiTaskLoss(num_tasks=4).to(device)\n","    optimizer = optim.AdamW(list(model.parameters()) + list(criterion.parameters()),\n","                           lr=learning_rate, weight_decay=1e-4)\n","\n","    # Learning rate scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n","                                                   factor=0.5, patience=10, verbose=True)\n","\n","    # Training history\n","    train_history = {'total': [], 'classification': [], 'regression': [],\n","                    'detection_depth': [], 'depth_map': []}\n","    val_history = {'total': [], 'classification': [], 'regression': [],\n","                  'detection_depth': [], 'depth_map': []}\n","\n","    best_val_loss = float('inf')\n","\n","    logger.info(f\"Starting training for {num_epochs} epochs\")\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_losses = {'total': 0, 'classification': 0, 'regression': 0,\n","                       'detection_depth': 0, 'depth_map': 0}\n","        train_samples = 0\n","\n","        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n","\n","        for batch_idx, (images, targets) in enumerate(pbar):\n","            # Move data to device\n","            images = images.to(device)\n","\n","            # Prepare targets for KITTI format\n","            # targets comes from kitti_collate_fn as a list of lists\n","            # Each inner list contains [category_num, x, y, w, h, disparity/depth] for each object\n","\n","            target_dict = {}\n","            batch_size = len(targets)\n","\n","            # Find max number of objects in the batch for padding\n","            max_objects = max(len(target_list) for target_list in targets) if targets else 1\n","\n","            # Initialize tensors\n","            cls_labels = torch.full((batch_size, max_objects), -1, dtype=torch.long)  # -1 for padding\n","            bbox_labels = torch.zeros((batch_size, max_objects, 4), dtype=torch.float32)\n","            depth_labels = torch.full((batch_size, max_objects, 1), float('nan'), dtype=torch.float32)\n","\n","            # Fill the tensors\n","            for b, target_list in enumerate(targets):\n","                for obj_idx, obj_data in enumerate(target_list):\n","                    if obj_idx < max_objects:\n","                        category_num, x, y, w, h, depth_or_disp = obj_data\n","                        cls_labels[b, obj_idx] = int(category_num)\n","                        bbox_labels[b, obj_idx] = torch.tensor([x, y, w, h], dtype=torch.float32)\n","\n","                        # Handle NaN values in depth/disparity\n","                        if not np.isnan(depth_or_disp):\n","                            depth_labels[b, obj_idx, 0] = depth_or_disp\n","\n","            # Move to device\n","            target_dict['classification'] = cls_labels.to(device)\n","            target_dict['regression'] = bbox_labels.to(device)\n","            target_dict['depth'] = depth_labels.to(device)\n","\n","            # Create dummy depth map for now (you should provide real depth maps)\n","            target_dict['depth_map'] = torch.zeros(batch_size, 1, 512, 512).to(device)\n","\n","            # Forward pass\n","            optimizer.zero_grad()\n","            outputs = model(images, mode=\"train\")\n","\n","            # Calculate losses\n","            losses = criterion(outputs, target_dict)\n","\n","            # Backward pass\n","            losses['total'].backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            # Update running losses\n","            batch_size = images.size(0)\n","            train_samples += batch_size\n","\n","            for key, loss in losses.items():\n","                train_losses[key] += loss.item() * batch_size\n","\n","            # Update progress bar\n","            pbar.set_postfix({\n","                'Total Loss': f\"{losses['total'].item():.4f}\",\n","                'Cls': f\"{losses.get('classification', torch.tensor(0)).item():.4f}\",\n","                'Reg': f\"{losses.get('regression', torch.tensor(0)).item():.4f}\",\n","                'Depth': f\"{losses.get('detection_depth', torch.tensor(0)).item():.4f}\",\n","                'DMap': f\"{losses.get('depth_map', torch.tensor(0)).item():.4f}\"\n","            })\n","\n","        # Calculate average training losses\n","        for key in train_losses:\n","            train_losses[key] /= train_samples\n","            train_history[key].append(train_losses[key])\n","\n","        # Validation phase\n","        if (epoch + 1) % val_interval == 0:\n","            model.eval()\n","            val_losses = {'total': 0, 'classification': 0, 'regression': 0,\n","                         'detection_depth': 0, 'depth_map': 0}\n","            val_samples = 0\n","\n","            with torch.no_grad():\n","                for images, targets in val_loader:\n","                    images = images.to(device)\n","\n","                    # Prepare targets for KITTI format - validation phase\n","                    target_dict = {}\n","                    batch_size = len(targets)\n","\n","                    # Find max number of objects in the batch for padding\n","                    max_objects = max(len(target_list) for target_list in targets) if targets else 1\n","\n","                    # Initialize tensors\n","                    cls_labels = torch.full((batch_size, max_objects), -1, dtype=torch.long)\n","                    bbox_labels = torch.zeros((batch_size, max_objects, 4), dtype=torch.float32)\n","                    depth_labels = torch.full((batch_size, max_objects, 1), float('nan'), dtype=torch.float32)\n","\n","                    # Fill the tensors\n","                    for b, target_list in enumerate(targets):\n","                        for obj_idx, obj_data in enumerate(target_list):\n","                            if obj_idx < max_objects:\n","                                category_num, x, y, w, h, depth_or_disp = obj_data\n","                                cls_labels[b, obj_idx] = int(category_num)\n","                                bbox_labels[b, obj_idx] = torch.tensor([x, y, w, h], dtype=torch.float32)\n","\n","                                if not np.isnan(depth_or_disp):\n","                                    depth_labels[b, obj_idx, 0] = depth_or_disp\n","\n","                    # Move to device\n","                    target_dict['classification'] = cls_labels.to(device)\n","                    target_dict['regression'] = bbox_labels.to(device)\n","                    target_dict['depth'] = depth_labels.to(device)\n","                    target_dict['depth_map'] = torch.zeros(batch_size, 1, 512, 512).to(device)\n","\n","                    outputs = model(images, mode=\"train\")\n","                    losses = criterion(outputs, target_dict)\n","\n","                    batch_size = images.size(0)\n","                    val_samples += batch_size\n","\n","                    for key, loss in losses.items():\n","                        val_losses[key] += loss.item() * batch_size\n","\n","            # Calculate average validation losses\n","            for key in val_losses:\n","                val_losses[key] /= val_samples\n","                val_history[key].append(val_losses[key])\n","\n","            # Update learning rate\n","            scheduler.step(val_losses['total'])\n","\n","            # Save best model\n","            if val_losses['total'] < best_val_loss:\n","                best_val_loss = val_losses['total']\n","                torch.save({\n","                    'epoch': epoch + 1,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'criterion_state_dict': criterion.state_dict(),\n","                    'best_val_loss': best_val_loss,\n","                    'train_history': train_history,\n","                    'val_history': val_history\n","                }, save_path)\n","                logger.info(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n","\n","        # Logging\n","        if (epoch + 1) % log_interval == 0:\n","            logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n","            logger.info(f\"Train - Total: {train_losses['total']:.4f}, \"\n","                       f\"Cls: {train_losses['classification']:.4f}, \"\n","                       f\"Reg: {train_losses['regression']:.4f}, \"\n","                       f\"Depth: {train_losses['detection_depth']:.4f}, \"\n","                       f\"DMap: {train_losses['depth_map']:.4f}\")\n","\n","            if val_losses['total'] > 0:\n","                logger.info(f\"Val - Total: {val_losses['total']:.4f}, \"\n","                           f\"Cls: {val_losses['classification']:.4f}, \"\n","                           f\"Reg: {val_losses['regression']:.4f}, \"\n","                           f\"Depth: {val_losses['detection_depth']:.4f}, \"\n","                           f\"DMap: {val_losses['depth_map']:.4f}\")\n","\n","    logger.info(\"Training completed!\")\n","    return train_history, val_history"]},{"cell_type":"code","execution_count":18,"id":"80WXSwtyTQt4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3173995,"status":"ok","timestamp":1754415608481,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"},"user_tz":-180},"id":"80WXSwtyTQt4","outputId":"3729f5ac-fe2d-47a7-fa81-a06ec1527d9f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","Epoch 1/15:   0%|          | 0/155 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n","Epoch 1/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=1.6404, Cls=1.1387, Reg=0.0330, Depth=0.4628, DMap=0.0101]\n","Epoch 2/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=4.1881, Cls=3.7105, Reg=0.0560, Depth=0.4259, DMap=0.0087]\n","Epoch 3/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=2.9803, Cls=2.5066, Reg=0.0329, Depth=0.4509, DMap=0.0052]\n","Epoch 4/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=0.1777, Cls=0.0912, Reg=0.0136, Depth=0.0825, DMap=0.0065]\n","Epoch 5/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=6.0073, Cls=5.6155, Reg=0.0211, Depth=0.4033, DMap=0.0018]\n","Epoch 6/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=3.7603, Cls=3.4905, Reg=0.0435, Depth=0.2598, DMap=0.0011]\n","Epoch 7/15: 100%|██████████| 155/155 [03:16<00:00,  1.26s/it, Total Loss=3.8976, Cls=3.3883, Reg=0.0098, Depth=0.5354, DMap=0.0012]\n","Epoch 8/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=2.2341, Cls=2.0295, Reg=0.0448, Depth=0.1900, DMap=0.0094]\n","Epoch 9/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.4938, Cls=0.0188, Reg=0.0669, Depth=0.4380, DMap=0.0003]\n","Epoch 10/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.4515, Cls=0.0165, Reg=0.0218, Depth=0.4467, DMap=0.0007]\n","Epoch 11/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=2.9088, Cls=2.7551, Reg=0.0085, Depth=0.2008, DMap=0.0033]\n","Epoch 12/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=6.0465, Cls=5.6639, Reg=0.0414, Depth=0.4198, DMap=0.0001]\n","Epoch 13/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=3.5110, Cls=3.3666, Reg=0.0274, Depth=0.1900, DMap=0.0003]\n","Epoch 14/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=5.3652, Cls=4.9841, Reg=0.0406, Depth=0.4259, DMap=0.0002]\n","Epoch 15/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.2214, Cls=0.0160, Reg=0.0311, Depth=0.2284, DMap=0.0021]\n"]}],"source":["model = CompleteMultiTaskModel(İsPretreained=True,num_classes=8)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_history, val_history = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    num_epochs=15,\n","    learning_rate=1e-5,\n","    device=device,\n","    save_path='best_multitask_model.pth',\n","    val_interval=1,\n","    log_interval=2\n",")"]},{"cell_type":"markdown","source":["Bundan önceki eğitimde dataset 'önemsiz' nesne türünü sınıf olarak saymıyordu.\n","İkinci eğitim sonucuna bakacak olursak sınıflandırma hatası yine dengesiz gitmesine rağmen ölçeği büyük oranda küçülmüştür."],"metadata":{"id":"sRL_WxEPycWL"},"id":"sRL_WxEPycWL"},{"cell_type":"code","source":["model = CompleteMultiTaskModel(İsPretreained=True,num_classes=8)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_history, val_history = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    num_epochs=15,\n","    learning_rate=1e-5,\n","    device=device,\n","    save_path='best_multitask_model.pth',\n","    val_interval=1,\n","    log_interval=2\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNp3cKjeaU2s","executionInfo":{"status":"ok","timestamp":1754422948509,"user_tz":-180,"elapsed":3169837,"user":{"displayName":"MehmetEmin ULUDAĞ","userId":"01242540695861601452"}},"outputId":"ea68991c-3a2c-443f-c228-7e29a70af5e8"},"id":"QNp3cKjeaU2s","execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","Epoch 1/15:   0%|          | 0/155 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n","Epoch 1/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=1.7065, Cls=1.4431, Reg=0.0172, Depth=0.2170, DMap=0.0340]\n","Epoch 2/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=0.9411, Cls=0.6650, Reg=0.0138, Depth=0.2639, DMap=0.0069]\n","Epoch 3/15: 100%|██████████| 155/155 [03:16<00:00,  1.27s/it, Total Loss=0.6951, Cls=0.4388, Reg=0.0412, Depth=0.2278, DMap=0.0003]\n","Epoch 4/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=1.0340, Cls=0.6556, Reg=0.0076, Depth=0.3809, DMap=0.0066]\n","Epoch 5/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.3663, Cls=0.0150, Reg=0.0346, Depth=0.3372, DMap=0.0013]\n","Epoch 6/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.4562, Cls=0.0057, Reg=0.0096, Depth=0.4655, DMap=0.0008]\n","Epoch 7/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.3889, Cls=0.0124, Reg=0.0423, Depth=0.3647, DMap=0.0000]\n","Epoch 8/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.6542, Cls=0.2184, Reg=0.0184, Depth=0.4509, DMap=0.0002]\n","Epoch 9/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.4118, Cls=0.0066, Reg=0.0268, Depth=0.4172, DMap=0.0000]\n","Epoch 10/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.2181, Cls=0.0191, Reg=0.0053, Depth=0.2375, DMap=0.0026]\n","Epoch 11/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.8244, Cls=0.5234, Reg=0.0407, Depth=0.3077, DMap=0.0000]\n","Epoch 12/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.9916, Cls=0.6587, Reg=0.0114, Depth=0.3697, DMap=0.0002]\n","Epoch 13/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.4400, Cls=0.1862, Reg=0.0477, Depth=0.2599, DMap=0.0000]\n","Epoch 14/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.0938, Cls=0.0131, Reg=0.0088, Depth=0.1319, DMap=0.0000]\n","Epoch 15/15: 100%|██████████| 155/155 [03:15<00:00,  1.26s/it, Total Loss=0.9777, Cls=0.6499, Reg=0.0048, Depth=0.3777, DMap=0.0000]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"O188fBFNd9u7"},"id":"O188fBFNd9u7","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"PytTorch","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":5}