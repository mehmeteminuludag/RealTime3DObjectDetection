{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f44ce32d",
      "metadata": {
        "id": "f44ce32d"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "from math import inf\n",
        "from posixpath import defpath\n",
        "import wandb\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "import optuna\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision.ops import nms\n",
        "import torchvision.ops as ops\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.patches as patches\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from collections import Counter\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,average_precision_score\n",
        "from typing import Dict, Any, Optional\n",
        "from torchvision import transforms , models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "HFHhIGESGWmk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFHhIGESGWmk",
        "outputId": "6c39cb44-ae38-46eb-c097-bdcd47776763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "src = \"/content/drive/MyDrive/kitti2012\"\n",
        "dst = \"/content\"\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "!cp -r \"$src\" \"$dst\"\n",
        "\n",
        "#src = \"/content/drive/MyDrive/vkitti_sample (1)\"\n",
        "#dst = \"/content\"\n",
        "#os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "#!cp -r \"$src\" \"$dst\"\n",
        "\n",
        "wandb.login()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KITTI_Dataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None, mode='train'):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.mode = mode  # 'train', 'val', veya 'test'\n",
        "        self.classes = ['Car', 'Van', 'Truck', 'Pedestrian', 'Cyclist', 'Tram', 'Misc']\n",
        "        self.class_map = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.data = []\n",
        "\n",
        "        # KITTI görsel boyutları - normalizasyon için\n",
        "        self.img_width = 256  # Model giriş boyutu\n",
        "        self.img_height = 256\n",
        "        self.original_width = 1242  # Orijinal KITTI boyutu\n",
        "        self.original_height = 375\n",
        "        # Varsayılan transform: yeniden boyutlandırma\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "        self.max_depth = 80.0  # KITTI max derinlik\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            # Test modunda sadece görselleri yükle\n",
        "            data_path =data_path+\"/testing/colored_0\"\n",
        "            file_names = os.listdir(data_path)\n",
        "            for fname in file_names:\n",
        "                if fname.endswith('.png'):\n",
        "                    img_path = os.path.join(data_path, fname)\n",
        "                    self.data.append(img_path)\n",
        "        else :\n",
        "          image_dir = os.path.join(data_path, 'training', 'colored_0')\n",
        "          label_dir = os.path.join(data_path, 'training', 'label_2')\n",
        "          disp_dir = os.path.join(data_path, 'training', 'disp_noc')\n",
        "\n",
        "          file_names = os.listdir(image_dir)\n",
        "          for fname in file_names:\n",
        "              if fname.endswith('.png'):\n",
        "                  scene_id = fname.split('_')[0]\n",
        "                  img_path = os.path.join(image_dir, fname)\n",
        "                  label_path = os.path.join(label_dir, f'{scene_id}.txt')\n",
        "                  disp_path = os.path.join(disp_dir, f'{scene_id}_10.png')\n",
        "                  if os.path.exists(label_path) and os.path.exists(disp_path):\n",
        "                      self.data.append((img_path, label_path, disp_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def convert_labels(self, label_file):\n",
        "      labels = []\n",
        "      with open(label_file, 'r') as f:\n",
        "          for line in f:\n",
        "              parts = line.strip().split()\n",
        "              category = parts[0]\n",
        "              if category in self.class_map and category != 'DontCare':\n",
        "                  x1, y1, x2, y2 = map(float, parts[4:8])\n",
        "                  # Orijinal boyutlara göre normalize et\n",
        "                  x1_norm = x1 / self.original_width\n",
        "                  y1_norm = y1 / self.original_height\n",
        "                  x2_norm = x2 / self.original_width\n",
        "                  y2_norm = y2 / self.original_height\n",
        "\n",
        "                  category_num = self.class_map[category]\n",
        "                  labels.append([category_num, x1_norm, y1_norm, x2_norm, y2_norm])\n",
        "      return labels\n",
        "\n",
        "    def load_disparity(self, disp_path):#konum haritasının olduğu image yüklenir\n",
        "        disp_map = cv2.imread(disp_path, cv2.IMREAD_UNCHANGED) / 256.0\n",
        "        return disp_map\n",
        "\n",
        "    def calculate_depth(self, disp_map):\n",
        "      baseline = 0.54\n",
        "      focal_length = 721.5377 * (256 / 1242)  # Odak uzaklığını ölçeklendir\n",
        "      depth = (baseline * focal_length) / (disp_map + 1e-6)\n",
        "      depth = np.clip(depth, 0, self.max_depth)\n",
        "      normalized_depth = depth / self.max_depth\n",
        "      # 256x256'ya yeniden boyutlandır\n",
        "      normalized_depth = cv2.resize(normalized_depth, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
        "      return normalized_depth\n",
        "\n",
        "    def get_depth_at_box(self, depth_map, x, y, w, h):#her nesnenin ortalama mesafesi için box içerisindeki merkez piksel depthi alınır\n",
        "\n",
        "        x_pixel = int(x * self.img_width)\n",
        "        y_pixel = int(y * self.img_height)\n",
        "\n",
        "        # Sınır kontrolü\n",
        "        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n",
        "        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n",
        "\n",
        "        if depth_map[y_pixel, x_pixel] == 0:\n",
        "            return 0.0\n",
        "        return depth_map[y_pixel, x_pixel]\n",
        "\n",
        "    def get_disparity_at_box(self, disp_map, x, y, w, h):#her nesnenin box içerisindeki konum değerini hesaplar\n",
        "\n",
        "\n",
        "        x_pixel = int(x * self.img_width)\n",
        "        y_pixel = int(y * self.img_height)\n",
        "\n",
        "        # Sınır kontrolü\n",
        "        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n",
        "        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n",
        "\n",
        "        if disp_map[y_pixel, x_pixel] == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Disparite değerini de normalize et (max disparite ~300 civarı)\n",
        "        max_disparity = 300.0\n",
        "        normalized_disparity = disp_map[y_pixel, x_pixel] / max_disparity\n",
        "        return np.clip(normalized_disparity, 0, 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if self.mode != 'test':\n",
        "          labels_with_depth = []\n",
        "          img_path, label_path, disp_path = self.data[idx]\n",
        "          image = Image.open(img_path).convert('RGB')\n",
        "          labels = self.convert_labels(label_path)\n",
        "          disp_map = self.load_disparity(disp_path)\n",
        "          depth_map = self.calculate_depth(disp_map)  # Derinlik haritası\n",
        "          for label in labels:\n",
        "              category_num, x1, y1, x2, y2 = label\n",
        "              # Bounding box merkezinden derinlik değerini al\n",
        "              center_x = (x1 + x2) / 2\n",
        "              center_y = (y1 + y2) / 2\n",
        "              depth = self.get_depth_at_box(depth_map, center_x, center_y, x2 - x1, y2 - y1)\n",
        "              labels_with_depth.append([category_num, x1, y1, x2, y2, depth])\n",
        "          if self.transform:\n",
        "              image = self.transform(image)\n",
        "          output_labels = torch.tensor(labels_with_depth, dtype=torch.float32)\n",
        "          return image, output_labels\n",
        "      else:\n",
        "          img_path = self.data[idx]\n",
        "          image = Image.open(img_path).convert('RGB')\n",
        "          if self.transform:\n",
        "              image = self.transform(image)\n",
        "          return image\n",
        "\n",
        "        #eğitimde doğrudan konum değerleri ile kayıp hesaplanırken test aşamasında direkt mesafe hesaplanabilir\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    \"\"\"Test mode için basit collate function\"\"\"\n",
        "    images = batch  # batch sadece image tensor'larının listesi\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images\n",
        "\n",
        "def kitti_collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images, labels\n",
        "\n",
        "def analyze_dataset(train_dataset, device):\n",
        "    \"\"\"\n",
        "    Veri seti analizi ve görselleştirme fonksiyonu - Optimize edilmiş versiyon\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    from collections import Counter\n",
        "\n",
        "    # === 0. Veri Setinden Özellikleri Çıkarma - TEK DÖNGÜ İLE ===\n",
        "    print(f\"Veri seti boyutu: {len(train_dataset)} örnek\")\n",
        "\n",
        "    # Tüm listeleri tek seferde oluştur\n",
        "    cls_labels = []\n",
        "    depth_maps = []\n",
        "    bboxes = []\n",
        "    objects_per_image = []\n",
        "\n",
        "\n",
        "    for i in range(len(train_dataset)):\n",
        "        image_data = train_dataset[i][1]  # Bir kez al\n",
        "        objects_per_image.append(len(image_data))\n",
        "\n",
        "        # Bu görüntüdeki tüm objeleri işle\n",
        "        for obj in image_data:\n",
        "            cls_labels.append(obj[0])           # sınıf\n",
        "            depth_maps.append(obj[5])           # depth\n",
        "            bboxes.append(obj[1:5])             # [x1,y1,x2,y2]\n",
        "    total_objects = len(cls_labels)\n",
        "    print(f\"Toplam obje sayısı: {total_objects}\")\n",
        "    print(f\"Toplam bbox sayısı: {len(bboxes)}\")\n",
        "    print(f\"Toplam depth değeri: {len(depth_maps)}\")\n",
        "\n",
        "    # === 1. Sınıf Dağılımı Analizi ===\n",
        "    cls_distribution = Counter(cls_labels)\n",
        "    sorted_cls_distribution = sorted(cls_distribution.items(), key=lambda x: x[0])\n",
        "\n",
        "    # Class weights hesaplama\n",
        "    class_weights = torch.tensor([count for _, count in sorted_cls_distribution],\n",
        "                                dtype=torch.float32).to(device)\n",
        "    # === 2. BBox Analizi - VEKTÖRLEŞTİRİLMİŞ ===\n",
        "    # NumPy array'e çevir hızlı işlem için\n",
        "    bboxes_array = np.array(bboxes)\n",
        "\n",
        "    # Format: [x1, y1, x2, y2]\n",
        "    bbox_widths = bboxes_array[:, 2] - bboxes_array[:, 0]   # x2 - x1\n",
        "    bbox_heights = bboxes_array[:, 3] - bboxes_array[:, 1]  # y2 - y1\n",
        "    bbox_areas = bbox_widths * bbox_heights\n",
        "\n",
        "    # Sıfıra bölme kontrolü ile aspect ratio\n",
        "    bbox_aspect_ratios = np.divide(bbox_widths, bbox_heights,\n",
        "                                  out=np.zeros_like(bbox_widths),\n",
        "                                  where=bbox_heights!=0)\n",
        "\n",
        "    # === 3. Görsel Başına Obje Sayısı Analizi - ZATEN HAZIR ===\n",
        "    # objects_per_image yukarıda hesaplandı\n",
        "\n",
        "    # === 4. Depth Analizi - VEKTÖRLEŞTİRİLMİŞ ===\n",
        "    depth_array = np.array(depth_maps)\n",
        "    valid_mask = ~np.isnan(depth_array)\n",
        "    valid_depths = depth_array[valid_mask]\n",
        "    invalid_depths = np.sum(~valid_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # === 5. Görselleştirme ===\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # Sınıf dağılımı pasta grafiği\n",
        "    plt.subplot(3, 4, 1)\n",
        "    labels, values = zip(*sorted_cls_distribution)\n",
        "    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "    plt.title('Sınıf Dağılımı', fontsize=12, fontweight='bold')\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # Sınıf dağılımı bar grafiği\n",
        "    plt.subplot(3, 4, 2)\n",
        "    plt.bar(labels, values, alpha=0.7, edgecolor='black')\n",
        "    plt.title('Sınıf Dağılımı (Bar Chart)', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Sınıf')\n",
        "    plt.ylabel('Örnek Sayısı')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Görsel başına obje sayısı histogramı\n",
        "    plt.subplot(3, 4, 3)\n",
        "    plt.hist(objects_per_image, bins=range(1, max(objects_per_image)+2),\n",
        "             alpha=0.7, edgecolor='black', color='skyblue')\n",
        "    plt.title('Görsel Başına Obje Sayısı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Obje Sayısı')\n",
        "    plt.ylabel('Görsel Sayısı')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox genişlik histogramı\n",
        "    plt.subplot(3, 4, 4)\n",
        "    plt.hist(bbox_widths, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
        "    plt.title('BBox Genişlik Dağılımı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Genişlik')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox yükseklik histogramı\n",
        "    plt.subplot(3, 4, 5)\n",
        "    plt.hist(bbox_heights, bins=50, alpha=0.7, edgecolor='black', color='red')\n",
        "    plt.title('BBox Yükseklik Dağılımı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Yükseklik')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox alan histogramı\n",
        "    plt.subplot(3, 4, 6)\n",
        "    plt.hist(bbox_areas, bins=50, alpha=0.7, edgecolor='black', color='purple')\n",
        "    plt.title('BBox Alan Dağılımı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Alan')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox aspect ratio histogramı\n",
        "    plt.subplot(3, 4, 7)\n",
        "    plt.hist(bbox_aspect_ratios, bins=50, alpha=0.7, edgecolor='black', color='brown')\n",
        "    plt.title('BBox En/Boy Oranı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('En/Boy Oranı')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Depth değerleri histogramı\n",
        "    plt.subplot(3, 4, 8)\n",
        "    if valid_depths:\n",
        "        plt.hist(valid_depths, bins=50, alpha=0.7, edgecolor='black', color='lightgreen')\n",
        "    plt.title('Depth Değerleri Dağılımı', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Normalize Edilmiş Depth')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Depth box plot\n",
        "    plt.subplot(3, 4, 9)\n",
        "    if valid_depths:\n",
        "        plt.boxplot(valid_depths, vert=True)\n",
        "    plt.title('Depth Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Normalize Edilmiş Depth')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Obje sayısı box plot\n",
        "    plt.subplot(3, 4, 10)\n",
        "    plt.boxplot(objects_per_image, vert=True)\n",
        "    plt.title('Obje Sayısı Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Obje Sayısı')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox alan box plot\n",
        "    plt.subplot(3, 4, 11)\n",
        "    plt.boxplot(bbox_areas, vert=True)\n",
        "    plt.title('BBox Alan Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Alan')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox aspect ratio box plot\n",
        "    plt.subplot(3, 4, 12)\n",
        "    plt.boxplot(bbox_aspect_ratios, vert=True)\n",
        "    plt.title('En/Boy Oranı Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('En/Boy Oranı')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # === 6. İstatistik Raporları ===\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DATASET ANALİZ RAPORU\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Genel bilgiler\n",
        "    print(f\"\\n📋 GENEL BİLGİLER:\")\n",
        "    print(f\"Toplam görsel sayısı:    {len(train_dataset):6d}\")\n",
        "    print(f\"Toplam obje sayısı:      {len(cls_labels):6d}\")\n",
        "    print(f\"Toplam bbox sayısı:      {len(bboxes):6d}\")\n",
        "\n",
        "    # Sınıf dağılımı raporu\n",
        "    print(f\"\\n SINIF DAĞILIMI:\")\n",
        "    print(f\"Toplam sınıf sayısı: {len(cls_distribution)}\")\n",
        "    for label, count in sorted_cls_distribution:\n",
        "        percentage = (count / sum(cls_distribution.values())) * 100\n",
        "        print(f\"  Sınıf {label}: {count:4d} örnek ({percentage:5.1f}%)\")\n",
        "\n",
        "    # BBox analiz raporu\n",
        "    print(f\"\\n BBOX ANALİZ RAPORU:\")\n",
        "    print(f\"Ortalama genişlik:       {np.mean(bbox_widths):8.2f}\")\n",
        "    print(f\"Ortalama yükseklik:      {np.mean(bbox_heights):8.2f}\")\n",
        "    print(f\"Ortalama alan:           {np.mean(bbox_areas):8.2f}\")\n",
        "    print(f\"Ortalama en/boy oranı:   {np.mean(bbox_aspect_ratios):8.2f}\")\n",
        "    print(f\"Min genişlik:            {np.min(bbox_widths):8.2f}\")\n",
        "    print(f\"Max genişlik:            {np.max(bbox_widths):8.2f}\")\n",
        "    print(f\"Min yükseklik:           {np.min(bbox_heights):8.2f}\")\n",
        "    print(f\"Max yükseklik:           {np.max(bbox_heights):8.2f}\")\n",
        "    print(f\"Min alan:                {np.min(bbox_areas):8.2f}\")\n",
        "    print(f\"Max alan:                {np.max(bbox_areas):8.2f}\")\n",
        "\n",
        "    # Obje sayısı raporu\n",
        "    print(f\"\\n OBJE SAYISI İSTATİSTİKLERİ:\")\n",
        "    print(f\"Ortalama obje sayısı: {np.mean(objects_per_image):6.2f}\")\n",
        "    print(f\"Medyan obje sayısı:   {np.median(objects_per_image):6.2f}\")\n",
        "    print(f\"Maksimum obje sayısı: {max(objects_per_image):6d}\")\n",
        "    print(f\"Minimum obje sayısı:  {min(objects_per_image):6d}\")\n",
        "    print(f\"Standart sapma:       {np.std(objects_per_image):6.2f}\")\n",
        "\n",
        "    # Depth raporu\n",
        "    print(f\"\\n DEPTH ANALİZ RAPORU:\")\n",
        "    print(f\"Toplam depth değeri:     {len(depth_maps):6d}\")\n",
        "    print(f\"Geçerli depth değeri:    {len(valid_depths):6d}\")\n",
        "    print(f\"Geçersiz depth (NaN):    {invalid_depths:6d}\")\n",
        "    print(f\"Depth geçerlilik oranı:  {len(valid_depths)/len(depth_maps)*100:6.1f}%\")\n",
        "\n",
        "    if valid_depths:\n",
        "        print(f\"\\n DEPTH İSTATİSTİKLERİ:\")\n",
        "        for key, value in depth_stats.items():\n",
        "            print(f\"{key:>8}: {value:8.4f}\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return {\n",
        "        'class_weights': class_weights,\n",
        "        'cls_distribution': sorted_cls_distribution,\n",
        "        'bbox_stats': {\n",
        "            'mean_width': np.mean(bbox_widths),\n",
        "            'mean_height': np.mean(bbox_heights),\n",
        "            'mean_area': np.mean(bbox_areas),\n",
        "            'mean_aspect_ratio': np.mean(bbox_aspect_ratios),\n",
        "            'width_std': np.std(bbox_widths),\n",
        "            'height_std': np.std(bbox_heights),\n",
        "            'area_std': np.std(bbox_areas)\n",
        "        },\n",
        "        'objects_per_image_stats': {\n",
        "            'mean': np.mean(objects_per_image),\n",
        "            'median': np.median(objects_per_image),\n",
        "            'max': max(objects_per_image),\n",
        "            'min': min(objects_per_image),\n",
        "            'std': np.std(objects_per_image)\n",
        "        },\n",
        "        'depth_stats': depth_stats,\n",
        "        'depth_validity_ratio': len(valid_depths)/len(depth_maps)*100 if depth_maps else 0,\n",
        "        'total_images': len(train_dataset),\n",
        "        'total_objects': len(cls_labels)\n",
        "    }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100,\n",
        "                learning_rate=1e-4, device='cuda', save_path='model_checkpoint.pth',\n",
        "                early_stop_patience=3,scheduler_patience=10, scheduler_factor=0.5, class_weights=None, task_weights=None,\n",
        "                p_iou_threshold=0.5, n_iou_threshold=0.4):\n",
        "\n",
        "    # WandB config'e weights ekle\n",
        "    config = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": \"EfficientBasedMultiTask\",\n",
        "        \"dataset\": \"KITTI-2012\",\n",
        "        \"epochs\": num_epochs,\n",
        "    }\n",
        "\n",
        "    # Task weights'i config'e ekle\n",
        "    if task_weights:\n",
        "        config.update({\n",
        "            \"task_weights\": task_weights,\n",
        "            \"classification_weight\": task_weights.get(\"classification\", 1.0),\n",
        "            \"regression_weight\": task_weights.get(\"regression\", 1.0),\n",
        "            \"detection_depth_weight\": task_weights.get(\"detection_depth\", 1.0),\n",
        "            \"depth_map_weight\": task_weights.get(\"depth_map\", 1.0)\n",
        "        })\n",
        "\n",
        "    wandb.init(\n",
        "        entity=\"mehmeteminuludag-kirikkale-university\",\n",
        "        project=\"StajProjesi\",\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    criterion = MultiTaskCriterion(loss_weights=task_weights, pos_iou_threshold=p_iou_threshold, neg_iou_threshold=n_iou_threshold).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        list(model.parameters()) + list(criterion.parameters()),\n",
        "        lr=learning_rate, weight_decay=1e-5, eps=1e-8\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=scheduler_factor,\n",
        "        patience=scheduler_patience,\n",
        "        verbose=True\n",
        "    )\n",
        "    best_val_loss = 1.0\n",
        "    counter=0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_losses = {\n",
        "            'total': 0,\n",
        "            'classification': 0,\n",
        "            'regression': 0,\n",
        "            'depth': 0,\n",
        "            'depth_map': 0\n",
        "        }\n",
        "        train_metrics_accum = {\n",
        "            'Accuracy': 0,\n",
        "            'F1_score': 0,\n",
        "            'MSE': 0,\n",
        "            'RMSE': 0,\n",
        "            'mAP': 0,\n",
        "            'TotalLoss': 0,\n",
        "            'ClsLoss': 0\n",
        "        }\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Train')\n",
        "\n",
        "        for batch_idx, (images, targets) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            batch_size = len(targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model_outputs = model(images, mode=\"train\")\n",
        "            values = {}\n",
        "            values['cls_preds'] = model_outputs['cls_preds']\n",
        "            values['reg_preds'] = model_outputs['reg_preds']\n",
        "            values['anchors'] = model_outputs['anchors']\n",
        "            values['depth_pred'] = model_outputs['depth_pred']\n",
        "            values['targets'] = targets\n",
        "\n",
        "            losses, metrics = criterion(values)\n",
        "            losses['total'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Hataları biriktirir\n",
        "            for key in train_losses:\n",
        "                if key in losses:\n",
        "                    if isinstance(losses[key], torch.Tensor):\n",
        "                        train_losses[key] += losses[key].item() * batch_size\n",
        "                    else:\n",
        "                        train_losses[key] += losses[key] * batch_size\n",
        "\n",
        "            # Metrikleri biriktir\n",
        "            for key in train_metrics_accum:\n",
        "                if key in metrics:\n",
        "                    if isinstance(metrics[key], torch.Tensor):\n",
        "                        train_metrics_accum[key] += metrics[key].item() * batch_size\n",
        "                    else:\n",
        "                        train_metrics_accum[key] += metrics[key] * batch_size\n",
        "\n",
        "            # DÜZELTME 4: Memory cleanup\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Acc': f\"{train_metrics_accum['Accuracy']/((batch_idx+1)*batch_size): .3f}\",\n",
        "                'ClsLoss': f\"{train_losses['classification']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'F1': f\"{train_metrics_accum['F1_score']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'RMSE': f\"{train_metrics_accum['RMSE']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'mAP': f\"{train_metrics_accum['mAP']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'TotalLoss': f\"{train_losses['total']/((batch_idx+1)*batch_size):.3f}\"\n",
        "            })\n",
        "\n",
        "        # DÜZELTME 3: Loop dışına çıkarıldı\n",
        "        for key in train_losses:\n",
        "            train_losses[key] /= len(train_loader.dataset)\n",
        "\n",
        "        for key in train_metrics_accum:\n",
        "            train_metrics_accum[key] /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "\n",
        "        val_losses = {\n",
        "            'total': 0,\n",
        "            'classification': 0,\n",
        "            'regression': 0,\n",
        "            'depth': 0,\n",
        "            'depth_map': 0\n",
        "        }\n",
        "        val_metrics_accum = {\n",
        "            'Accuracy': 0,\n",
        "            'F1_score': 0,\n",
        "            'MSE': 0,\n",
        "            'RMSE': 0,\n",
        "            'mAP': 0,\n",
        "            'TotalLoss': 0,\n",
        "            'ClsLoss': 0\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            #model.train()\n",
        "            pbar2 = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation')\n",
        "\n",
        "            for batch_idx, (images, targets) in enumerate(pbar2):\n",
        "                images = images.to(device)\n",
        "                batch_size = len(targets)\n",
        "\n",
        "\n",
        "                model_outputs = model(images, mode=\"train\")\n",
        "                values = {}\n",
        "                values['cls_preds'] = model_outputs['cls_preds']\n",
        "                values['reg_preds'] = model_outputs['reg_preds']\n",
        "                values['anchors'] = model_outputs['anchors']\n",
        "                values['depth_pred'] = model_outputs['depth_pred']\n",
        "                values['targets'] = targets\n",
        "                losses, metrics = criterion(values)\n",
        "\n",
        "                # Hataları biriktirir\n",
        "                for key in val_losses:\n",
        "                    if key in losses:\n",
        "                        if isinstance(losses[key], torch.Tensor):\n",
        "                            val_losses[key] += losses[key].item() * batch_size\n",
        "                        else:\n",
        "                            val_losses[key] += losses[key] * batch_size\n",
        "\n",
        "                # Metrikleri biriktir\n",
        "                for key in val_metrics_accum:\n",
        "                    if key in metrics:\n",
        "                        if isinstance(metrics[key], torch.Tensor):\n",
        "                            val_metrics_accum[key] += metrics[key].item() * batch_size\n",
        "                        else:\n",
        "                            val_metrics_accum[key] += metrics[key] * batch_size\n",
        "\n",
        "                pbar2.set_postfix({\n",
        "                    'Acc': f\"{val_metrics_accum['Accuracy']/((batch_idx+1)*batch_size): .3f}\",\n",
        "                    'ClsLoss': f\"{val_losses['classification']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'F1': f\"{val_metrics_accum['F1_score']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'RMSE': f\"{val_metrics_accum['RMSE']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'mAP': f\"{val_metrics_accum['mAP']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'TotalLoss': f\"{val_losses['total']/((batch_idx+1)*batch_size):.3f}\"\n",
        "                })\n",
        "\n",
        "        # DÜZELTME 5: Validation loop dışına çıkarıldı\n",
        "        for key in val_losses:\n",
        "            val_losses[key] /= len(val_loader.dataset)\n",
        "\n",
        "        for key in val_metrics_accum:\n",
        "            val_metrics_accum[key] /= len(val_loader.dataset)\n",
        "\n",
        "\n",
        "        scheduler.step(val_losses['total'])\n",
        "\n",
        "        if float(val_losses['total']) < best_val_loss:\n",
        "          best_val_loss = float(val_losses['total'])\n",
        "          counter = 0  # İyileşme oldu, sıfırla\n",
        "          torch.save({\n",
        "              'epoch': epoch + 1,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'criterion_state_dict': criterion.state_dict()\n",
        "          }, save_path)\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"No improvement in val loss for {counter} epochs.\")\n",
        "\n",
        "        # wandb logları alınıyor\n",
        "        wandb.log({\n",
        "            \"val/accuracy\": val_metrics_accum['Accuracy'],\n",
        "            \"val/classification_loss\": val_losses['classification'],\n",
        "            \"val/f1_score\": val_metrics_accum['F1_score'],\n",
        "            \"val/rmse\": val_metrics_accum['RMSE'],\n",
        "            \"val/map\": val_metrics_accum['mAP'],\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        }, step=epoch+1)\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/accuracy\": train_metrics_accum['Accuracy'],\n",
        "            \"train/classification_loss\": train_losses['classification'],\n",
        "            \"train/f1_score\": train_metrics_accum['F1_score'],\n",
        "            \"train/rmse\": train_metrics_accum['RMSE'],\n",
        "            \"train/map\": train_metrics_accum['mAP'],\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        }, step=epoch+1)\n",
        "\n",
        "\n",
        "\n",
        "        if counter >= early_stop_patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    wandb.finish()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "\n",
        "    def _apply_memory_efficient_nms(self, boxes, scores, classes):\n",
        "        if len(boxes) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device)\n",
        "        unique_classes = torch.unique(classes)\n",
        "        all_keep_indices = []\n",
        "        for cls in unique_classes:\n",
        "            if cls == 8:  # Skip DontCare\n",
        "                continue\n",
        "            cls_mask = classes == cls\n",
        "            if cls_mask.sum() == 0:\n",
        "                continue\n",
        "            cls_boxes = boxes[cls_mask]\n",
        "            cls_scores = scores[cls_mask]\n",
        "            cls_indices = torch.where(cls_mask)[0]\n",
        "            abs_boxes = cls_boxes * self.img_size\n",
        "            keep_cls = nms(abs_boxes, cls_scores, self.nms_threshold)\n",
        "            all_keep_indices.append(cls_indices[keep_cls])\n",
        "        if len(all_keep_indices) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device)\n",
        "        final_keep_indices = torch.cat(all_keep_indices)\n",
        "        if len(final_keep_indices) > self.max_detections_per_image:\n",
        "            keep_scores = scores[final_keep_indices]\n",
        "            top_indices = torch.topk(keep_scores, self.max_detections_per_image)[1]\n",
        "            final_keep_indices = final_keep_indices[top_indices]\n",
        "        return final_keep_indices\n",
        "\n",
        "    def _get_depth_values(self, depth_map, boxes):\n",
        "        if depth_map is None:\n",
        "            return [0.0] * len(boxes)\n",
        "        H, W = depth_map.shape\n",
        "        depths = []\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box\n",
        "            x1_pix = int(x1 * W)\n",
        "            y1_pix = int(y1 * H)\n",
        "            x2_pix = int(x2 * W)\n",
        "            y2_pix = int(y2 * H)\n",
        "            x1_pix = max(0, min(x1_pix, W-1))\n",
        "            y1_pix = max(0, min(y1_pix, H-1))\n",
        "            x2_pix = max(0, min(x2_pix, W-1))\n",
        "            y2_pix = max(0, min(y2_pix, H-1))\n",
        "            center_x = (x1_pix + x2_pix) // 2\n",
        "            center_y = (y1_pix + y2_pix) // 2\n",
        "            depth_value = depth_map[center_y, center_x].item() if depth_map is not None else 0.0\n",
        "            depths.append(depth_value)\n",
        "        return depths\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        x_out = self.conv1(x_cat)\n",
        "        attention_map = self.sigmoid(x_out)\n",
        "        return x * attention_map\n",
        "\n",
        "class EncoderBackBone(nn.Module):\n",
        "    def __init__(self,İsPretreained=True):\n",
        "        super(EncoderBackBone,self).__init__()\n",
        "        efficient = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "        self.features = efficient.features\n",
        "        self.SAttention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):         # B,C,H,W\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        # Her iki frame için özellikler\n",
        "        for i, block in enumerate(self.features):\n",
        "            x = block(x)\n",
        "            if i > 2:  # C3'ten sonrası için Spatial Attention\n",
        "                x = x * self.SAttention(x)\n",
        "            if i in [3,5,7]:\n",
        "                out = F.interpolate(x, size=256, mode='bilinear', align_corners=False)\n",
        "                outs.append(out)\n",
        "        return outs\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                 stride, padding, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return self.swish(x)\n",
        "\n",
        "class BiFPNBlock(nn.Module):\n",
        "    def __init__(self, channels, epsilon=1e-4):\n",
        "        super(BiFPNBlock, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.channels = channels\n",
        "\n",
        "        # Convolution layers for each level\n",
        "        self.conv_p3 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p4 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p5 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p6 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p7 = DepthwiseSeparableConv(channels, channels)\n",
        "\n",
        "        # Weight parameters for feature fusion\n",
        "        self.w1 = nn.Parameter(torch.ones(2))\n",
        "        self.w2 = nn.Parameter(torch.ones(2))\n",
        "        self.w3 = nn.Parameter(torch.ones(2))\n",
        "        self.w4 = nn.Parameter(torch.ones(2))\n",
        "        self.w5 = nn.Parameter(torch.ones(3))\n",
        "        self.w6 = nn.Parameter(torch.ones(3))\n",
        "        self.w7 = nn.Parameter(torch.ones(3))\n",
        "        self.w8 = nn.Parameter(torch.ones(2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        P3, P4, P5, P6, P7 = inputs\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        w1 = F.relu(self.w1)\n",
        "        P6_td = (w1[0] * P6 + w1[1] * self.up_sampling(P7, P6.shape[-2:])) / (w1.sum() + self.epsilon)\n",
        "        P6_td = self.conv_p6(P6_td)\n",
        "\n",
        "        w2 = F.relu(self.w2)\n",
        "        P5_td = (w2[0] * P5 + w2[1] * self.up_sampling(P6_td, P5.shape[-2:])) / (w2.sum() + self.epsilon)\n",
        "        P5_td = self.conv_p5(P5_td)\n",
        "\n",
        "        w3 = F.relu(self.w3)\n",
        "        P4_td = (w3[0] * P4 + w3[1] * self.up_sampling(P5_td, P4.shape[-2:])) / (w3.sum() + self.epsilon)\n",
        "        P4_td = self.conv_p4(P4_td)\n",
        "\n",
        "        # Top-down pathway\n",
        "        w4 = F.relu(self.w4)\n",
        "        P3_out = (w4[0] * P3 + w4[1] * self.up_sampling(P4_td, P3.shape[-2:])) / (w4.sum() + self.epsilon)\n",
        "        P3_out = self.conv_p3(P3_out)\n",
        "\n",
        "        w5 = F.relu(self.w5)\n",
        "        P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * self.down_sampling(P3_out, P4.shape[-2:])) / (w5.sum() + self.epsilon)\n",
        "        P4_out = self.conv_p4(P4_out)\n",
        "\n",
        "        w6 = F.relu(self.w6)\n",
        "        P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * self.down_sampling(P4_out, P5.shape[-2:])) / (w6.sum() + self.epsilon)\n",
        "        P5_out = self.conv_p5(P5_out)\n",
        "\n",
        "        w7 = F.relu(self.w7)\n",
        "        P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * self.down_sampling(P5_out, P6.shape[-2:])) / (w7.sum() + self.epsilon)\n",
        "        P6_out = self.conv_p6(P6_out)\n",
        "\n",
        "        w8 = F.relu(self.w8)\n",
        "        P7_out = (w8[0] * P7 + w8[1] * self.down_sampling(P6_out, P7.shape[-2:])) / (w8.sum() + self.epsilon)\n",
        "        P7_out = self.conv_p7(P7_out)\n",
        "\n",
        "        return [P3_out, P4_out, P5_out, P6_out, P7_out]\n",
        "\n",
        "    def up_sampling(self, x, target_size):\n",
        "        return F.interpolate(x, size=target_size, mode='nearest')\n",
        "\n",
        "    def down_sampling(self, x, target_size):\n",
        "        if x.shape[-2:] == target_size:\n",
        "            return x\n",
        "        stride = x.shape[-1] // target_size[-1]\n",
        "        kernel_size = stride\n",
        "        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels=256, num_blocks=3):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Input projection layers\n",
        "        self.input_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, out_channels, 1, bias=False)\n",
        "            for in_ch in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # Additional P6 and P7 layers\n",
        "        self.p6_conv = nn.Conv2d(in_channels_list[-1], out_channels, 3, stride=2, padding=1)\n",
        "        self.p7_conv = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN blocks\n",
        "        self.bifpn_blocks = nn.ModuleList([\n",
        "            BiFPNBlock(out_channels) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Project input features\n",
        "        features = []\n",
        "        for i, feat in enumerate(inputs):\n",
        "            features.append(self.input_convs[i](feat))\n",
        "\n",
        "        # Create P6 and P7\n",
        "        P6 = self.p6_conv(inputs[-1])\n",
        "        P7 = self.p7_conv(P6)\n",
        "\n",
        "        # Initial feature list\n",
        "        pyramid_features = features + [P6, P7]\n",
        "\n",
        "        # Apply BiFPN blocks\n",
        "        for block in self.bifpn_blocks:\n",
        "            pyramid_features = block(pyramid_features)\n",
        "\n",
        "        return pyramid_features\n",
        "\n",
        "class NNConv3UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        return x\n",
        "\n",
        "class FusionBlock(nn.Module):\n",
        "    def __init__(self, fusion_type='add'):\n",
        "        super().__init__()\n",
        "        self.fusion_type = fusion_type\n",
        "\n",
        "    def forward(self, high_level, low_level):\n",
        "        if self.fusion_type == 'add':\n",
        "            return high_level + low_level\n",
        "        elif self.fusion_type == 'concat':\n",
        "            return torch.cat([high_level, low_level], dim=1)\n",
        "\n",
        "class PredictionDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels//2, out_channels, kernel_size=3, padding=1)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "class RTMonoDepthDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_channels=[48, 136, 384], decoder_channels=[256, 128, 64, 32]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Upsampling blocks\n",
        "        self.upconv2 = NNConv3UpBlock(encoder_channels[2], decoder_channels[0])  # F3 -> D2\n",
        "        self.upconv1 = NNConv3UpBlock(decoder_channels[0], decoder_channels[1])  # After fusion -> D1\n",
        "        self.upconv0 = NNConv3UpBlock(decoder_channels[1], decoder_channels[2])  # After fusion -> D0\n",
        "\n",
        "        # Projection layers to match dimensions for fusion\n",
        "        self.proj2 = nn.Conv2d(encoder_channels[1], decoder_channels[0], 1)  # F2 -> D2 channels\n",
        "        self.proj1 = nn.Conv2d(encoder_channels[0], decoder_channels[1], 1)  # F1 -> D1 channels\n",
        "\n",
        "        # Fusion blocks\n",
        "        self.fusion1 = FusionBlock('add')\n",
        "        self.fusion0 = FusionBlock('concat')\n",
        "\n",
        "        # Prediction decoders at each scale\n",
        "        self.decoder2 = PredictionDecoder(decoder_channels[0])\n",
        "        self.decoder1 = PredictionDecoder(decoder_channels[1])\n",
        "        # After concat: up1_resized (128) + f1_proj (128) = 256 channels\n",
        "        self.decoder0 = PredictionDecoder(decoder_channels[1] + decoder_channels[1])\n",
        "\n",
        "    def forward(self, features, inference_mode=False):\n",
        "        f1, f2, f3 = features  # [low_res -> high_res]\n",
        "        depth_maps = {}\n",
        "\n",
        "        # Level 2: Start from highest level feature\n",
        "        up2 = self.upconv2(f3)\n",
        "        depth_maps['depth_2'] = self.decoder2(up2)\n",
        "\n",
        "        # Level 1: Project F2 to match up2 channels and fuse\n",
        "        f2_proj = self.proj2(f2)\n",
        "        # Resize up2 to match f2 spatial dimensions\n",
        "        up2_resized = F.interpolate(up2, size=f2_proj.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        fused1 = self.fusion1(up2_resized, f2_proj)\n",
        "        up1 = self.upconv1(fused1)\n",
        "        depth_maps['depth_1'] = self.decoder1(up1)\n",
        "\n",
        "        # Level 0: Project F1 to match up1 channels and fuse\n",
        "        f1_proj = self.proj1(f1)\n",
        "        # Resize up1 to match f1 spatial dimensions\n",
        "        up1_resized = F.interpolate(up1, size=f1_proj.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        fused0 = self.fusion0(up1_resized, f1_proj)\n",
        "        depth_maps['depth_0'] = self.decoder0(fused0)\n",
        "\n",
        "        return depth_maps\n",
        "\n",
        "class DepthHead2(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=1):\n",
        "        super(DepthHead2, self).__init__()\n",
        "        # small refinement conv stack\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 3, in_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // 2, out_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, depth_features):\n",
        "        # Accept either dict from RTMonoDepthDecoder or list/tuple\n",
        "        if isinstance(depth_features, dict):\n",
        "            # prefer 'depth_0' as highest res; upsample others to its size\n",
        "            keys = ['depth_0', 'depth_1', 'depth_2']\n",
        "            maps = []\n",
        "            # find first existing key for target size\n",
        "            target = None\n",
        "            for k in keys:\n",
        "                if k in depth_features:\n",
        "                    target = depth_features[k].shape[2:]\n",
        "                    break\n",
        "            if target is None:\n",
        "                raise ValueError(\"depth_features dict empty or unexpected keys\")\n",
        "            for k in keys:\n",
        "                if k in depth_features:\n",
        "                    m = depth_features[k]\n",
        "                    if m.shape[2:] != target:\n",
        "                        m = F.interpolate(m, size=target, mode='bilinear', align_corners=False)\n",
        "                    maps.append(m)\n",
        "            # if less than 3 maps, duplicate last to keep consistent channels\n",
        "            while len(maps) < 3:\n",
        "                maps.append(maps[-1])\n",
        "        else:\n",
        "            # assume iterable: take first 3 or duplicate if fewer\n",
        "            maps = list(depth_features)\n",
        "            while len(maps) < 3:\n",
        "                maps.append(maps[-1])\n",
        "            # upsample to first map's size\n",
        "            target = maps[0].shape[2:]\n",
        "            maps = [m if m.shape[2:] == target else F.interpolate(m, size=target, mode='bilinear', align_corners=False) for m in maps[:3]]\n",
        "\n",
        "        # concat along channels and refine\n",
        "        concat = torch.cat(maps, dim=1)  # C= sum of channels\n",
        "        out = self.refine(concat)\n",
        "        return out  # [B,1,H,W] sigmoid-normalized\n",
        "\n",
        "class DepthHead(nn.Module):\n",
        "    def __init__(self, in_channels=1):  # Change to 1 to match input depth maps\n",
        "        super(DepthHead, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.weights = nn.Parameter(torch.ones(3))  # Adjust to 3 for the number of features (depth_2, depth_1, depth_0); previously 5, which may cause issues in zip()\n",
        "\n",
        "    def weighted_fusion(self, features, weights, target_size):\n",
        "        weights = F.softmax(weights, dim=0)\n",
        "        fused = None\n",
        "        for feat, weight in zip(features, weights): # features liste halinde : depthmap2 B,1,512,512 depthmap1 B,1,512,512 depthmap0 B,1,256,256\n",
        "            if feat.shape[2:] != target_size:\n",
        "                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
        "            if fused is None:\n",
        "                fused = weight * feat\n",
        "            else:\n",
        "                fused += weight * feat\n",
        "        return fused\n",
        "\n",
        "    def forward(self, features):\n",
        "        processed = [self.conv(feat) for feat in features]\n",
        "        target_size = processed[0].shape[2:]  # Or use processed[-1].shape[2:] for higher resolution (e.g., 512x512) if preferred\n",
        "        return self.weighted_fusion(processed, self.weights, target_size)\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels=256, num_anchors=3, num_classes=8):\n",
        "        super(DetectionHead, self).__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Classification head: her anchor için sınıf olasılıkları (sigmoid / softmax)\n",
        "        self.cls_conv = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        # Regression head: her anchor için bbox 4 koordinatı\n",
        "        self.reg_conv = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        # istersen headlerde BatchNorm + Activation koyabilirsin\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: list of tensor, her biri [B, C, H, W]\n",
        "        returns:\n",
        "            cls_preds: [B, total_anchors, num_classes]\n",
        "            reg_preds: [B, total_anchors, 4]\n",
        "        \"\"\"\n",
        "        cls_outputs = []\n",
        "        reg_outputs = []\n",
        "\n",
        "        for feat in features:\n",
        "            # feat: [B, C, H, W]\n",
        "            cls_out = self.cls_conv(feat)  # [B, A*C, H, W]\n",
        "            reg_out = self.reg_conv(feat)  # [B, A*4, H, W]\n",
        "\n",
        "            B, _, H, W = cls_out.shape\n",
        "\n",
        "            # reshape: (B, A, C, H, W) → (B, H*W*A, C)\n",
        "            cls_out = cls_out.view(B, self.num_anchors, self.num_classes, H, W)\n",
        "            cls_out = cls_out.permute(0, 3, 4, 1, 2).contiguous()  # B, H, W, A, C\n",
        "            cls_out = cls_out.view(B, -1, self.num_classes)         # B, (H*W*A), C\n",
        "\n",
        "            # regression benzer şekilde (B, A, 4, H, W) → (B, H*W*A, 4)\n",
        "            reg_out = reg_out.view(B, self.num_anchors, 4, H, W)\n",
        "            reg_out = reg_out.permute(0, 3, 4, 1, 2).contiguous()  # B, H, W, A, 4\n",
        "            reg_out = reg_out.view(B, -1, 4)                       # B, (H*W*A), 4\n",
        "\n",
        "            cls_outputs.append(cls_out)\n",
        "            reg_outputs.append(reg_out)\n",
        "\n",
        "        # Tüm seviyeleri birleştir\n",
        "        cls_preds = torch.cat(cls_outputs, dim=1)  # B, total_anchors, num_classes\n",
        "        reg_preds = torch.cat(reg_outputs, dim=1)  # B, total_anchors, 4\n",
        "\n",
        "        return cls_preds, reg_preds\n",
        "\n",
        "class MultiTaskHeads(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=256, num_anchors=3):\n",
        "        super(MultiTaskHeads, self).__init__()\n",
        "        # Depth heads unchanged (they expect depth_maps dict/list)\n",
        "        self.depth_head1 = DepthHead(in_channels=1)   # train fusion from depth_decoder outputs\n",
        "        self.depth_head2 = DepthHead2(in_channels=1)  # inference refinement\n",
        "        self.detection_head = DetectionHead(num_anchors=num_anchors)\n",
        "\n",
        "    def forward(self, bifpn_features, depth_features):\n",
        "        \"\"\"\n",
        "        bifpn_features: list [P3,P4,P5,P6,P7] each [B, C, H, W]\n",
        "        depth_features: dict from RTMonoDepthDecoder (depth_0,1,2)\n",
        "        returns:\n",
        "           classification: list(len=5) of [B, num_anchors*num_classes, H, W]\n",
        "           regression:     list(len=5) of [B, num_anchors*4, H, W]\n",
        "           depth: [B,1,Hd,Wd]\n",
        "        \"\"\"\n",
        "        # Depth: use decoder outputs (pixel-wise). Use depth_head1 in train, depth_head2 in inference\n",
        "        depth_list = [depth_features['depth_2'], depth_features['depth_1'], depth_features['depth_0']]\n",
        "\n",
        "        depth = self.depth_head1(depth_list)\n",
        "\n",
        "        cls_preds, reg_preds = self.detection_head(bifpn_features)\n",
        "\n",
        "        return {\n",
        "            'depth': depth,\n",
        "            'classification': cls_preds,\n",
        "            'regression': reg_preds\n",
        "        }\n",
        "\n",
        "class PostProcessor:\n",
        "    def __init__(self, num_classes=7, num_anchors=15, strides=[8, 16, 32, 64, 128]):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchors = num_anchors\n",
        "        self.strides = strides\n",
        "        # KITTI-specific anchor scales and ratios\n",
        "        self.anchor_scales = [[32, 48, 64], [64, 96, 128], [128, 192, 256], [256, 384, 512], [512, 768, 1024]]\n",
        "        self.anchor_ratios = [[0.5, 1.0, 2.0, 0.33, 3.0]] * 5  # Her feature map için aynı\n",
        "        self.strides = [8, 16, 32, 64, 128]\n",
        "        self.anchor_generator = AnchorGenerator(sizes=self.anchor_scales, aspect_ratios=self.anchor_ratios)\n",
        "    def generate_anchors_for_level(self, feat, level_idx):\n",
        "        \"\"\"\n",
        "        feat: feature map tensor [B, C, H, W]\n",
        "        level_idx: hangi feature map seviyesi (0..4)\n",
        "        \"\"\"\n",
        "    def generate_all_anchors(self, bifpn_features,image_list):\n",
        "        anchors_per_level = self.anchor_generator(feature_maps=bifpn_features,image_list=image_list)\n",
        "        return torch.cat(anchors_per_level, dim=0)  # [N_total, 4]\n",
        "\n",
        "class CompleteMultiTaskModel(nn.Module):\n",
        "    def __init__(self, İsPretreained=True, num_classes=7, bifpn_channels=256, bifpn_blocks=3,\n",
        "                 confidence_threshold=0.1, max_detections=100, num_anchors=15, MAX_CANDIDATES=1000):  # Threshold düşürüldü\n",
        "        super(CompleteMultiTaskModel, self).__init__()\n",
        "\n",
        "        # Model components\n",
        "        self.encoder = EncoderBackBone(İsPretreained)\n",
        "        in_channels_list = [48, 136, 384]\n",
        "        self.bifpn = BiFPN(in_channels_list, bifpn_channels, bifpn_blocks)\n",
        "        self.depth_decoder = RTMonoDepthDecoder()\n",
        "        self.multi_head = MultiTaskHeads(num_classes, bifpn_channels, num_anchors)\n",
        "\n",
        "        # Parameters\n",
        "        self.num_classes = num_classes\n",
        "        self.conf_thresh = confidence_threshold\n",
        "        self.max_detections = max_detections\n",
        "        self.num_anchors = num_anchors\n",
        "        self.strides = [8, 16, 32, 64, 128]\n",
        "        self.MAX_CANDIDATES = MAX_CANDIDATES\n",
        "        self.img_size = 256  # KITTI resized image size\n",
        "\n",
        "        # Post processor\n",
        "        self.post_processor = PostProcessor(num_classes, num_anchors, self.strides)\n",
        "\n",
        "    def forward(self, images, targets=None, mode=\"train\"):\n",
        "        B = images.shape[0]\n",
        "\n",
        "        # DÜZELTME: Model mode'u doğru ayarla\n",
        "        if mode == \"inference\":\n",
        "            self.eval()  # Inference için eval mode\n",
        "        else:\n",
        "            self.train()  # Training için train mode\n",
        "\n",
        "        with torch.set_grad_enabled(mode == \"train\"):  # Gradient sadece training'de\n",
        "            # Forward pass\n",
        "            backbone_features = self.encoder(images)\n",
        "            bifpn_features = self.bifpn(backbone_features)\n",
        "            depth_maps = self.depth_decoder(backbone_features, inference_mode=(mode != \"train\"))\n",
        "            raw_preds = self.multi_head(bifpn_features, depth_maps)\n",
        "            image_sizes = [img.shape[-2:] for img in images]  # Her image için (H,W)\n",
        "            image_list = ImageList(images, image_sizes=image_sizes)\n",
        "            anchors_all = self.post_processor.generate_all_anchors(bifpn_features,image_list)\n",
        "            cls_preds = raw_preds['classification']\n",
        "            reg_preds = raw_preds['regression']\n",
        "            depth_pred = raw_preds['depth']\n",
        "            if mode == \"train\":\n",
        "                return {\n",
        "                    \"cls_preds\": cls_preds,\n",
        "                    \"reg_preds\": reg_preds,\n",
        "                    \"anchors\": anchors_all,\n",
        "                    \"depth_pred\": depth_pred,\n",
        "                    \"targets\": targets\n",
        "                }\n",
        "\n",
        "            elif mode == \"inference\":\n",
        "                return self._inference_postprocess_fixed(cls_preds,reg_preds,depth_pred,anchors_all,images)\n",
        "\n",
        "    def _inference_postprocess_fixed(self, cls_preds, reg_preds, depth_pred, anchors_all, images):\n",
        "        \"\"\"Düzeltilmiş inference postprocessing\"\"\"\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "        img_h, img_w = images.shape[2], images.shape[3]  # Model input size\n",
        "\n",
        "        # Generate anchors (pixel coordinates)\n",
        "        results = []\n",
        "\n",
        "        for b in range(B):\n",
        "            try:\n",
        "                batch_cls = cls_preds[b]  # [N, num_classes]\n",
        "                batch_reg = reg_preds[b]  # [N, 4]\n",
        "\n",
        "                # 1. Confidence filtering\n",
        "                cls_probs = torch.softmax(batch_cls, dim=-1)\n",
        "                max_probs, pred_labels = torch.max(cls_probs, dim=-1)\n",
        "\n",
        "                # Confidence threshold\n",
        "                conf_mask = max_probs > self.conf_thresh\n",
        "                if conf_mask.sum() == 0:\n",
        "                    results.append(self._get_empty_result_single(device))\n",
        "                    continue\n",
        "\n",
        "                # Filter predictions\n",
        "                filtered_probs = max_probs[conf_mask]\n",
        "                filtered_labels = pred_labels[conf_mask]\n",
        "                filtered_reg = batch_reg[conf_mask]\n",
        "                filtered_anchors = anchors_all[conf_mask]\n",
        "\n",
        "                # 2. Top-K selection for memory efficiency\n",
        "                if len(filtered_probs) > self.MAX_CANDIDATES:\n",
        "                    top_k_scores, top_k_idx = torch.topk(filtered_probs, self.MAX_CANDIDATES)\n",
        "                    filtered_probs = top_k_scores\n",
        "                    filtered_labels = filtered_labels[top_k_idx]\n",
        "                    filtered_reg = filtered_reg[top_k_idx]\n",
        "                    filtered_anchors = filtered_anchors[top_k_idx]\n",
        "                # 3. Decode boxes - DÜZELTME: Doğru koordinat sistemi\n",
        "                decoded_boxes = self._decode_boxes_corrected(filtered_anchors, filtered_reg)\n",
        "\n",
        "                decoded_boxes_normalized = decoded_boxes / torch.tensor([256, 256, 256, 256], device=decoded_boxes.device)\n",
        "                # Sınır dışını engelle\n",
        "                decoded_boxes_normalized = decoded_boxes_normalized.clamp(0, 1)\n",
        "                boxes = decoded_boxes_normalized * torch.tensor([img_w, img_h, img_w, img_h], device=decoded_boxes_normalized.device)\n",
        "\n",
        "\n",
        "                # 5. Box validation\n",
        "                valid_boxes, valid_scores, valid_labels = self._validate_boxes_fixed(\n",
        "                    boxes, filtered_probs, filtered_labels\n",
        "                )\n",
        "                print(str(valid_boxes)+ \"--\"+str(valid_labels)+\"--\"+str(valid_scores))\n",
        "                if len(valid_scores) == 0:\n",
        "                    results.append(self._get_empty_result_single(device))\n",
        "                    continue\n",
        "\n",
        "                # 6. NMS - DÜZELTME: Daha düşük threshold\n",
        "                final_boxes, final_scores, final_labels = self._apply_class_wise_nms(\n",
        "                    valid_boxes, valid_scores, valid_labels, iou_threshold=0.3  # Düşürüldü\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "                # 8. Depth extraction\n",
        "                depth_values = None\n",
        "                if depth_pred is not None:\n",
        "                    try:\n",
        "                        depth_values = self._extract_depth_values(\n",
        "                            depth_pred[b], final_boxes\n",
        "                        )\n",
        "                    except:\n",
        "                        depth_values = [0.0] * len(final_boxes)\n",
        "\n",
        "                results.append({\n",
        "                    \"boxes\": final_boxes,  # Pixel coordinates for visualization\n",
        "                    \"scores\": final_scores,\n",
        "                    \"labels\": final_labels,\n",
        "                    \"depth\": depth_values\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch {b}: {e}\")\n",
        "                results.append(self._get_empty_result_single(device))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _extract_depth_values(self, depth_map, final_boxes_pixel):\n",
        "      depth_values = []\n",
        "      for box in final_boxes_pixel:\n",
        "          x1, y1, x2, y2 = box\n",
        "          center_x_pix = int((x1 + x2) / 2 )\n",
        "          center_y_pix = int((y1 + y2) / 2 )\n",
        "\n",
        "          # Sınır kontrolü\n",
        "          center_x_pix = max(0, min(center_x_pix, -1))\n",
        "          center_y_pix = max(0, min(center_y_pix, -1))\n",
        "\n",
        "          depth_value = depth_map[center_y_pix, center_x_pix].item()\n",
        "          depth_values.append(depth_value * 80.0)  # Gerçek mesafeye çevir\n",
        "      return depth_values\n",
        "\n",
        "    def _apply_class_wise_nms(self, boxes, scores, labels, iou_threshold=0.2):\n",
        "      final_boxes = []\n",
        "      final_scores = []\n",
        "      final_labels = []\n",
        "\n",
        "      unique_labels = labels.unique()\n",
        "      for lbl in unique_labels:\n",
        "          mask = labels == lbl\n",
        "          boxes_lbl = boxes[mask]\n",
        "          scores_lbl = scores[mask]\n",
        "\n",
        "          keep = nms(boxes_lbl, scores_lbl, iou_threshold)\n",
        "\n",
        "          final_boxes.append(boxes_lbl[keep])\n",
        "          final_scores.append(scores_lbl[keep])\n",
        "          final_labels.append(labels[mask][keep])\n",
        "\n",
        "      if final_boxes:\n",
        "          final_boxes = torch.cat(final_boxes)\n",
        "          final_scores = torch.cat(final_scores)\n",
        "          final_labels = torch.cat(final_labels)\n",
        "      else:\n",
        "          final_boxes = torch.empty((0, 4))\n",
        "          final_scores = torch.empty((0,))\n",
        "          final_labels = torch.empty((0,), dtype=torch.long)\n",
        "\n",
        "      return final_boxes, final_scores, final_labels\n",
        "\n",
        "    def _decode_boxes_corrected(self, anchors, deltas): # x1,y1,x2,y2 anchors formatı tx,ty,tw,th deltas formatı ikiside piksel cinsinde\n",
        "        if anchors.numel() == 0:\n",
        "            return torch.zeros((0, 4), device=anchors.device)\n",
        "\n",
        "        # Anchor boyut ve merkezleri\n",
        "        anchor_widths  = anchors[:, 2] - anchors[:, 0]\n",
        "        anchor_heights = anchors[:, 3] - anchors[:, 1]\n",
        "        anchor_ctr_x   = anchors[:, 0] + 0.5 * anchor_widths\n",
        "        anchor_ctr_y   = anchors[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "\n",
        "        # Delta değerleri\n",
        "        dx, dy, dw, dh = deltas[:, 0], deltas[:, 1], deltas[:, 2], deltas[:, 3]\n",
        "\n",
        "        # Patlamayı önlemek için genişlik/yükseklik log-scale clamp\n",
        "        dw = torch.clamp(dw, max=4.0)\n",
        "        dh = torch.clamp(dh, max=4.0)\n",
        "\n",
        "        # Deltaları uygula\n",
        "        pred_ctr_x = dx * anchor_widths + anchor_ctr_x\n",
        "        pred_ctr_y = dy * anchor_heights + anchor_ctr_y\n",
        "        pred_w = torch.exp(dw) * anchor_widths\n",
        "        pred_h = torch.exp(dh) * anchor_heights\n",
        "\n",
        "        # Köşe formatına çevir\n",
        "        x1 = pred_ctr_x - 0.5 * pred_w\n",
        "        y1 = pred_ctr_y - 0.5 * pred_h\n",
        "        x2 = pred_ctr_x + 0.5 * pred_w\n",
        "        y2 = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "        return torch.stack((x1, y1, x2, y2), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def _validate_boxes_fixed(self, boxes, scores, labels):\n",
        "        \"\"\"Normalize koordinatlarda box validation\"\"\"\n",
        "        if len(boxes) == 0:\n",
        "            return self._get_empty_tensors(boxes.device)\n",
        "\n",
        "        # Geometric validation\n",
        "        valid_geom = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
        "\n",
        "        # Size validation (normalized coordinates)\n",
        "        box_w = boxes[:, 2] - boxes[:, 0]\n",
        "        box_h = boxes[:, 3] - boxes[:, 1]\n",
        "        min_size = 0.001  # 1% of image\n",
        "        max_size = 1  # 95% of image\n",
        "        valid_size = (box_w >= min_size) & (box_h >= min_size) & \\\n",
        "                     (box_w <= max_size) & (box_h <= max_size)\n",
        "\n",
        "        # Bounds validation (normalized)\n",
        "        margin = 0.05  # 5% margin\n",
        "        valid_bounds = (boxes[:, 0] >= -margin) & (boxes[:, 1] >= -margin) & \\\n",
        "                      (boxes[:, 2] <= 1 + margin) & (boxes[:, 3] <= 1 + margin)\n",
        "\n",
        "        valid_mask = valid_geom & valid_size & valid_bounds\n",
        "\n",
        "        if valid_mask.sum() == 0:\n",
        "            return self._get_empty_tensors(boxes.device)\n",
        "\n",
        "        boxes = boxes[valid_mask]\n",
        "        scores = scores[valid_mask]\n",
        "        labels = labels[valid_mask]\n",
        "        boxes = torch.clamp(boxes, 0, 1)\n",
        "\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def _get_empty_tensors(self, device):\n",
        "        return (torch.zeros((0, 4), device=device),\n",
        "                torch.zeros((0,), device=device),\n",
        "                torch.zeros((0,), dtype=torch.long, device=device))\n",
        "\n",
        "    def _get_empty_result_single(self, device):\n",
        "        return {\n",
        "            \"boxes\": torch.zeros((0, 4), device=device),\n",
        "            \"scores\": torch.zeros((0,), device=device),\n",
        "            \"labels\": torch.zeros((0,), device=device),\n",
        "            \"depth\": None\n",
        "        }\n",
        "\n",
        "class MultiTaskCriterion(nn.Module):\n",
        "    def __init__(self, num_classes=7, loss_weights=None, device='cuda',\n",
        "                 pos_iou_threshold=0.5, neg_iou_threshold=0.3, img_size=256):\n",
        "        super(MultiTaskCriterion, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.loss_weights = loss_weights if loss_weights else {\n",
        "            'classification': 1.0,\n",
        "            'regression': 1.0,\n",
        "            'depth': 1.0,\n",
        "            'depth_map': 0.1\n",
        "        }\n",
        "\n",
        "        self.cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        self.reg_criterion = nn.SmoothL1Loss(reduction='none')\n",
        "        self.depth_criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "        self.pos_iou_threshold = pos_iou_threshold\n",
        "        self.neg_iou_threshold = neg_iou_threshold\n",
        "\n",
        "    def forward(self, model_output):\n",
        "        cls_preds = model_output['cls_preds']\n",
        "        reg_preds = model_output['reg_preds']\n",
        "        anchors = model_output['anchors']\n",
        "        depth_pred = model_output['depth_pred']\n",
        "        targets = model_output['targets']\n",
        "        batch_size = cls_preds.size(0)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        cls_loss_sum = 0.0\n",
        "        reg_loss_sum = 0.0\n",
        "        depth_loss_sum = 0.0\n",
        "        depth_map_loss_val = 0.0\n",
        "\n",
        "        # DÜZELTME: Object-based metrikler için\n",
        "        total_objects = 0\n",
        "        detected_objects = 0\n",
        "        correct_detections = 0\n",
        "        depth_errors = []\n",
        "        valid_samples = 0\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            try:\n",
        "                if isinstance(targets, list):\n",
        "                    if batch_idx >= len(targets):\n",
        "                        continue\n",
        "                    target = targets[batch_idx]\n",
        "                    if isinstance(target, torch.Tensor):\n",
        "                        if target.numel() == 0 or target.size(0) == 0:\n",
        "                            continue\n",
        "                        target = target.to(self.device)\n",
        "                    else:\n",
        "                        continue\n",
        "                else:\n",
        "                    if batch_idx not in targets or len(targets[batch_idx]) == 0:\n",
        "                        continue\n",
        "                    target = targets[batch_idx].to(self.device)\n",
        "\n",
        "                gt_classes = target[:, 0].long()\n",
        "                gt_boxes = target[:, 1:5]\n",
        "                gt_depths = target[:, 5] if target.size(1) > 5 else None\n",
        "\n",
        "                batch_cls_preds = cls_preds[batch_idx]\n",
        "                batch_reg_preds = reg_preds[batch_idx]\n",
        "                batch_depth_preds = depth_pred[batch_idx] if depth_pred is not None else None\n",
        "\n",
        "                # Anchor assignment (loss için gerekli)\n",
        "                pos_indices, neg_indices, matched_gt_indices = self._assign_targets_to_anchors(\n",
        "                    anchors, gt_boxes, gt_classes\n",
        "                )\n",
        "\n",
        "                # Classification Loss (anchor-based, loss için)\n",
        "                if len(pos_indices) > 0:\n",
        "                    cls_loss = self._compute_classification_loss(\n",
        "                        batch_cls_preds, gt_classes, pos_indices, neg_indices, matched_gt_indices\n",
        "                    )\n",
        "                    if cls_loss is not None:\n",
        "                        total_loss += cls_loss * self.loss_weights['classification']\n",
        "                        cls_loss_sum += cls_loss.detach().item()\n",
        "\n",
        "                # Regression Loss (anchor-based, loss için)\n",
        "                if len(pos_indices) > 0:\n",
        "                    reg_loss = self._compute_regression_loss(\n",
        "                        batch_reg_preds, gt_boxes, anchors, pos_indices, matched_gt_indices\n",
        "                    )\n",
        "                    if reg_loss is not None:\n",
        "                        total_loss += reg_loss * self.loss_weights['regression']\n",
        "                        reg_loss_sum += reg_loss.detach().item()\n",
        "\n",
        "                # Depth Loss (anchor-based, loss için)\n",
        "                if gt_depths is not None and batch_depth_preds is not None:\n",
        "                    depth_loss = self._compute_depth_loss_normalized(\n",
        "                        batch_depth_preds, gt_depths, gt_boxes\n",
        "                    )\n",
        "                    if depth_loss is not None:\n",
        "                        total_loss += depth_loss * self.loss_weights['depth']\n",
        "                        depth_loss_sum += depth_loss.detach().item()\n",
        "\n",
        "                # DÜZELTME: Object-based evaluation\n",
        "                obj_metrics = self._evaluate_object_detection(\n",
        "                    batch_cls_preds, batch_reg_preds, anchors,\n",
        "                    gt_classes, gt_boxes, gt_depths, batch_depth_preds\n",
        "                )\n",
        "\n",
        "                total_objects += obj_metrics['total_objects']\n",
        "                detected_objects += obj_metrics['detected_objects']\n",
        "                correct_detections += obj_metrics['correct_detections']\n",
        "                if obj_metrics['depth_errors']:\n",
        "                    depth_errors.extend(obj_metrics['depth_errors'])\n",
        "\n",
        "                valid_samples += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Depth map smoothness loss\n",
        "        if depth_pred is not None:\n",
        "            depth_map_loss = self._compute_depth_smoothness_loss(depth_pred)\n",
        "            total_loss += depth_map_loss * self.loss_weights['depth_map']\n",
        "            depth_map_loss_val = depth_map_loss.detach().item()\n",
        "\n",
        "        # DÜZELTME: Object-based metrikler\n",
        "        object_accuracy = correct_detections / max(total_objects, 1)\n",
        "        object_precision = correct_detections / max(detected_objects, 1)\n",
        "        object_recall = correct_detections / max(total_objects, 1)\n",
        "        object_f1 = 2 * (object_precision * object_recall) / max(object_precision + object_recall, 1e-6)\n",
        "\n",
        "        avg_depth_error = np.mean(depth_errors) if depth_errors else 0.0\n",
        "        rmse_depth = np.sqrt(avg_depth_error) if depth_errors else 0.0\n",
        "\n",
        "        losses = {\n",
        "            'total': total_loss,\n",
        "            'classification': cls_loss_sum / max(valid_samples, 1),\n",
        "            'regression': reg_loss_sum / max(valid_samples, 1),\n",
        "            'depth': depth_loss_sum / max(valid_samples, 1),\n",
        "            'depth_map': depth_map_loss_val\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            'Accuracy': object_accuracy,  # Artık object-based\n",
        "            'F1_score': object_f1,        # Artık object-based\n",
        "            'MSE': avg_depth_error,\n",
        "            'RMSE': rmse_depth,\n",
        "            'mAP': object_precision,      # Basitleştirilmiş object precision\n",
        "            'TotalLoss': total_loss.detach().item() if isinstance(total_loss, torch.Tensor) else total_loss,\n",
        "            'ClsLoss': cls_loss_sum / max(valid_samples, 1),\n",
        "            'Precision': object_precision,\n",
        "            'Recall': object_recall,\n",
        "            'TotalObjects': total_objects,\n",
        "            'DetectedObjects': detected_objects,\n",
        "            'CorrectDetections': correct_detections\n",
        "        }\n",
        "\n",
        "        return losses, metrics\n",
        "\n",
        "    def _evaluate_object_detection(self, cls_preds, reg_preds, anchors, gt_classes, gt_boxes, gt_depths=None, depth_pred=None):\n",
        "        \"\"\"\n",
        "        DÜZELTME: Object-based detection evaluation\n",
        "        Her GT object için en iyi anchor'u bulur ve değerlendirir\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            total_objects = len(gt_boxes)\n",
        "            detected_objects = 0\n",
        "            correct_detections = 0\n",
        "            depth_errors = []\n",
        "\n",
        "            if total_objects == 0:\n",
        "                return {\n",
        "                    'total_objects': 0,\n",
        "                    'detected_objects': 0,\n",
        "                    'correct_detections': 0,\n",
        "                    'depth_errors': []\n",
        "                }\n",
        "\n",
        "            # Her GT object için en iyi anchor'u bul\n",
        "            anchors_norm = anchors.clone()\n",
        "            anchors_norm[:, [0,2]] /= 1242\n",
        "            anchors_norm[:, [1,3]] /= 375\n",
        "\n",
        "            ious = self.bbox_iou(anchors_norm, gt_boxes)  # [N_anchors, N_gt]\n",
        "\n",
        "            for gt_idx in range(len(gt_boxes)):\n",
        "                # Bu GT object için en yüksek IoU'ya sahip anchor\n",
        "                gt_ious = ious[:, gt_idx]\n",
        "                best_anchor_idx = torch.argmax(gt_ious)\n",
        "                best_iou = gt_ious[best_anchor_idx]\n",
        "\n",
        "                # Eğer IoU yeterince yüksekse detection var sayıyoruz\n",
        "                if best_iou >= self.pos_iou_threshold:\n",
        "                    detected_objects += 1\n",
        "\n",
        "                    # Classification doğruluğunu kontrol et\n",
        "                    pred_cls = torch.argmax(cls_preds[best_anchor_idx])\n",
        "                    gt_cls = gt_classes[gt_idx]\n",
        "\n",
        "                    if pred_cls == gt_cls:\n",
        "                        correct_detections += 1\n",
        "\n",
        "                    # Depth error hesapla\n",
        "                    if gt_depths is not None and depth_pred is not None:\n",
        "                        gt_box = gt_boxes[gt_idx]\n",
        "                        gt_depth = gt_depths[gt_idx]\n",
        "\n",
        "                        # Depth map'ten center pixel'ı sample et\n",
        "                        if depth_pred.dim() == 3:\n",
        "                            depth_map = depth_pred[0]\n",
        "                        else:\n",
        "                            depth_map = depth_pred\n",
        "\n",
        "                        H, W = depth_map.shape\n",
        "                        center_x = int((gt_box[0] + gt_box[2]) / 2 * W)\n",
        "                        center_y = int((gt_box[1] + gt_box[3]) / 2 * H)\n",
        "                        center_x = max(0, min(center_x, W-1))\n",
        "                        center_y = max(0, min(center_y, H-1))\n",
        "\n",
        "                        pred_depth = depth_map[center_y, center_x].clamp(0, 1)\n",
        "                        depth_error = F.mse_loss(pred_depth, gt_depth.clamp(0, 1)).item()\n",
        "                        depth_errors.append(depth_error)\n",
        "\n",
        "            return {\n",
        "                'total_objects': total_objects,\n",
        "                'detected_objects': detected_objects,\n",
        "                'correct_detections': correct_detections,\n",
        "                'depth_errors': depth_errors\n",
        "            }\n",
        "\n",
        "    def _compute_classification_loss(self, cls_preds, gt_classes, pos_indices, neg_indices, matched_gt_indices):\n",
        "        \"\"\"Classification loss - anchor based (loss için)\"\"\"\n",
        "        if len(pos_indices) == 0:\n",
        "            return None\n",
        "\n",
        "        pos_cls_preds = cls_preds[pos_indices]\n",
        "        pos_gt_classes = gt_classes[matched_gt_indices]\n",
        "        pos_loss = self.focal_loss(pos_cls_preds, pos_gt_classes)\n",
        "\n",
        "        if len(neg_indices) > 0:\n",
        "            max_neg_samples = min(len(neg_indices), len(pos_indices) * 3)\n",
        "            neg_cls_preds = cls_preds[neg_indices]\n",
        "            neg_scores = torch.max(torch.softmax(neg_cls_preds, dim=1), dim=1)[0]\n",
        "            _, hard_neg_indices = torch.topk(neg_scores, max_neg_samples, largest=True)\n",
        "\n",
        "            selected_neg_indices = neg_indices[hard_neg_indices]\n",
        "            selected_neg_cls_preds = cls_preds[selected_neg_indices]\n",
        "            neg_gt_classes = torch.zeros(len(selected_neg_indices), dtype=torch.long, device=self.device)\n",
        "            neg_loss = self.focal_loss(selected_neg_cls_preds, neg_gt_classes)\n",
        "            total_cls_loss = pos_loss + neg_loss\n",
        "        else:\n",
        "            total_cls_loss = pos_loss\n",
        "\n",
        "        return total_cls_loss\n",
        "\n",
        "    def _compute_regression_loss(self, reg_preds, gt_boxes, anchors, pos_indices, matched_gt_indices):\n",
        "        \"\"\"Regression loss - anchor based (loss için)\"\"\"\n",
        "        if len(pos_indices) == 0:\n",
        "            return None\n",
        "\n",
        "        pos_anchors = anchors[pos_indices]\n",
        "        pos_reg_preds = reg_preds[pos_indices]\n",
        "        pos_gt_boxes = gt_boxes[matched_gt_indices]\n",
        "\n",
        "        # Normalize anchors\n",
        "        pos_anchors_norm = pos_anchors.clone()\n",
        "        pos_anchors_norm[:, [0,2]] /= 1242\n",
        "        pos_anchors_norm[:, [1,3]] /= 375\n",
        "\n",
        "        pos_gt_encoded = self._encode_boxes(pos_anchors_norm, pos_gt_boxes)\n",
        "        reg_loss = self.reg_criterion(pos_reg_preds, pos_gt_encoded).mean()\n",
        "\n",
        "        return reg_loss\n",
        "\n",
        "    def _compute_depth_loss_normalized(self, depth_pred, gt_depths, gt_boxes):\n",
        "        \"\"\"Depth loss - anchor based (loss için)\"\"\"\n",
        "        if len(gt_depths) == 0:\n",
        "            return None\n",
        "\n",
        "        if depth_pred.dim() == 3:\n",
        "            depth_map = depth_pred[0]\n",
        "        else:\n",
        "            depth_map = depth_pred\n",
        "\n",
        "        H, W = depth_map.shape\n",
        "        sampled_depths = []\n",
        "        target_depths = []\n",
        "\n",
        "        for gt_box, gt_depth in zip(gt_boxes, gt_depths):\n",
        "            x1, y1, x2, y2 = gt_box\n",
        "\n",
        "            x1_pix = int(x1 * W)\n",
        "            y1_pix = int(y1 * H)\n",
        "            x2_pix = int(x2 * W)\n",
        "            y2_pix = int(y2 * H)\n",
        "\n",
        "            x1_pix = max(0, min(x1_pix, W-1))\n",
        "            y1_pix = max(0, min(y1_pix, H-1))\n",
        "            x2_pix = max(0, min(x2_pix, W-1))\n",
        "            y2_pix = max(0, min(y2_pix, H-1))\n",
        "\n",
        "            center_y = (y1_pix + y2_pix) // 2\n",
        "            center_x = (x1_pix + x2_pix) // 2\n",
        "\n",
        "            sampled_depth = depth_map[center_y, center_x]\n",
        "            sampled_depth = torch.clamp(sampled_depth, 0, 1)\n",
        "            sampled_depths.append(sampled_depth)\n",
        "            target_depths.append(gt_depth.clamp(0, 1))\n",
        "\n",
        "        if len(sampled_depths) == 0:\n",
        "            return None\n",
        "\n",
        "        sampled_depths = torch.stack(sampled_depths)\n",
        "        target_depths = torch.stack(target_depths)\n",
        "        depth_loss = self.depth_criterion(sampled_depths, target_depths).mean()\n",
        "\n",
        "        return depth_loss\n",
        "\n",
        "    def _compute_depth_smoothness_loss(self, depth_pred):\n",
        "        \"\"\"Compute depth map smoothness loss\"\"\"\n",
        "        grad_x = torch.abs(depth_pred[:, :, :, :-1] - depth_pred[:, :, :, 1:])\n",
        "        grad_y = torch.abs(depth_pred[:, :, :-1, :] - depth_pred[:, :, 1:, :])\n",
        "        smoothness_loss = grad_x.mean() + grad_y.mean()\n",
        "        return smoothness_loss\n",
        "\n",
        "    def _assign_targets_to_anchors(self, anchors, gt_boxes, gt_classes):\n",
        "        \"\"\"Assign ground truth to anchors based on IoU\"\"\"\n",
        "        if len(gt_boxes) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device), \\\n",
        "                   torch.tensor([], dtype=torch.long, device=self.device), \\\n",
        "                   torch.tensor([], dtype=torch.long, device=self.device)\n",
        "\n",
        "        anchors_norm = anchors.clone()\n",
        "        anchors_norm[:, [0,2]] /= 1242\n",
        "        anchors_norm[:, [1,3]] /= 375\n",
        "        ious = self.bbox_iou(anchors_norm, gt_boxes)\n",
        "        max_ious, matched_gt_indices = torch.max(ious, dim=1)\n",
        "\n",
        "        pos_mask = max_ious >= self.pos_iou_threshold\n",
        "        neg_mask = max_ious < self.neg_iou_threshold\n",
        "\n",
        "        pos_indices = torch.where(pos_mask)[0]\n",
        "        neg_indices = torch.where(neg_mask)[0]\n",
        "        matched_gt_indices = matched_gt_indices[pos_indices]\n",
        "\n",
        "        return pos_indices, neg_indices, matched_gt_indices\n",
        "\n",
        "    def _encode_boxes(self, anchors, gt_boxes):\n",
        "        \"\"\"Box encoding - normalize koordinatlarda\"\"\"\n",
        "        anchor_widths = anchors[:, 2] - anchors[:, 0]\n",
        "        anchor_heights = anchors[:, 3] - anchors[:, 1]\n",
        "        anchor_ctr_x = anchors[:, 0] + 0.5 * anchor_widths\n",
        "        anchor_ctr_y = anchors[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "        gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "        gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "        gt_ctr_x = gt_boxes[:, 0] + 0.5 * gt_widths\n",
        "        gt_ctr_y = gt_boxes[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "        dx = (gt_ctr_x - anchor_ctr_x) / (anchor_widths + 1e-6)\n",
        "        dy = (gt_ctr_y - anchor_ctr_y) / (anchor_heights + 1e-6)\n",
        "        dw = torch.log(gt_widths / (anchor_widths + 1e-6))\n",
        "        dh = torch.log(gt_heights / (anchor_heights + 1e-6))\n",
        "\n",
        "        return torch.stack((dx, dy, dw, dh), dim=1)\n",
        "\n",
        "    def bbox_iou(self, box1, box2):\n",
        "        \"\"\"Compute IoU between two sets of boxes\"\"\"\n",
        "        area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
        "        area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
        "\n",
        "        lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "        rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "        union = area1[:, None] + area2 - inter\n",
        "        iou = inter / union\n",
        "        return iou\n",
        "\n",
        "    def focal_loss(self, inputs, targets, alpha=0.25, gamma=2.0):\n",
        "        \"\"\"Focal Loss implementation\"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "def test_model_corrected(model, test_loader, device='cuda', save_predictions=True):\n",
        "    \"\"\"Düzeltilmiş test fonksiyonu\"\"\"\n",
        "    model.eval()  # IMPORTANT: Model'i eval mode'a al\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, images in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # DÜZELTME: mode=\"inference\" kullan\n",
        "            batch_results = model(images, mode=\"inference\")\n",
        "\n",
        "            for i, result in enumerate(batch_results):\n",
        "                image_result = {\n",
        "                    'image_idx': batch_idx * len(images) + i,\n",
        "                    'boxes': result['boxes'].cpu().numpy(),\n",
        "                    'scores': result['scores'].cpu().numpy(),\n",
        "                    'labels': result['labels'].cpu().numpy(),\n",
        "                    'depth': result['depth'] if result['depth'] is not None else None\n",
        "                }\n",
        "                results.append(image_result)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Processed {batch_idx}/{len(test_loader)} batches\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def visualize_single_prediction(image_path, prediction, show_depth=True, img_size=256):\n",
        "    \"\"\"\n",
        "    image_path: Görsel dosya yolu\n",
        "    prediction: test_model_corrected çıktısındaki tek bir dict\n",
        "    img_size: Görselin yeniden boyutlandırılacak boyutu (square)\n",
        "    \"\"\"\n",
        "    # Görseli yükle ve resize et\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    boxes = prediction[379]['boxes']\n",
        "    labels = prediction[379]['labels']\n",
        "    depth_values = prediction[379]['depth']\n",
        "\n",
        "    # Eğer boxes normalized ise img_size ile çarp\n",
        "    if boxes.max() <= 1.0:\n",
        "        boxes = boxes * img_size\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "\n",
        "      x1, y1, x2, y2 = box\n",
        "      print(box)\n",
        "      x1 *= 256\n",
        "      x2 *= 256\n",
        "      y1 *= 256\n",
        "      y2 *= 256\n",
        "      rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                          fill=False, color='red', linewidth=2)\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "      label_str = kitti_class_dict[labels[i]]\n",
        "      text = f\"{label_str}\"\n",
        "      if show_depth and depth_values is not None:\n",
        "          text += f\", {depth_values[i]:.2f} metre\"\n",
        "      ax.text(x1, y1, text, color='yellow', fontsize=10)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "kitti_class_dict = {\n",
        "    0: \"Car\",\n",
        "    1: \"Van\",\n",
        "    2: \"Truck\",\n",
        "    3: \"Pedestrian\",\n",
        "    4: \"Person_sitting\",\n",
        "    5: \"Cyclist\",\n",
        "    6: \"Misc\"\n",
        "}"
      ],
      "metadata": {
        "id": "yWHY765bh6LH"
      },
      "id": "yWHY765bh6LH",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f04dbc5a",
      "metadata": {
        "id": "f04dbc5a"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/kitti2012\" #colab\n",
        "#data_path = \"C:/Users/Mehmet/Desktop/kitti2012\" #lcoal\n",
        "\n",
        "image_size=256\n",
        "batch_size=1\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_val_dataset = KITTI_Dataset(data_path=data_path ,transform=transform, mode='train')\n",
        "train_size = int(0.8 * len(train_val_dataset))  # ~155 sahne\n",
        "val_size = len(train_val_dataset) - train_size  # ~39 sahne\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset  = KITTI_Dataset(data_path,transform=transform,mode='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True, num_workers=0,collate_fn=test_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n",
        "\n",
        "class_weights = torch.tensor([w / sum([1 / w for w in [1201,113,39,161,76,36,35]]) for w in [1 / w for w in [1201,113,39,161,76,36,35]]], dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rrO5j91dPxmQ",
      "metadata": {
        "collapsed": true,
        "id": "rrO5j91dPxmQ"
      },
      "outputs": [],
      "source": [
        "results = analyze_dataset(train_dataset,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDLujm0-svCb",
      "metadata": {
        "collapsed": true,
        "id": "TDLujm0-svCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "9cb0598e-d79f-46ff-eb68-973c0b58f16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-13 22:08:56,189] A new study created in memory with name: no-name-b2d5fc17-598c-40b9-9c81-4ec75abc669f\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmehmeteminuludag\u001b[0m (\u001b[33mmehmeteminuludag-kirikkale-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250813_220857-nd5v637k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k' target=\"_blank\">earthy-disco-342</a></strong> to <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi' target=\"_blank\">https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k' target=\"_blank\">https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/30 Train:  39%|███▉      | 122/310 [01:11<01:54,  1.64it/s, Acc=0.127, ClsLoss=0.127, F1=0.172, RMSE=0.159, mAP=0.339, TotalLoss=0.233]"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    task_weights = {\n",
        "          'classification': 1.0,\n",
        "          'regression': 2.0,\n",
        "          'depth': 0.3,\n",
        "          'depth_map': 0.3}\n",
        "\n",
        "    # IoU thresholds\n",
        "    p_iou_threshold = trial.suggest_categorical('p_iou_threshold', [0.35, 0.45])\n",
        "    n_iou_threshold = trial.suggest_categorical('n_iou_threshold', [0.2, 0.3])\n",
        "    model = CompleteMultiTaskModel(num_classes=7,max_detections=15, num_anchors=15,MAX_CANDIDATES=3000,confidence_threshold=0.25).to(device)\n",
        "    # Model oluştur ve BatchNorm ayarla\n",
        "\n",
        "    best_val_loss = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=30,  # Optuna için kısa epoch\n",
        "        learning_rate=1e-3,\n",
        "        device=device,\n",
        "        save_path=f'weights_{trial.number}.pth',\n",
        "        class_weights=class_weights,\n",
        "        task_weights=task_weights,\n",
        "        scheduler_patience=3,\n",
        "        scheduler_factor=0.2,\n",
        "        p_iou_threshold=p_iou_threshold,\n",
        "        n_iou_threshold=n_iou_threshold,\n",
        "        early_stop_patience=5\n",
        "    )\n",
        "\n",
        "    return best_val_loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=2)\n",
        "\n",
        "print(\"En iyi parametreler:\", study.best_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PytTorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}