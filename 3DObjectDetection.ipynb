{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f44ce32d",
      "metadata": {
        "id": "f44ce32d"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "from math import inf\n",
        "from posixpath import defpath\n",
        "import wandb\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "import optuna\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision.ops import nms\n",
        "import torchvision.ops as ops\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.patches as patches\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from collections import Counter\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,average_precision_score\n",
        "from typing import Dict, Any, Optional\n",
        "from torchvision import transforms , models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "HFHhIGESGWmk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFHhIGESGWmk",
        "outputId": "6c39cb44-ae38-46eb-c097-bdcd47776763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "src = \"/content/drive/MyDrive/kitti2012\"\n",
        "dst = \"/content\"\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "!cp -r \"$src\" \"$dst\"\n",
        "\n",
        "#src = \"/content/drive/MyDrive/vkitti_sample (1)\"\n",
        "#dst = \"/content\"\n",
        "#os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "#!cp -r \"$src\" \"$dst\"\n",
        "\n",
        "wandb.login()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KITTI_Dataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None, mode='train'):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.mode = mode  # 'train', 'val', veya 'test'\n",
        "        self.classes = ['Car', 'Van', 'Truck', 'Pedestrian', 'Cyclist', 'Tram', 'Misc']\n",
        "        self.class_map = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.data = []\n",
        "\n",
        "        # KITTI gÃ¶rsel boyutlarÄ± - normalizasyon iÃ§in\n",
        "        self.img_width = 256  # Model giriÅŸ boyutu\n",
        "        self.img_height = 256\n",
        "        self.original_width = 1242  # Orijinal KITTI boyutu\n",
        "        self.original_height = 375\n",
        "        # VarsayÄ±lan transform: yeniden boyutlandÄ±rma\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "        self.max_depth = 80.0  # KITTI max derinlik\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            # Test modunda sadece gÃ¶rselleri yÃ¼kle\n",
        "            data_path =data_path+\"/testing/colored_0\"\n",
        "            file_names = os.listdir(data_path)\n",
        "            for fname in file_names:\n",
        "                if fname.endswith('.png'):\n",
        "                    img_path = os.path.join(data_path, fname)\n",
        "                    self.data.append(img_path)\n",
        "        else :\n",
        "          image_dir = os.path.join(data_path, 'training', 'colored_0')\n",
        "          label_dir = os.path.join(data_path, 'training', 'label_2')\n",
        "          disp_dir = os.path.join(data_path, 'training', 'disp_noc')\n",
        "\n",
        "          file_names = os.listdir(image_dir)\n",
        "          for fname in file_names:\n",
        "              if fname.endswith('.png'):\n",
        "                  scene_id = fname.split('_')[0]\n",
        "                  img_path = os.path.join(image_dir, fname)\n",
        "                  label_path = os.path.join(label_dir, f'{scene_id}.txt')\n",
        "                  disp_path = os.path.join(disp_dir, f'{scene_id}_10.png')\n",
        "                  if os.path.exists(label_path) and os.path.exists(disp_path):\n",
        "                      self.data.append((img_path, label_path, disp_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def convert_labels(self, label_file):\n",
        "      labels = []\n",
        "      with open(label_file, 'r') as f:\n",
        "          for line in f:\n",
        "              parts = line.strip().split()\n",
        "              category = parts[0]\n",
        "              if category in self.class_map and category != 'DontCare':\n",
        "                  x1, y1, x2, y2 = map(float, parts[4:8])\n",
        "                  # Orijinal boyutlara gÃ¶re normalize et\n",
        "                  x1_norm = x1 / self.original_width\n",
        "                  y1_norm = y1 / self.original_height\n",
        "                  x2_norm = x2 / self.original_width\n",
        "                  y2_norm = y2 / self.original_height\n",
        "\n",
        "                  category_num = self.class_map[category]\n",
        "                  labels.append([category_num, x1_norm, y1_norm, x2_norm, y2_norm])\n",
        "      return labels\n",
        "\n",
        "    def load_disparity(self, disp_path):#konum haritasÄ±nÄ±n olduÄŸu image yÃ¼klenir\n",
        "        disp_map = cv2.imread(disp_path, cv2.IMREAD_UNCHANGED) / 256.0\n",
        "        return disp_map\n",
        "\n",
        "    def calculate_depth(self, disp_map):\n",
        "      baseline = 0.54\n",
        "      focal_length = 721.5377 * (256 / 1242)  # Odak uzaklÄ±ÄŸÄ±nÄ± Ã¶lÃ§eklendir\n",
        "      depth = (baseline * focal_length) / (disp_map + 1e-6)\n",
        "      depth = np.clip(depth, 0, self.max_depth)\n",
        "      normalized_depth = depth / self.max_depth\n",
        "      # 256x256'ya yeniden boyutlandÄ±r\n",
        "      normalized_depth = cv2.resize(normalized_depth, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
        "      return normalized_depth\n",
        "\n",
        "    def get_depth_at_box(self, depth_map, x, y, w, h):#her nesnenin ortalama mesafesi iÃ§in box iÃ§erisindeki merkez piksel depthi alÄ±nÄ±r\n",
        "\n",
        "        x_pixel = int(x * self.img_width)\n",
        "        y_pixel = int(y * self.img_height)\n",
        "\n",
        "        # SÄ±nÄ±r kontrolÃ¼\n",
        "        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n",
        "        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n",
        "\n",
        "        if depth_map[y_pixel, x_pixel] == 0:\n",
        "            return 0.0\n",
        "        return depth_map[y_pixel, x_pixel]\n",
        "\n",
        "    def get_disparity_at_box(self, disp_map, x, y, w, h):#her nesnenin box iÃ§erisindeki konum deÄŸerini hesaplar\n",
        "\n",
        "\n",
        "        x_pixel = int(x * self.img_width)\n",
        "        y_pixel = int(y * self.img_height)\n",
        "\n",
        "        # SÄ±nÄ±r kontrolÃ¼\n",
        "        x_pixel = np.clip(x_pixel, 0, self.img_width - 1)\n",
        "        y_pixel = np.clip(y_pixel, 0, self.img_height - 1)\n",
        "\n",
        "        if disp_map[y_pixel, x_pixel] == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Disparite deÄŸerini de normalize et (max disparite ~300 civarÄ±)\n",
        "        max_disparity = 300.0\n",
        "        normalized_disparity = disp_map[y_pixel, x_pixel] / max_disparity\n",
        "        return np.clip(normalized_disparity, 0, 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if self.mode != 'test':\n",
        "          labels_with_depth = []\n",
        "          img_path, label_path, disp_path = self.data[idx]\n",
        "          image = Image.open(img_path).convert('RGB')\n",
        "          labels = self.convert_labels(label_path)\n",
        "          disp_map = self.load_disparity(disp_path)\n",
        "          depth_map = self.calculate_depth(disp_map)  # Derinlik haritasÄ±\n",
        "          for label in labels:\n",
        "              category_num, x1, y1, x2, y2 = label\n",
        "              # Bounding box merkezinden derinlik deÄŸerini al\n",
        "              center_x = (x1 + x2) / 2\n",
        "              center_y = (y1 + y2) / 2\n",
        "              depth = self.get_depth_at_box(depth_map, center_x, center_y, x2 - x1, y2 - y1)\n",
        "              labels_with_depth.append([category_num, x1, y1, x2, y2, depth])\n",
        "          if self.transform:\n",
        "              image = self.transform(image)\n",
        "          output_labels = torch.tensor(labels_with_depth, dtype=torch.float32)\n",
        "          return image, output_labels\n",
        "      else:\n",
        "          img_path = self.data[idx]\n",
        "          image = Image.open(img_path).convert('RGB')\n",
        "          if self.transform:\n",
        "              image = self.transform(image)\n",
        "          return image\n",
        "\n",
        "        #eÄŸitimde doÄŸrudan konum deÄŸerleri ile kayÄ±p hesaplanÄ±rken test aÅŸamasÄ±nda direkt mesafe hesaplanabilir\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    \"\"\"Test mode iÃ§in basit collate function\"\"\"\n",
        "    images = batch  # batch sadece image tensor'larÄ±nÄ±n listesi\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images\n",
        "\n",
        "def kitti_collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    images = torch.stack(images, dim=0)  # [B, C, H, W]\n",
        "    return images, labels\n",
        "\n",
        "def analyze_dataset(train_dataset, device):\n",
        "    \"\"\"\n",
        "    Veri seti analizi ve gÃ¶rselleÅŸtirme fonksiyonu - Optimize edilmiÅŸ versiyon\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    from collections import Counter\n",
        "\n",
        "    # === 0. Veri Setinden Ã–zellikleri Ã‡Ä±karma - TEK DÃ–NGÃœ Ä°LE ===\n",
        "    print(f\"Veri seti boyutu: {len(train_dataset)} Ã¶rnek\")\n",
        "\n",
        "    # TÃ¼m listeleri tek seferde oluÅŸtur\n",
        "    cls_labels = []\n",
        "    depth_maps = []\n",
        "    bboxes = []\n",
        "    objects_per_image = []\n",
        "\n",
        "\n",
        "    for i in range(len(train_dataset)):\n",
        "        image_data = train_dataset[i][1]  # Bir kez al\n",
        "        objects_per_image.append(len(image_data))\n",
        "\n",
        "        # Bu gÃ¶rÃ¼ntÃ¼deki tÃ¼m objeleri iÅŸle\n",
        "        for obj in image_data:\n",
        "            cls_labels.append(obj[0])           # sÄ±nÄ±f\n",
        "            depth_maps.append(obj[5])           # depth\n",
        "            bboxes.append(obj[1:5])             # [x1,y1,x2,y2]\n",
        "    total_objects = len(cls_labels)\n",
        "    print(f\"Toplam obje sayÄ±sÄ±: {total_objects}\")\n",
        "    print(f\"Toplam bbox sayÄ±sÄ±: {len(bboxes)}\")\n",
        "    print(f\"Toplam depth deÄŸeri: {len(depth_maps)}\")\n",
        "\n",
        "    # === 1. SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± Analizi ===\n",
        "    cls_distribution = Counter(cls_labels)\n",
        "    sorted_cls_distribution = sorted(cls_distribution.items(), key=lambda x: x[0])\n",
        "\n",
        "    # Class weights hesaplama\n",
        "    class_weights = torch.tensor([count for _, count in sorted_cls_distribution],\n",
        "                                dtype=torch.float32).to(device)\n",
        "    # === 2. BBox Analizi - VEKTÃ–RLEÅžTÄ°RÄ°LMÄ°Åž ===\n",
        "    # NumPy array'e Ã§evir hÄ±zlÄ± iÅŸlem iÃ§in\n",
        "    bboxes_array = np.array(bboxes)\n",
        "\n",
        "    # Format: [x1, y1, x2, y2]\n",
        "    bbox_widths = bboxes_array[:, 2] - bboxes_array[:, 0]   # x2 - x1\n",
        "    bbox_heights = bboxes_array[:, 3] - bboxes_array[:, 1]  # y2 - y1\n",
        "    bbox_areas = bbox_widths * bbox_heights\n",
        "\n",
        "    # SÄ±fÄ±ra bÃ¶lme kontrolÃ¼ ile aspect ratio\n",
        "    bbox_aspect_ratios = np.divide(bbox_widths, bbox_heights,\n",
        "                                  out=np.zeros_like(bbox_widths),\n",
        "                                  where=bbox_heights!=0)\n",
        "\n",
        "    # === 3. GÃ¶rsel BaÅŸÄ±na Obje SayÄ±sÄ± Analizi - ZATEN HAZIR ===\n",
        "    # objects_per_image yukarÄ±da hesaplandÄ±\n",
        "\n",
        "    # === 4. Depth Analizi - VEKTÃ–RLEÅžTÄ°RÄ°LMÄ°Åž ===\n",
        "    depth_array = np.array(depth_maps)\n",
        "    valid_mask = ~np.isnan(depth_array)\n",
        "    valid_depths = depth_array[valid_mask]\n",
        "    invalid_depths = np.sum(~valid_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # === 5. GÃ¶rselleÅŸtirme ===\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± pasta grafiÄŸi\n",
        "    plt.subplot(3, 4, 1)\n",
        "    labels, values = zip(*sorted_cls_distribution)\n",
        "    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "    plt.title('SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± bar grafiÄŸi\n",
        "    plt.subplot(3, 4, 2)\n",
        "    plt.bar(labels, values, alpha=0.7, edgecolor='black')\n",
        "    plt.title('SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± (Bar Chart)', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('SÄ±nÄ±f')\n",
        "    plt.ylabel('Ã–rnek SayÄ±sÄ±')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # GÃ¶rsel baÅŸÄ±na obje sayÄ±sÄ± histogramÄ±\n",
        "    plt.subplot(3, 4, 3)\n",
        "    plt.hist(objects_per_image, bins=range(1, max(objects_per_image)+2),\n",
        "             alpha=0.7, edgecolor='black', color='skyblue')\n",
        "    plt.title('GÃ¶rsel BaÅŸÄ±na Obje SayÄ±sÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Obje SayÄ±sÄ±')\n",
        "    plt.ylabel('GÃ¶rsel SayÄ±sÄ±')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox geniÅŸlik histogramÄ±\n",
        "    plt.subplot(3, 4, 4)\n",
        "    plt.hist(bbox_widths, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
        "    plt.title('BBox GeniÅŸlik DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('GeniÅŸlik')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox yÃ¼kseklik histogramÄ±\n",
        "    plt.subplot(3, 4, 5)\n",
        "    plt.hist(bbox_heights, bins=50, alpha=0.7, edgecolor='black', color='red')\n",
        "    plt.title('BBox YÃ¼kseklik DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('YÃ¼kseklik')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox alan histogramÄ±\n",
        "    plt.subplot(3, 4, 6)\n",
        "    plt.hist(bbox_areas, bins=50, alpha=0.7, edgecolor='black', color='purple')\n",
        "    plt.title('BBox Alan DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Alan')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox aspect ratio histogramÄ±\n",
        "    plt.subplot(3, 4, 7)\n",
        "    plt.hist(bbox_aspect_ratios, bins=50, alpha=0.7, edgecolor='black', color='brown')\n",
        "    plt.title('BBox En/Boy OranÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('En/Boy OranÄ±')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Depth deÄŸerleri histogramÄ±\n",
        "    plt.subplot(3, 4, 8)\n",
        "    if valid_depths:\n",
        "        plt.hist(valid_depths, bins=50, alpha=0.7, edgecolor='black', color='lightgreen')\n",
        "    plt.title('Depth DeÄŸerleri DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Normalize EdilmiÅŸ Depth')\n",
        "    plt.ylabel('Frekans')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Depth box plot\n",
        "    plt.subplot(3, 4, 9)\n",
        "    if valid_depths:\n",
        "        plt.boxplot(valid_depths, vert=True)\n",
        "    plt.title('Depth Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Normalize EdilmiÅŸ Depth')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Obje sayÄ±sÄ± box plot\n",
        "    plt.subplot(3, 4, 10)\n",
        "    plt.boxplot(objects_per_image, vert=True)\n",
        "    plt.title('Obje SayÄ±sÄ± Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Obje SayÄ±sÄ±')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox alan box plot\n",
        "    plt.subplot(3, 4, 11)\n",
        "    plt.boxplot(bbox_areas, vert=True)\n",
        "    plt.title('BBox Alan Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Alan')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # BBox aspect ratio box plot\n",
        "    plt.subplot(3, 4, 12)\n",
        "    plt.boxplot(bbox_aspect_ratios, vert=True)\n",
        "    plt.title('En/Boy OranÄ± Box Plot', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('En/Boy OranÄ±')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # === 6. Ä°statistik RaporlarÄ± ===\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DATASET ANALÄ°Z RAPORU\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Genel bilgiler\n",
        "    print(f\"\\nðŸ“‹ GENEL BÄ°LGÄ°LER:\")\n",
        "    print(f\"Toplam gÃ¶rsel sayÄ±sÄ±:    {len(train_dataset):6d}\")\n",
        "    print(f\"Toplam obje sayÄ±sÄ±:      {len(cls_labels):6d}\")\n",
        "    print(f\"Toplam bbox sayÄ±sÄ±:      {len(bboxes):6d}\")\n",
        "\n",
        "    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± raporu\n",
        "    print(f\"\\n SINIF DAÄžILIMI:\")\n",
        "    print(f\"Toplam sÄ±nÄ±f sayÄ±sÄ±: {len(cls_distribution)}\")\n",
        "    for label, count in sorted_cls_distribution:\n",
        "        percentage = (count / sum(cls_distribution.values())) * 100\n",
        "        print(f\"  SÄ±nÄ±f {label}: {count:4d} Ã¶rnek ({percentage:5.1f}%)\")\n",
        "\n",
        "    # BBox analiz raporu\n",
        "    print(f\"\\n BBOX ANALÄ°Z RAPORU:\")\n",
        "    print(f\"Ortalama geniÅŸlik:       {np.mean(bbox_widths):8.2f}\")\n",
        "    print(f\"Ortalama yÃ¼kseklik:      {np.mean(bbox_heights):8.2f}\")\n",
        "    print(f\"Ortalama alan:           {np.mean(bbox_areas):8.2f}\")\n",
        "    print(f\"Ortalama en/boy oranÄ±:   {np.mean(bbox_aspect_ratios):8.2f}\")\n",
        "    print(f\"Min geniÅŸlik:            {np.min(bbox_widths):8.2f}\")\n",
        "    print(f\"Max geniÅŸlik:            {np.max(bbox_widths):8.2f}\")\n",
        "    print(f\"Min yÃ¼kseklik:           {np.min(bbox_heights):8.2f}\")\n",
        "    print(f\"Max yÃ¼kseklik:           {np.max(bbox_heights):8.2f}\")\n",
        "    print(f\"Min alan:                {np.min(bbox_areas):8.2f}\")\n",
        "    print(f\"Max alan:                {np.max(bbox_areas):8.2f}\")\n",
        "\n",
        "    # Obje sayÄ±sÄ± raporu\n",
        "    print(f\"\\n OBJE SAYISI Ä°STATÄ°STÄ°KLERÄ°:\")\n",
        "    print(f\"Ortalama obje sayÄ±sÄ±: {np.mean(objects_per_image):6.2f}\")\n",
        "    print(f\"Medyan obje sayÄ±sÄ±:   {np.median(objects_per_image):6.2f}\")\n",
        "    print(f\"Maksimum obje sayÄ±sÄ±: {max(objects_per_image):6d}\")\n",
        "    print(f\"Minimum obje sayÄ±sÄ±:  {min(objects_per_image):6d}\")\n",
        "    print(f\"Standart sapma:       {np.std(objects_per_image):6.2f}\")\n",
        "\n",
        "    # Depth raporu\n",
        "    print(f\"\\n DEPTH ANALÄ°Z RAPORU:\")\n",
        "    print(f\"Toplam depth deÄŸeri:     {len(depth_maps):6d}\")\n",
        "    print(f\"GeÃ§erli depth deÄŸeri:    {len(valid_depths):6d}\")\n",
        "    print(f\"GeÃ§ersiz depth (NaN):    {invalid_depths:6d}\")\n",
        "    print(f\"Depth geÃ§erlilik oranÄ±:  {len(valid_depths)/len(depth_maps)*100:6.1f}%\")\n",
        "\n",
        "    if valid_depths:\n",
        "        print(f\"\\n DEPTH Ä°STATÄ°STÄ°KLERÄ°:\")\n",
        "        for key, value in depth_stats.items():\n",
        "            print(f\"{key:>8}: {value:8.4f}\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return {\n",
        "        'class_weights': class_weights,\n",
        "        'cls_distribution': sorted_cls_distribution,\n",
        "        'bbox_stats': {\n",
        "            'mean_width': np.mean(bbox_widths),\n",
        "            'mean_height': np.mean(bbox_heights),\n",
        "            'mean_area': np.mean(bbox_areas),\n",
        "            'mean_aspect_ratio': np.mean(bbox_aspect_ratios),\n",
        "            'width_std': np.std(bbox_widths),\n",
        "            'height_std': np.std(bbox_heights),\n",
        "            'area_std': np.std(bbox_areas)\n",
        "        },\n",
        "        'objects_per_image_stats': {\n",
        "            'mean': np.mean(objects_per_image),\n",
        "            'median': np.median(objects_per_image),\n",
        "            'max': max(objects_per_image),\n",
        "            'min': min(objects_per_image),\n",
        "            'std': np.std(objects_per_image)\n",
        "        },\n",
        "        'depth_stats': depth_stats,\n",
        "        'depth_validity_ratio': len(valid_depths)/len(depth_maps)*100 if depth_maps else 0,\n",
        "        'total_images': len(train_dataset),\n",
        "        'total_objects': len(cls_labels)\n",
        "    }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100,\n",
        "                learning_rate=1e-4, device='cuda', save_path='model_checkpoint.pth',\n",
        "                early_stop_patience=3,scheduler_patience=10, scheduler_factor=0.5, class_weights=None, task_weights=None,\n",
        "                p_iou_threshold=0.5, n_iou_threshold=0.4):\n",
        "\n",
        "    # WandB config'e weights ekle\n",
        "    config = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": \"EfficientBasedMultiTask\",\n",
        "        \"dataset\": \"KITTI-2012\",\n",
        "        \"epochs\": num_epochs,\n",
        "    }\n",
        "\n",
        "    # Task weights'i config'e ekle\n",
        "    if task_weights:\n",
        "        config.update({\n",
        "            \"task_weights\": task_weights,\n",
        "            \"classification_weight\": task_weights.get(\"classification\", 1.0),\n",
        "            \"regression_weight\": task_weights.get(\"regression\", 1.0),\n",
        "            \"detection_depth_weight\": task_weights.get(\"detection_depth\", 1.0),\n",
        "            \"depth_map_weight\": task_weights.get(\"depth_map\", 1.0)\n",
        "        })\n",
        "\n",
        "    wandb.init(\n",
        "        entity=\"mehmeteminuludag-kirikkale-university\",\n",
        "        project=\"StajProjesi\",\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    criterion = MultiTaskCriterion(loss_weights=task_weights, pos_iou_threshold=p_iou_threshold, neg_iou_threshold=n_iou_threshold).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        list(model.parameters()) + list(criterion.parameters()),\n",
        "        lr=learning_rate, weight_decay=1e-5, eps=1e-8\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=scheduler_factor,\n",
        "        patience=scheduler_patience,\n",
        "        verbose=True\n",
        "    )\n",
        "    best_val_loss = 1.0\n",
        "    counter=0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_losses = {\n",
        "            'total': 0,\n",
        "            'classification': 0,\n",
        "            'regression': 0,\n",
        "            'depth': 0,\n",
        "            'depth_map': 0\n",
        "        }\n",
        "        train_metrics_accum = {\n",
        "            'Accuracy': 0,\n",
        "            'F1_score': 0,\n",
        "            'MSE': 0,\n",
        "            'RMSE': 0,\n",
        "            'mAP': 0,\n",
        "            'TotalLoss': 0,\n",
        "            'ClsLoss': 0\n",
        "        }\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Train')\n",
        "\n",
        "        for batch_idx, (images, targets) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            batch_size = len(targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model_outputs = model(images, mode=\"train\")\n",
        "            values = {}\n",
        "            values['cls_preds'] = model_outputs['cls_preds']\n",
        "            values['reg_preds'] = model_outputs['reg_preds']\n",
        "            values['anchors'] = model_outputs['anchors']\n",
        "            values['depth_pred'] = model_outputs['depth_pred']\n",
        "            values['targets'] = targets\n",
        "\n",
        "            losses, metrics = criterion(values)\n",
        "            losses['total'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # HatalarÄ± biriktirir\n",
        "            for key in train_losses:\n",
        "                if key in losses:\n",
        "                    if isinstance(losses[key], torch.Tensor):\n",
        "                        train_losses[key] += losses[key].item() * batch_size\n",
        "                    else:\n",
        "                        train_losses[key] += losses[key] * batch_size\n",
        "\n",
        "            # Metrikleri biriktir\n",
        "            for key in train_metrics_accum:\n",
        "                if key in metrics:\n",
        "                    if isinstance(metrics[key], torch.Tensor):\n",
        "                        train_metrics_accum[key] += metrics[key].item() * batch_size\n",
        "                    else:\n",
        "                        train_metrics_accum[key] += metrics[key] * batch_size\n",
        "\n",
        "            # DÃœZELTME 4: Memory cleanup\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Acc': f\"{train_metrics_accum['Accuracy']/((batch_idx+1)*batch_size): .3f}\",\n",
        "                'ClsLoss': f\"{train_losses['classification']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'F1': f\"{train_metrics_accum['F1_score']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'RMSE': f\"{train_metrics_accum['RMSE']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'mAP': f\"{train_metrics_accum['mAP']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                'TotalLoss': f\"{train_losses['total']/((batch_idx+1)*batch_size):.3f}\"\n",
        "            })\n",
        "\n",
        "        # DÃœZELTME 3: Loop dÄ±ÅŸÄ±na Ã§Ä±karÄ±ldÄ±\n",
        "        for key in train_losses:\n",
        "            train_losses[key] /= len(train_loader.dataset)\n",
        "\n",
        "        for key in train_metrics_accum:\n",
        "            train_metrics_accum[key] /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "\n",
        "        val_losses = {\n",
        "            'total': 0,\n",
        "            'classification': 0,\n",
        "            'regression': 0,\n",
        "            'depth': 0,\n",
        "            'depth_map': 0\n",
        "        }\n",
        "        val_metrics_accum = {\n",
        "            'Accuracy': 0,\n",
        "            'F1_score': 0,\n",
        "            'MSE': 0,\n",
        "            'RMSE': 0,\n",
        "            'mAP': 0,\n",
        "            'TotalLoss': 0,\n",
        "            'ClsLoss': 0\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            #model.train()\n",
        "            pbar2 = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation')\n",
        "\n",
        "            for batch_idx, (images, targets) in enumerate(pbar2):\n",
        "                images = images.to(device)\n",
        "                batch_size = len(targets)\n",
        "\n",
        "\n",
        "                model_outputs = model(images, mode=\"train\")\n",
        "                values = {}\n",
        "                values['cls_preds'] = model_outputs['cls_preds']\n",
        "                values['reg_preds'] = model_outputs['reg_preds']\n",
        "                values['anchors'] = model_outputs['anchors']\n",
        "                values['depth_pred'] = model_outputs['depth_pred']\n",
        "                values['targets'] = targets\n",
        "                losses, metrics = criterion(values)\n",
        "\n",
        "                # HatalarÄ± biriktirir\n",
        "                for key in val_losses:\n",
        "                    if key in losses:\n",
        "                        if isinstance(losses[key], torch.Tensor):\n",
        "                            val_losses[key] += losses[key].item() * batch_size\n",
        "                        else:\n",
        "                            val_losses[key] += losses[key] * batch_size\n",
        "\n",
        "                # Metrikleri biriktir\n",
        "                for key in val_metrics_accum:\n",
        "                    if key in metrics:\n",
        "                        if isinstance(metrics[key], torch.Tensor):\n",
        "                            val_metrics_accum[key] += metrics[key].item() * batch_size\n",
        "                        else:\n",
        "                            val_metrics_accum[key] += metrics[key] * batch_size\n",
        "\n",
        "                pbar2.set_postfix({\n",
        "                    'Acc': f\"{val_metrics_accum['Accuracy']/((batch_idx+1)*batch_size): .3f}\",\n",
        "                    'ClsLoss': f\"{val_losses['classification']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'F1': f\"{val_metrics_accum['F1_score']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'RMSE': f\"{val_metrics_accum['RMSE']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'mAP': f\"{val_metrics_accum['mAP']/((batch_idx+1)*batch_size):.3f}\",\n",
        "                    'TotalLoss': f\"{val_losses['total']/((batch_idx+1)*batch_size):.3f}\"\n",
        "                })\n",
        "\n",
        "        # DÃœZELTME 5: Validation loop dÄ±ÅŸÄ±na Ã§Ä±karÄ±ldÄ±\n",
        "        for key in val_losses:\n",
        "            val_losses[key] /= len(val_loader.dataset)\n",
        "\n",
        "        for key in val_metrics_accum:\n",
        "            val_metrics_accum[key] /= len(val_loader.dataset)\n",
        "\n",
        "\n",
        "        scheduler.step(val_losses['total'])\n",
        "\n",
        "        if float(val_losses['total']) < best_val_loss:\n",
        "          best_val_loss = float(val_losses['total'])\n",
        "          counter = 0  # Ä°yileÅŸme oldu, sÄ±fÄ±rla\n",
        "          torch.save({\n",
        "              'epoch': epoch + 1,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'criterion_state_dict': criterion.state_dict()\n",
        "          }, save_path)\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"No improvement in val loss for {counter} epochs.\")\n",
        "\n",
        "        # wandb loglarÄ± alÄ±nÄ±yor\n",
        "        wandb.log({\n",
        "            \"val/accuracy\": val_metrics_accum['Accuracy'],\n",
        "            \"val/classification_loss\": val_losses['classification'],\n",
        "            \"val/f1_score\": val_metrics_accum['F1_score'],\n",
        "            \"val/rmse\": val_metrics_accum['RMSE'],\n",
        "            \"val/map\": val_metrics_accum['mAP'],\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        }, step=epoch+1)\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/accuracy\": train_metrics_accum['Accuracy'],\n",
        "            \"train/classification_loss\": train_losses['classification'],\n",
        "            \"train/f1_score\": train_metrics_accum['F1_score'],\n",
        "            \"train/rmse\": train_metrics_accum['RMSE'],\n",
        "            \"train/map\": train_metrics_accum['mAP'],\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        }, step=epoch+1)\n",
        "\n",
        "\n",
        "\n",
        "        if counter >= early_stop_patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    wandb.finish()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "\n",
        "    def _apply_memory_efficient_nms(self, boxes, scores, classes):\n",
        "        if len(boxes) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device)\n",
        "        unique_classes = torch.unique(classes)\n",
        "        all_keep_indices = []\n",
        "        for cls in unique_classes:\n",
        "            if cls == 8:  # Skip DontCare\n",
        "                continue\n",
        "            cls_mask = classes == cls\n",
        "            if cls_mask.sum() == 0:\n",
        "                continue\n",
        "            cls_boxes = boxes[cls_mask]\n",
        "            cls_scores = scores[cls_mask]\n",
        "            cls_indices = torch.where(cls_mask)[0]\n",
        "            abs_boxes = cls_boxes * self.img_size\n",
        "            keep_cls = nms(abs_boxes, cls_scores, self.nms_threshold)\n",
        "            all_keep_indices.append(cls_indices[keep_cls])\n",
        "        if len(all_keep_indices) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device)\n",
        "        final_keep_indices = torch.cat(all_keep_indices)\n",
        "        if len(final_keep_indices) > self.max_detections_per_image:\n",
        "            keep_scores = scores[final_keep_indices]\n",
        "            top_indices = torch.topk(keep_scores, self.max_detections_per_image)[1]\n",
        "            final_keep_indices = final_keep_indices[top_indices]\n",
        "        return final_keep_indices\n",
        "\n",
        "    def _get_depth_values(self, depth_map, boxes):\n",
        "        if depth_map is None:\n",
        "            return [0.0] * len(boxes)\n",
        "        H, W = depth_map.shape\n",
        "        depths = []\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box\n",
        "            x1_pix = int(x1 * W)\n",
        "            y1_pix = int(y1 * H)\n",
        "            x2_pix = int(x2 * W)\n",
        "            y2_pix = int(y2 * H)\n",
        "            x1_pix = max(0, min(x1_pix, W-1))\n",
        "            y1_pix = max(0, min(y1_pix, H-1))\n",
        "            x2_pix = max(0, min(x2_pix, W-1))\n",
        "            y2_pix = max(0, min(y2_pix, H-1))\n",
        "            center_x = (x1_pix + x2_pix) // 2\n",
        "            center_y = (y1_pix + y2_pix) // 2\n",
        "            depth_value = depth_map[center_y, center_x].item() if depth_map is not None else 0.0\n",
        "            depths.append(depth_value)\n",
        "        return depths\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        x_out = self.conv1(x_cat)\n",
        "        attention_map = self.sigmoid(x_out)\n",
        "        return x * attention_map\n",
        "\n",
        "class EncoderBackBone(nn.Module):\n",
        "    def __init__(self,Ä°sPretreained=True):\n",
        "        super(EncoderBackBone,self).__init__()\n",
        "        efficient = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "        self.features = efficient.features\n",
        "        self.SAttention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):         # B,C,H,W\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        # Her iki frame iÃ§in Ã¶zellikler\n",
        "        for i, block in enumerate(self.features):\n",
        "            x = block(x)\n",
        "            if i > 2:  # C3'ten sonrasÄ± iÃ§in Spatial Attention\n",
        "                x = x * self.SAttention(x)\n",
        "            if i in [3,5,7]:\n",
        "                out = F.interpolate(x, size=256, mode='bilinear', align_corners=False)\n",
        "                outs.append(out)\n",
        "        return outs\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                 stride, padding, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return self.swish(x)\n",
        "\n",
        "class BiFPNBlock(nn.Module):\n",
        "    def __init__(self, channels, epsilon=1e-4):\n",
        "        super(BiFPNBlock, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.channels = channels\n",
        "\n",
        "        # Convolution layers for each level\n",
        "        self.conv_p3 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p4 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p5 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p6 = DepthwiseSeparableConv(channels, channels)\n",
        "        self.conv_p7 = DepthwiseSeparableConv(channels, channels)\n",
        "\n",
        "        # Weight parameters for feature fusion\n",
        "        self.w1 = nn.Parameter(torch.ones(2))\n",
        "        self.w2 = nn.Parameter(torch.ones(2))\n",
        "        self.w3 = nn.Parameter(torch.ones(2))\n",
        "        self.w4 = nn.Parameter(torch.ones(2))\n",
        "        self.w5 = nn.Parameter(torch.ones(3))\n",
        "        self.w6 = nn.Parameter(torch.ones(3))\n",
        "        self.w7 = nn.Parameter(torch.ones(3))\n",
        "        self.w8 = nn.Parameter(torch.ones(2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        P3, P4, P5, P6, P7 = inputs\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        w1 = F.relu(self.w1)\n",
        "        P6_td = (w1[0] * P6 + w1[1] * self.up_sampling(P7, P6.shape[-2:])) / (w1.sum() + self.epsilon)\n",
        "        P6_td = self.conv_p6(P6_td)\n",
        "\n",
        "        w2 = F.relu(self.w2)\n",
        "        P5_td = (w2[0] * P5 + w2[1] * self.up_sampling(P6_td, P5.shape[-2:])) / (w2.sum() + self.epsilon)\n",
        "        P5_td = self.conv_p5(P5_td)\n",
        "\n",
        "        w3 = F.relu(self.w3)\n",
        "        P4_td = (w3[0] * P4 + w3[1] * self.up_sampling(P5_td, P4.shape[-2:])) / (w3.sum() + self.epsilon)\n",
        "        P4_td = self.conv_p4(P4_td)\n",
        "\n",
        "        # Top-down pathway\n",
        "        w4 = F.relu(self.w4)\n",
        "        P3_out = (w4[0] * P3 + w4[1] * self.up_sampling(P4_td, P3.shape[-2:])) / (w4.sum() + self.epsilon)\n",
        "        P3_out = self.conv_p3(P3_out)\n",
        "\n",
        "        w5 = F.relu(self.w5)\n",
        "        P4_out = (w5[0] * P4 + w5[1] * P4_td + w5[2] * self.down_sampling(P3_out, P4.shape[-2:])) / (w5.sum() + self.epsilon)\n",
        "        P4_out = self.conv_p4(P4_out)\n",
        "\n",
        "        w6 = F.relu(self.w6)\n",
        "        P5_out = (w6[0] * P5 + w6[1] * P5_td + w6[2] * self.down_sampling(P4_out, P5.shape[-2:])) / (w6.sum() + self.epsilon)\n",
        "        P5_out = self.conv_p5(P5_out)\n",
        "\n",
        "        w7 = F.relu(self.w7)\n",
        "        P6_out = (w7[0] * P6 + w7[1] * P6_td + w7[2] * self.down_sampling(P5_out, P6.shape[-2:])) / (w7.sum() + self.epsilon)\n",
        "        P6_out = self.conv_p6(P6_out)\n",
        "\n",
        "        w8 = F.relu(self.w8)\n",
        "        P7_out = (w8[0] * P7 + w8[1] * self.down_sampling(P6_out, P7.shape[-2:])) / (w8.sum() + self.epsilon)\n",
        "        P7_out = self.conv_p7(P7_out)\n",
        "\n",
        "        return [P3_out, P4_out, P5_out, P6_out, P7_out]\n",
        "\n",
        "    def up_sampling(self, x, target_size):\n",
        "        return F.interpolate(x, size=target_size, mode='nearest')\n",
        "\n",
        "    def down_sampling(self, x, target_size):\n",
        "        if x.shape[-2:] == target_size:\n",
        "            return x\n",
        "        stride = x.shape[-1] // target_size[-1]\n",
        "        kernel_size = stride\n",
        "        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels=256, num_blocks=3):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Input projection layers\n",
        "        self.input_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, out_channels, 1, bias=False)\n",
        "            for in_ch in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # Additional P6 and P7 layers\n",
        "        self.p6_conv = nn.Conv2d(in_channels_list[-1], out_channels, 3, stride=2, padding=1)\n",
        "        self.p7_conv = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN blocks\n",
        "        self.bifpn_blocks = nn.ModuleList([\n",
        "            BiFPNBlock(out_channels) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Project input features\n",
        "        features = []\n",
        "        for i, feat in enumerate(inputs):\n",
        "            features.append(self.input_convs[i](feat))\n",
        "\n",
        "        # Create P6 and P7\n",
        "        P6 = self.p6_conv(inputs[-1])\n",
        "        P7 = self.p7_conv(P6)\n",
        "\n",
        "        # Initial feature list\n",
        "        pyramid_features = features + [P6, P7]\n",
        "\n",
        "        # Apply BiFPN blocks\n",
        "        for block in self.bifpn_blocks:\n",
        "            pyramid_features = block(pyramid_features)\n",
        "\n",
        "        return pyramid_features\n",
        "\n",
        "class NNConv3UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        return x\n",
        "\n",
        "class FusionBlock(nn.Module):\n",
        "    def __init__(self, fusion_type='add'):\n",
        "        super().__init__()\n",
        "        self.fusion_type = fusion_type\n",
        "\n",
        "    def forward(self, high_level, low_level):\n",
        "        if self.fusion_type == 'add':\n",
        "            return high_level + low_level\n",
        "        elif self.fusion_type == 'concat':\n",
        "            return torch.cat([high_level, low_level], dim=1)\n",
        "\n",
        "class PredictionDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels//2, out_channels, kernel_size=3, padding=1)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "class RTMonoDepthDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_channels=[48, 136, 384], decoder_channels=[256, 128, 64, 32]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Upsampling blocks\n",
        "        self.upconv2 = NNConv3UpBlock(encoder_channels[2], decoder_channels[0])  # F3 -> D2\n",
        "        self.upconv1 = NNConv3UpBlock(decoder_channels[0], decoder_channels[1])  # After fusion -> D1\n",
        "        self.upconv0 = NNConv3UpBlock(decoder_channels[1], decoder_channels[2])  # After fusion -> D0\n",
        "\n",
        "        # Projection layers to match dimensions for fusion\n",
        "        self.proj2 = nn.Conv2d(encoder_channels[1], decoder_channels[0], 1)  # F2 -> D2 channels\n",
        "        self.proj1 = nn.Conv2d(encoder_channels[0], decoder_channels[1], 1)  # F1 -> D1 channels\n",
        "\n",
        "        # Fusion blocks\n",
        "        self.fusion1 = FusionBlock('add')\n",
        "        self.fusion0 = FusionBlock('concat')\n",
        "\n",
        "        # Prediction decoders at each scale\n",
        "        self.decoder2 = PredictionDecoder(decoder_channels[0])\n",
        "        self.decoder1 = PredictionDecoder(decoder_channels[1])\n",
        "        # After concat: up1_resized (128) + f1_proj (128) = 256 channels\n",
        "        self.decoder0 = PredictionDecoder(decoder_channels[1] + decoder_channels[1])\n",
        "\n",
        "    def forward(self, features, inference_mode=False):\n",
        "        f1, f2, f3 = features  # [low_res -> high_res]\n",
        "        depth_maps = {}\n",
        "\n",
        "        # Level 2: Start from highest level feature\n",
        "        up2 = self.upconv2(f3)\n",
        "        depth_maps['depth_2'] = self.decoder2(up2)\n",
        "\n",
        "        # Level 1: Project F2 to match up2 channels and fuse\n",
        "        f2_proj = self.proj2(f2)\n",
        "        # Resize up2 to match f2 spatial dimensions\n",
        "        up2_resized = F.interpolate(up2, size=f2_proj.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        fused1 = self.fusion1(up2_resized, f2_proj)\n",
        "        up1 = self.upconv1(fused1)\n",
        "        depth_maps['depth_1'] = self.decoder1(up1)\n",
        "\n",
        "        # Level 0: Project F1 to match up1 channels and fuse\n",
        "        f1_proj = self.proj1(f1)\n",
        "        # Resize up1 to match f1 spatial dimensions\n",
        "        up1_resized = F.interpolate(up1, size=f1_proj.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        fused0 = self.fusion0(up1_resized, f1_proj)\n",
        "        depth_maps['depth_0'] = self.decoder0(fused0)\n",
        "\n",
        "        return depth_maps\n",
        "\n",
        "class DepthHead2(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=1):\n",
        "        super(DepthHead2, self).__init__()\n",
        "        # small refinement conv stack\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 3, in_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // 2, out_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, depth_features):\n",
        "        # Accept either dict from RTMonoDepthDecoder or list/tuple\n",
        "        if isinstance(depth_features, dict):\n",
        "            # prefer 'depth_0' as highest res; upsample others to its size\n",
        "            keys = ['depth_0', 'depth_1', 'depth_2']\n",
        "            maps = []\n",
        "            # find first existing key for target size\n",
        "            target = None\n",
        "            for k in keys:\n",
        "                if k in depth_features:\n",
        "                    target = depth_features[k].shape[2:]\n",
        "                    break\n",
        "            if target is None:\n",
        "                raise ValueError(\"depth_features dict empty or unexpected keys\")\n",
        "            for k in keys:\n",
        "                if k in depth_features:\n",
        "                    m = depth_features[k]\n",
        "                    if m.shape[2:] != target:\n",
        "                        m = F.interpolate(m, size=target, mode='bilinear', align_corners=False)\n",
        "                    maps.append(m)\n",
        "            # if less than 3 maps, duplicate last to keep consistent channels\n",
        "            while len(maps) < 3:\n",
        "                maps.append(maps[-1])\n",
        "        else:\n",
        "            # assume iterable: take first 3 or duplicate if fewer\n",
        "            maps = list(depth_features)\n",
        "            while len(maps) < 3:\n",
        "                maps.append(maps[-1])\n",
        "            # upsample to first map's size\n",
        "            target = maps[0].shape[2:]\n",
        "            maps = [m if m.shape[2:] == target else F.interpolate(m, size=target, mode='bilinear', align_corners=False) for m in maps[:3]]\n",
        "\n",
        "        # concat along channels and refine\n",
        "        concat = torch.cat(maps, dim=1)  # C= sum of channels\n",
        "        out = self.refine(concat)\n",
        "        return out  # [B,1,H,W] sigmoid-normalized\n",
        "\n",
        "class DepthHead(nn.Module):\n",
        "    def __init__(self, in_channels=1):  # Change to 1 to match input depth maps\n",
        "        super(DepthHead, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.weights = nn.Parameter(torch.ones(3))  # Adjust to 3 for the number of features (depth_2, depth_1, depth_0); previously 5, which may cause issues in zip()\n",
        "\n",
        "    def weighted_fusion(self, features, weights, target_size):\n",
        "        weights = F.softmax(weights, dim=0)\n",
        "        fused = None\n",
        "        for feat, weight in zip(features, weights): # features liste halinde : depthmap2 B,1,512,512 depthmap1 B,1,512,512 depthmap0 B,1,256,256\n",
        "            if feat.shape[2:] != target_size:\n",
        "                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
        "            if fused is None:\n",
        "                fused = weight * feat\n",
        "            else:\n",
        "                fused += weight * feat\n",
        "        return fused\n",
        "\n",
        "    def forward(self, features):\n",
        "        processed = [self.conv(feat) for feat in features]\n",
        "        target_size = processed[0].shape[2:]  # Or use processed[-1].shape[2:] for higher resolution (e.g., 512x512) if preferred\n",
        "        return self.weighted_fusion(processed, self.weights, target_size)\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels=256, num_anchors=3, num_classes=8):\n",
        "        super(DetectionHead, self).__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Classification head: her anchor iÃ§in sÄ±nÄ±f olasÄ±lÄ±klarÄ± (sigmoid / softmax)\n",
        "        self.cls_conv = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        # Regression head: her anchor iÃ§in bbox 4 koordinatÄ±\n",
        "        self.reg_conv = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        # istersen headlerde BatchNorm + Activation koyabilirsin\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: list of tensor, her biri [B, C, H, W]\n",
        "        returns:\n",
        "            cls_preds: [B, total_anchors, num_classes]\n",
        "            reg_preds: [B, total_anchors, 4]\n",
        "        \"\"\"\n",
        "        cls_outputs = []\n",
        "        reg_outputs = []\n",
        "\n",
        "        for feat in features:\n",
        "            # feat: [B, C, H, W]\n",
        "            cls_out = self.cls_conv(feat)  # [B, A*C, H, W]\n",
        "            reg_out = self.reg_conv(feat)  # [B, A*4, H, W]\n",
        "\n",
        "            B, _, H, W = cls_out.shape\n",
        "\n",
        "            # reshape: (B, A, C, H, W) â†’ (B, H*W*A, C)\n",
        "            cls_out = cls_out.view(B, self.num_anchors, self.num_classes, H, W)\n",
        "            cls_out = cls_out.permute(0, 3, 4, 1, 2).contiguous()  # B, H, W, A, C\n",
        "            cls_out = cls_out.view(B, -1, self.num_classes)         # B, (H*W*A), C\n",
        "\n",
        "            # regression benzer ÅŸekilde (B, A, 4, H, W) â†’ (B, H*W*A, 4)\n",
        "            reg_out = reg_out.view(B, self.num_anchors, 4, H, W)\n",
        "            reg_out = reg_out.permute(0, 3, 4, 1, 2).contiguous()  # B, H, W, A, 4\n",
        "            reg_out = reg_out.view(B, -1, 4)                       # B, (H*W*A), 4\n",
        "\n",
        "            cls_outputs.append(cls_out)\n",
        "            reg_outputs.append(reg_out)\n",
        "\n",
        "        # TÃ¼m seviyeleri birleÅŸtir\n",
        "        cls_preds = torch.cat(cls_outputs, dim=1)  # B, total_anchors, num_classes\n",
        "        reg_preds = torch.cat(reg_outputs, dim=1)  # B, total_anchors, 4\n",
        "\n",
        "        return cls_preds, reg_preds\n",
        "\n",
        "class MultiTaskHeads(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=256, num_anchors=3):\n",
        "        super(MultiTaskHeads, self).__init__()\n",
        "        # Depth heads unchanged (they expect depth_maps dict/list)\n",
        "        self.depth_head1 = DepthHead(in_channels=1)   # train fusion from depth_decoder outputs\n",
        "        self.depth_head2 = DepthHead2(in_channels=1)  # inference refinement\n",
        "        self.detection_head = DetectionHead(num_anchors=num_anchors)\n",
        "\n",
        "    def forward(self, bifpn_features, depth_features):\n",
        "        \"\"\"\n",
        "        bifpn_features: list [P3,P4,P5,P6,P7] each [B, C, H, W]\n",
        "        depth_features: dict from RTMonoDepthDecoder (depth_0,1,2)\n",
        "        returns:\n",
        "           classification: list(len=5) of [B, num_anchors*num_classes, H, W]\n",
        "           regression:     list(len=5) of [B, num_anchors*4, H, W]\n",
        "           depth: [B,1,Hd,Wd]\n",
        "        \"\"\"\n",
        "        # Depth: use decoder outputs (pixel-wise). Use depth_head1 in train, depth_head2 in inference\n",
        "        depth_list = [depth_features['depth_2'], depth_features['depth_1'], depth_features['depth_0']]\n",
        "\n",
        "        depth = self.depth_head1(depth_list)\n",
        "\n",
        "        cls_preds, reg_preds = self.detection_head(bifpn_features)\n",
        "\n",
        "        return {\n",
        "            'depth': depth,\n",
        "            'classification': cls_preds,\n",
        "            'regression': reg_preds\n",
        "        }\n",
        "\n",
        "class PostProcessor:\n",
        "    def __init__(self, num_classes=7, num_anchors=15, strides=[8, 16, 32, 64, 128]):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchors = num_anchors\n",
        "        self.strides = strides\n",
        "        # KITTI-specific anchor scales and ratios\n",
        "        self.anchor_scales = [[32, 48, 64], [64, 96, 128], [128, 192, 256], [256, 384, 512], [512, 768, 1024]]\n",
        "        self.anchor_ratios = [[0.5, 1.0, 2.0, 0.33, 3.0]] * 5  # Her feature map iÃ§in aynÄ±\n",
        "        self.strides = [8, 16, 32, 64, 128]\n",
        "        self.anchor_generator = AnchorGenerator(sizes=self.anchor_scales, aspect_ratios=self.anchor_ratios)\n",
        "    def generate_anchors_for_level(self, feat, level_idx):\n",
        "        \"\"\"\n",
        "        feat: feature map tensor [B, C, H, W]\n",
        "        level_idx: hangi feature map seviyesi (0..4)\n",
        "        \"\"\"\n",
        "    def generate_all_anchors(self, bifpn_features,image_list):\n",
        "        anchors_per_level = self.anchor_generator(feature_maps=bifpn_features,image_list=image_list)\n",
        "        return torch.cat(anchors_per_level, dim=0)  # [N_total, 4]\n",
        "\n",
        "class CompleteMultiTaskModel(nn.Module):\n",
        "    def __init__(self, Ä°sPretreained=True, num_classes=7, bifpn_channels=256, bifpn_blocks=3,\n",
        "                 confidence_threshold=0.1, max_detections=100, num_anchors=15, MAX_CANDIDATES=1000):  # Threshold dÃ¼ÅŸÃ¼rÃ¼ldÃ¼\n",
        "        super(CompleteMultiTaskModel, self).__init__()\n",
        "\n",
        "        # Model components\n",
        "        self.encoder = EncoderBackBone(Ä°sPretreained)\n",
        "        in_channels_list = [48, 136, 384]\n",
        "        self.bifpn = BiFPN(in_channels_list, bifpn_channels, bifpn_blocks)\n",
        "        self.depth_decoder = RTMonoDepthDecoder()\n",
        "        self.multi_head = MultiTaskHeads(num_classes, bifpn_channels, num_anchors)\n",
        "\n",
        "        # Parameters\n",
        "        self.num_classes = num_classes\n",
        "        self.conf_thresh = confidence_threshold\n",
        "        self.max_detections = max_detections\n",
        "        self.num_anchors = num_anchors\n",
        "        self.strides = [8, 16, 32, 64, 128]\n",
        "        self.MAX_CANDIDATES = MAX_CANDIDATES\n",
        "        self.img_size = 256  # KITTI resized image size\n",
        "\n",
        "        # Post processor\n",
        "        self.post_processor = PostProcessor(num_classes, num_anchors, self.strides)\n",
        "\n",
        "    def forward(self, images, targets=None, mode=\"train\"):\n",
        "        B = images.shape[0]\n",
        "\n",
        "        # DÃœZELTME: Model mode'u doÄŸru ayarla\n",
        "        if mode == \"inference\":\n",
        "            self.eval()  # Inference iÃ§in eval mode\n",
        "        else:\n",
        "            self.train()  # Training iÃ§in train mode\n",
        "\n",
        "        with torch.set_grad_enabled(mode == \"train\"):  # Gradient sadece training'de\n",
        "            # Forward pass\n",
        "            backbone_features = self.encoder(images)\n",
        "            bifpn_features = self.bifpn(backbone_features)\n",
        "            depth_maps = self.depth_decoder(backbone_features, inference_mode=(mode != \"train\"))\n",
        "            raw_preds = self.multi_head(bifpn_features, depth_maps)\n",
        "            image_sizes = [img.shape[-2:] for img in images]  # Her image iÃ§in (H,W)\n",
        "            image_list = ImageList(images, image_sizes=image_sizes)\n",
        "            anchors_all = self.post_processor.generate_all_anchors(bifpn_features,image_list)\n",
        "            cls_preds = raw_preds['classification']\n",
        "            reg_preds = raw_preds['regression']\n",
        "            depth_pred = raw_preds['depth']\n",
        "            if mode == \"train\":\n",
        "                return {\n",
        "                    \"cls_preds\": cls_preds,\n",
        "                    \"reg_preds\": reg_preds,\n",
        "                    \"anchors\": anchors_all,\n",
        "                    \"depth_pred\": depth_pred,\n",
        "                    \"targets\": targets\n",
        "                }\n",
        "\n",
        "            elif mode == \"inference\":\n",
        "                return self._inference_postprocess_fixed(cls_preds,reg_preds,depth_pred,anchors_all,images)\n",
        "\n",
        "    def _inference_postprocess_fixed(self, cls_preds, reg_preds, depth_pred, anchors_all, images):\n",
        "        \"\"\"DÃ¼zeltilmiÅŸ inference postprocessing\"\"\"\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "        img_h, img_w = images.shape[2], images.shape[3]  # Model input size\n",
        "\n",
        "        # Generate anchors (pixel coordinates)\n",
        "        results = []\n",
        "\n",
        "        for b in range(B):\n",
        "            try:\n",
        "                batch_cls = cls_preds[b]  # [N, num_classes]\n",
        "                batch_reg = reg_preds[b]  # [N, 4]\n",
        "\n",
        "                # 1. Confidence filtering\n",
        "                cls_probs = torch.softmax(batch_cls, dim=-1)\n",
        "                max_probs, pred_labels = torch.max(cls_probs, dim=-1)\n",
        "\n",
        "                # Confidence threshold\n",
        "                conf_mask = max_probs > self.conf_thresh\n",
        "                if conf_mask.sum() == 0:\n",
        "                    results.append(self._get_empty_result_single(device))\n",
        "                    continue\n",
        "\n",
        "                # Filter predictions\n",
        "                filtered_probs = max_probs[conf_mask]\n",
        "                filtered_labels = pred_labels[conf_mask]\n",
        "                filtered_reg = batch_reg[conf_mask]\n",
        "                filtered_anchors = anchors_all[conf_mask]\n",
        "\n",
        "                # 2. Top-K selection for memory efficiency\n",
        "                if len(filtered_probs) > self.MAX_CANDIDATES:\n",
        "                    top_k_scores, top_k_idx = torch.topk(filtered_probs, self.MAX_CANDIDATES)\n",
        "                    filtered_probs = top_k_scores\n",
        "                    filtered_labels = filtered_labels[top_k_idx]\n",
        "                    filtered_reg = filtered_reg[top_k_idx]\n",
        "                    filtered_anchors = filtered_anchors[top_k_idx]\n",
        "                # 3. Decode boxes - DÃœZELTME: DoÄŸru koordinat sistemi\n",
        "                decoded_boxes = self._decode_boxes_corrected(filtered_anchors, filtered_reg)\n",
        "\n",
        "                decoded_boxes_normalized = decoded_boxes / torch.tensor([256, 256, 256, 256], device=decoded_boxes.device)\n",
        "                # SÄ±nÄ±r dÄ±ÅŸÄ±nÄ± engelle\n",
        "                decoded_boxes_normalized = decoded_boxes_normalized.clamp(0, 1)\n",
        "                boxes = decoded_boxes_normalized * torch.tensor([img_w, img_h, img_w, img_h], device=decoded_boxes_normalized.device)\n",
        "\n",
        "\n",
        "                # 5. Box validation\n",
        "                valid_boxes, valid_scores, valid_labels = self._validate_boxes_fixed(\n",
        "                    boxes, filtered_probs, filtered_labels\n",
        "                )\n",
        "                print(str(valid_boxes)+ \"--\"+str(valid_labels)+\"--\"+str(valid_scores))\n",
        "                if len(valid_scores) == 0:\n",
        "                    results.append(self._get_empty_result_single(device))\n",
        "                    continue\n",
        "\n",
        "                # 6. NMS - DÃœZELTME: Daha dÃ¼ÅŸÃ¼k threshold\n",
        "                final_boxes, final_scores, final_labels = self._apply_class_wise_nms(\n",
        "                    valid_boxes, valid_scores, valid_labels, iou_threshold=0.3  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "                # 8. Depth extraction\n",
        "                depth_values = None\n",
        "                if depth_pred is not None:\n",
        "                    try:\n",
        "                        depth_values = self._extract_depth_values(\n",
        "                            depth_pred[b], final_boxes\n",
        "                        )\n",
        "                    except:\n",
        "                        depth_values = [0.0] * len(final_boxes)\n",
        "\n",
        "                results.append({\n",
        "                    \"boxes\": final_boxes,  # Pixel coordinates for visualization\n",
        "                    \"scores\": final_scores,\n",
        "                    \"labels\": final_labels,\n",
        "                    \"depth\": depth_values\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch {b}: {e}\")\n",
        "                results.append(self._get_empty_result_single(device))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _extract_depth_values(self, depth_map, final_boxes_pixel):\n",
        "      depth_values = []\n",
        "      for box in final_boxes_pixel:\n",
        "          x1, y1, x2, y2 = box\n",
        "          center_x_pix = int((x1 + x2) / 2 )\n",
        "          center_y_pix = int((y1 + y2) / 2 )\n",
        "\n",
        "          # SÄ±nÄ±r kontrolÃ¼\n",
        "          center_x_pix = max(0, min(center_x_pix, -1))\n",
        "          center_y_pix = max(0, min(center_y_pix, -1))\n",
        "\n",
        "          depth_value = depth_map[center_y_pix, center_x_pix].item()\n",
        "          depth_values.append(depth_value * 80.0)  # GerÃ§ek mesafeye Ã§evir\n",
        "      return depth_values\n",
        "\n",
        "    def _apply_class_wise_nms(self, boxes, scores, labels, iou_threshold=0.2):\n",
        "      final_boxes = []\n",
        "      final_scores = []\n",
        "      final_labels = []\n",
        "\n",
        "      unique_labels = labels.unique()\n",
        "      for lbl in unique_labels:\n",
        "          mask = labels == lbl\n",
        "          boxes_lbl = boxes[mask]\n",
        "          scores_lbl = scores[mask]\n",
        "\n",
        "          keep = nms(boxes_lbl, scores_lbl, iou_threshold)\n",
        "\n",
        "          final_boxes.append(boxes_lbl[keep])\n",
        "          final_scores.append(scores_lbl[keep])\n",
        "          final_labels.append(labels[mask][keep])\n",
        "\n",
        "      if final_boxes:\n",
        "          final_boxes = torch.cat(final_boxes)\n",
        "          final_scores = torch.cat(final_scores)\n",
        "          final_labels = torch.cat(final_labels)\n",
        "      else:\n",
        "          final_boxes = torch.empty((0, 4))\n",
        "          final_scores = torch.empty((0,))\n",
        "          final_labels = torch.empty((0,), dtype=torch.long)\n",
        "\n",
        "      return final_boxes, final_scores, final_labels\n",
        "\n",
        "    def _decode_boxes_corrected(self, anchors, deltas): # x1,y1,x2,y2 anchors formatÄ± tx,ty,tw,th deltas formatÄ± ikiside piksel cinsinde\n",
        "        if anchors.numel() == 0:\n",
        "            return torch.zeros((0, 4), device=anchors.device)\n",
        "\n",
        "        # Anchor boyut ve merkezleri\n",
        "        anchor_widths  = anchors[:, 2] - anchors[:, 0]\n",
        "        anchor_heights = anchors[:, 3] - anchors[:, 1]\n",
        "        anchor_ctr_x   = anchors[:, 0] + 0.5 * anchor_widths\n",
        "        anchor_ctr_y   = anchors[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "\n",
        "        # Delta deÄŸerleri\n",
        "        dx, dy, dw, dh = deltas[:, 0], deltas[:, 1], deltas[:, 2], deltas[:, 3]\n",
        "\n",
        "        # PatlamayÄ± Ã¶nlemek iÃ§in geniÅŸlik/yÃ¼kseklik log-scale clamp\n",
        "        dw = torch.clamp(dw, max=4.0)\n",
        "        dh = torch.clamp(dh, max=4.0)\n",
        "\n",
        "        # DeltalarÄ± uygula\n",
        "        pred_ctr_x = dx * anchor_widths + anchor_ctr_x\n",
        "        pred_ctr_y = dy * anchor_heights + anchor_ctr_y\n",
        "        pred_w = torch.exp(dw) * anchor_widths\n",
        "        pred_h = torch.exp(dh) * anchor_heights\n",
        "\n",
        "        # KÃ¶ÅŸe formatÄ±na Ã§evir\n",
        "        x1 = pred_ctr_x - 0.5 * pred_w\n",
        "        y1 = pred_ctr_y - 0.5 * pred_h\n",
        "        x2 = pred_ctr_x + 0.5 * pred_w\n",
        "        y2 = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "        return torch.stack((x1, y1, x2, y2), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def _validate_boxes_fixed(self, boxes, scores, labels):\n",
        "        \"\"\"Normalize koordinatlarda box validation\"\"\"\n",
        "        if len(boxes) == 0:\n",
        "            return self._get_empty_tensors(boxes.device)\n",
        "\n",
        "        # Geometric validation\n",
        "        valid_geom = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
        "\n",
        "        # Size validation (normalized coordinates)\n",
        "        box_w = boxes[:, 2] - boxes[:, 0]\n",
        "        box_h = boxes[:, 3] - boxes[:, 1]\n",
        "        min_size = 0.001  # 1% of image\n",
        "        max_size = 1  # 95% of image\n",
        "        valid_size = (box_w >= min_size) & (box_h >= min_size) & \\\n",
        "                     (box_w <= max_size) & (box_h <= max_size)\n",
        "\n",
        "        # Bounds validation (normalized)\n",
        "        margin = 0.05  # 5% margin\n",
        "        valid_bounds = (boxes[:, 0] >= -margin) & (boxes[:, 1] >= -margin) & \\\n",
        "                      (boxes[:, 2] <= 1 + margin) & (boxes[:, 3] <= 1 + margin)\n",
        "\n",
        "        valid_mask = valid_geom & valid_size & valid_bounds\n",
        "\n",
        "        if valid_mask.sum() == 0:\n",
        "            return self._get_empty_tensors(boxes.device)\n",
        "\n",
        "        boxes = boxes[valid_mask]\n",
        "        scores = scores[valid_mask]\n",
        "        labels = labels[valid_mask]\n",
        "        boxes = torch.clamp(boxes, 0, 1)\n",
        "\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def _get_empty_tensors(self, device):\n",
        "        return (torch.zeros((0, 4), device=device),\n",
        "                torch.zeros((0,), device=device),\n",
        "                torch.zeros((0,), dtype=torch.long, device=device))\n",
        "\n",
        "    def _get_empty_result_single(self, device):\n",
        "        return {\n",
        "            \"boxes\": torch.zeros((0, 4), device=device),\n",
        "            \"scores\": torch.zeros((0,), device=device),\n",
        "            \"labels\": torch.zeros((0,), device=device),\n",
        "            \"depth\": None\n",
        "        }\n",
        "\n",
        "class MultiTaskCriterion(nn.Module):\n",
        "    def __init__(self, num_classes=7, loss_weights=None, device='cuda',\n",
        "                 pos_iou_threshold=0.5, neg_iou_threshold=0.3, img_size=256):\n",
        "        super(MultiTaskCriterion, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.loss_weights = loss_weights if loss_weights else {\n",
        "            'classification': 1.0,\n",
        "            'regression': 1.0,\n",
        "            'depth': 1.0,\n",
        "            'depth_map': 0.1\n",
        "        }\n",
        "\n",
        "        self.cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        self.reg_criterion = nn.SmoothL1Loss(reduction='none')\n",
        "        self.depth_criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "        self.pos_iou_threshold = pos_iou_threshold\n",
        "        self.neg_iou_threshold = neg_iou_threshold\n",
        "\n",
        "    def forward(self, model_output):\n",
        "        cls_preds = model_output['cls_preds']\n",
        "        reg_preds = model_output['reg_preds']\n",
        "        anchors = model_output['anchors']\n",
        "        depth_pred = model_output['depth_pred']\n",
        "        targets = model_output['targets']\n",
        "        batch_size = cls_preds.size(0)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        cls_loss_sum = 0.0\n",
        "        reg_loss_sum = 0.0\n",
        "        depth_loss_sum = 0.0\n",
        "        depth_map_loss_val = 0.0\n",
        "\n",
        "        # DÃœZELTME: Object-based metrikler iÃ§in\n",
        "        total_objects = 0\n",
        "        detected_objects = 0\n",
        "        correct_detections = 0\n",
        "        depth_errors = []\n",
        "        valid_samples = 0\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            try:\n",
        "                if isinstance(targets, list):\n",
        "                    if batch_idx >= len(targets):\n",
        "                        continue\n",
        "                    target = targets[batch_idx]\n",
        "                    if isinstance(target, torch.Tensor):\n",
        "                        if target.numel() == 0 or target.size(0) == 0:\n",
        "                            continue\n",
        "                        target = target.to(self.device)\n",
        "                    else:\n",
        "                        continue\n",
        "                else:\n",
        "                    if batch_idx not in targets or len(targets[batch_idx]) == 0:\n",
        "                        continue\n",
        "                    target = targets[batch_idx].to(self.device)\n",
        "\n",
        "                gt_classes = target[:, 0].long()\n",
        "                gt_boxes = target[:, 1:5]\n",
        "                gt_depths = target[:, 5] if target.size(1) > 5 else None\n",
        "\n",
        "                batch_cls_preds = cls_preds[batch_idx]\n",
        "                batch_reg_preds = reg_preds[batch_idx]\n",
        "                batch_depth_preds = depth_pred[batch_idx] if depth_pred is not None else None\n",
        "\n",
        "                # Anchor assignment (loss iÃ§in gerekli)\n",
        "                pos_indices, neg_indices, matched_gt_indices = self._assign_targets_to_anchors(\n",
        "                    anchors, gt_boxes, gt_classes\n",
        "                )\n",
        "\n",
        "                # Classification Loss (anchor-based, loss iÃ§in)\n",
        "                if len(pos_indices) > 0:\n",
        "                    cls_loss = self._compute_classification_loss(\n",
        "                        batch_cls_preds, gt_classes, pos_indices, neg_indices, matched_gt_indices\n",
        "                    )\n",
        "                    if cls_loss is not None:\n",
        "                        total_loss += cls_loss * self.loss_weights['classification']\n",
        "                        cls_loss_sum += cls_loss.detach().item()\n",
        "\n",
        "                # Regression Loss (anchor-based, loss iÃ§in)\n",
        "                if len(pos_indices) > 0:\n",
        "                    reg_loss = self._compute_regression_loss(\n",
        "                        batch_reg_preds, gt_boxes, anchors, pos_indices, matched_gt_indices\n",
        "                    )\n",
        "                    if reg_loss is not None:\n",
        "                        total_loss += reg_loss * self.loss_weights['regression']\n",
        "                        reg_loss_sum += reg_loss.detach().item()\n",
        "\n",
        "                # Depth Loss (anchor-based, loss iÃ§in)\n",
        "                if gt_depths is not None and batch_depth_preds is not None:\n",
        "                    depth_loss = self._compute_depth_loss_normalized(\n",
        "                        batch_depth_preds, gt_depths, gt_boxes\n",
        "                    )\n",
        "                    if depth_loss is not None:\n",
        "                        total_loss += depth_loss * self.loss_weights['depth']\n",
        "                        depth_loss_sum += depth_loss.detach().item()\n",
        "\n",
        "                # DÃœZELTME: Object-based evaluation\n",
        "                obj_metrics = self._evaluate_object_detection(\n",
        "                    batch_cls_preds, batch_reg_preds, anchors,\n",
        "                    gt_classes, gt_boxes, gt_depths, batch_depth_preds\n",
        "                )\n",
        "\n",
        "                total_objects += obj_metrics['total_objects']\n",
        "                detected_objects += obj_metrics['detected_objects']\n",
        "                correct_detections += obj_metrics['correct_detections']\n",
        "                if obj_metrics['depth_errors']:\n",
        "                    depth_errors.extend(obj_metrics['depth_errors'])\n",
        "\n",
        "                valid_samples += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Depth map smoothness loss\n",
        "        if depth_pred is not None:\n",
        "            depth_map_loss = self._compute_depth_smoothness_loss(depth_pred)\n",
        "            total_loss += depth_map_loss * self.loss_weights['depth_map']\n",
        "            depth_map_loss_val = depth_map_loss.detach().item()\n",
        "\n",
        "        # DÃœZELTME: Object-based metrikler\n",
        "        object_accuracy = correct_detections / max(total_objects, 1)\n",
        "        object_precision = correct_detections / max(detected_objects, 1)\n",
        "        object_recall = correct_detections / max(total_objects, 1)\n",
        "        object_f1 = 2 * (object_precision * object_recall) / max(object_precision + object_recall, 1e-6)\n",
        "\n",
        "        avg_depth_error = np.mean(depth_errors) if depth_errors else 0.0\n",
        "        rmse_depth = np.sqrt(avg_depth_error) if depth_errors else 0.0\n",
        "\n",
        "        losses = {\n",
        "            'total': total_loss,\n",
        "            'classification': cls_loss_sum / max(valid_samples, 1),\n",
        "            'regression': reg_loss_sum / max(valid_samples, 1),\n",
        "            'depth': depth_loss_sum / max(valid_samples, 1),\n",
        "            'depth_map': depth_map_loss_val\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            'Accuracy': object_accuracy,  # ArtÄ±k object-based\n",
        "            'F1_score': object_f1,        # ArtÄ±k object-based\n",
        "            'MSE': avg_depth_error,\n",
        "            'RMSE': rmse_depth,\n",
        "            'mAP': object_precision,      # BasitleÅŸtirilmiÅŸ object precision\n",
        "            'TotalLoss': total_loss.detach().item() if isinstance(total_loss, torch.Tensor) else total_loss,\n",
        "            'ClsLoss': cls_loss_sum / max(valid_samples, 1),\n",
        "            'Precision': object_precision,\n",
        "            'Recall': object_recall,\n",
        "            'TotalObjects': total_objects,\n",
        "            'DetectedObjects': detected_objects,\n",
        "            'CorrectDetections': correct_detections\n",
        "        }\n",
        "\n",
        "        return losses, metrics\n",
        "\n",
        "    def _evaluate_object_detection(self, cls_preds, reg_preds, anchors, gt_classes, gt_boxes, gt_depths=None, depth_pred=None):\n",
        "        \"\"\"\n",
        "        DÃœZELTME: Object-based detection evaluation\n",
        "        Her GT object iÃ§in en iyi anchor'u bulur ve deÄŸerlendirir\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            total_objects = len(gt_boxes)\n",
        "            detected_objects = 0\n",
        "            correct_detections = 0\n",
        "            depth_errors = []\n",
        "\n",
        "            if total_objects == 0:\n",
        "                return {\n",
        "                    'total_objects': 0,\n",
        "                    'detected_objects': 0,\n",
        "                    'correct_detections': 0,\n",
        "                    'depth_errors': []\n",
        "                }\n",
        "\n",
        "            # Her GT object iÃ§in en iyi anchor'u bul\n",
        "            anchors_norm = anchors.clone()\n",
        "            anchors_norm[:, [0,2]] /= 1242\n",
        "            anchors_norm[:, [1,3]] /= 375\n",
        "\n",
        "            ious = self.bbox_iou(anchors_norm, gt_boxes)  # [N_anchors, N_gt]\n",
        "\n",
        "            for gt_idx in range(len(gt_boxes)):\n",
        "                # Bu GT object iÃ§in en yÃ¼ksek IoU'ya sahip anchor\n",
        "                gt_ious = ious[:, gt_idx]\n",
        "                best_anchor_idx = torch.argmax(gt_ious)\n",
        "                best_iou = gt_ious[best_anchor_idx]\n",
        "\n",
        "                # EÄŸer IoU yeterince yÃ¼ksekse detection var sayÄ±yoruz\n",
        "                if best_iou >= self.pos_iou_threshold:\n",
        "                    detected_objects += 1\n",
        "\n",
        "                    # Classification doÄŸruluÄŸunu kontrol et\n",
        "                    pred_cls = torch.argmax(cls_preds[best_anchor_idx])\n",
        "                    gt_cls = gt_classes[gt_idx]\n",
        "\n",
        "                    if pred_cls == gt_cls:\n",
        "                        correct_detections += 1\n",
        "\n",
        "                    # Depth error hesapla\n",
        "                    if gt_depths is not None and depth_pred is not None:\n",
        "                        gt_box = gt_boxes[gt_idx]\n",
        "                        gt_depth = gt_depths[gt_idx]\n",
        "\n",
        "                        # Depth map'ten center pixel'Ä± sample et\n",
        "                        if depth_pred.dim() == 3:\n",
        "                            depth_map = depth_pred[0]\n",
        "                        else:\n",
        "                            depth_map = depth_pred\n",
        "\n",
        "                        H, W = depth_map.shape\n",
        "                        center_x = int((gt_box[0] + gt_box[2]) / 2 * W)\n",
        "                        center_y = int((gt_box[1] + gt_box[3]) / 2 * H)\n",
        "                        center_x = max(0, min(center_x, W-1))\n",
        "                        center_y = max(0, min(center_y, H-1))\n",
        "\n",
        "                        pred_depth = depth_map[center_y, center_x].clamp(0, 1)\n",
        "                        depth_error = F.mse_loss(pred_depth, gt_depth.clamp(0, 1)).item()\n",
        "                        depth_errors.append(depth_error)\n",
        "\n",
        "            return {\n",
        "                'total_objects': total_objects,\n",
        "                'detected_objects': detected_objects,\n",
        "                'correct_detections': correct_detections,\n",
        "                'depth_errors': depth_errors\n",
        "            }\n",
        "\n",
        "    def _compute_classification_loss(self, cls_preds, gt_classes, pos_indices, neg_indices, matched_gt_indices):\n",
        "        \"\"\"Classification loss - anchor based (loss iÃ§in)\"\"\"\n",
        "        if len(pos_indices) == 0:\n",
        "            return None\n",
        "\n",
        "        pos_cls_preds = cls_preds[pos_indices]\n",
        "        pos_gt_classes = gt_classes[matched_gt_indices]\n",
        "        pos_loss = self.focal_loss(pos_cls_preds, pos_gt_classes)\n",
        "\n",
        "        if len(neg_indices) > 0:\n",
        "            max_neg_samples = min(len(neg_indices), len(pos_indices) * 3)\n",
        "            neg_cls_preds = cls_preds[neg_indices]\n",
        "            neg_scores = torch.max(torch.softmax(neg_cls_preds, dim=1), dim=1)[0]\n",
        "            _, hard_neg_indices = torch.topk(neg_scores, max_neg_samples, largest=True)\n",
        "\n",
        "            selected_neg_indices = neg_indices[hard_neg_indices]\n",
        "            selected_neg_cls_preds = cls_preds[selected_neg_indices]\n",
        "            neg_gt_classes = torch.zeros(len(selected_neg_indices), dtype=torch.long, device=self.device)\n",
        "            neg_loss = self.focal_loss(selected_neg_cls_preds, neg_gt_classes)\n",
        "            total_cls_loss = pos_loss + neg_loss\n",
        "        else:\n",
        "            total_cls_loss = pos_loss\n",
        "\n",
        "        return total_cls_loss\n",
        "\n",
        "    def _compute_regression_loss(self, reg_preds, gt_boxes, anchors, pos_indices, matched_gt_indices):\n",
        "        \"\"\"Regression loss - anchor based (loss iÃ§in)\"\"\"\n",
        "        if len(pos_indices) == 0:\n",
        "            return None\n",
        "\n",
        "        pos_anchors = anchors[pos_indices]\n",
        "        pos_reg_preds = reg_preds[pos_indices]\n",
        "        pos_gt_boxes = gt_boxes[matched_gt_indices]\n",
        "\n",
        "        # Normalize anchors\n",
        "        pos_anchors_norm = pos_anchors.clone()\n",
        "        pos_anchors_norm[:, [0,2]] /= 1242\n",
        "        pos_anchors_norm[:, [1,3]] /= 375\n",
        "\n",
        "        pos_gt_encoded = self._encode_boxes(pos_anchors_norm, pos_gt_boxes)\n",
        "        reg_loss = self.reg_criterion(pos_reg_preds, pos_gt_encoded).mean()\n",
        "\n",
        "        return reg_loss\n",
        "\n",
        "    def _compute_depth_loss_normalized(self, depth_pred, gt_depths, gt_boxes):\n",
        "        \"\"\"Depth loss - anchor based (loss iÃ§in)\"\"\"\n",
        "        if len(gt_depths) == 0:\n",
        "            return None\n",
        "\n",
        "        if depth_pred.dim() == 3:\n",
        "            depth_map = depth_pred[0]\n",
        "        else:\n",
        "            depth_map = depth_pred\n",
        "\n",
        "        H, W = depth_map.shape\n",
        "        sampled_depths = []\n",
        "        target_depths = []\n",
        "\n",
        "        for gt_box, gt_depth in zip(gt_boxes, gt_depths):\n",
        "            x1, y1, x2, y2 = gt_box\n",
        "\n",
        "            x1_pix = int(x1 * W)\n",
        "            y1_pix = int(y1 * H)\n",
        "            x2_pix = int(x2 * W)\n",
        "            y2_pix = int(y2 * H)\n",
        "\n",
        "            x1_pix = max(0, min(x1_pix, W-1))\n",
        "            y1_pix = max(0, min(y1_pix, H-1))\n",
        "            x2_pix = max(0, min(x2_pix, W-1))\n",
        "            y2_pix = max(0, min(y2_pix, H-1))\n",
        "\n",
        "            center_y = (y1_pix + y2_pix) // 2\n",
        "            center_x = (x1_pix + x2_pix) // 2\n",
        "\n",
        "            sampled_depth = depth_map[center_y, center_x]\n",
        "            sampled_depth = torch.clamp(sampled_depth, 0, 1)\n",
        "            sampled_depths.append(sampled_depth)\n",
        "            target_depths.append(gt_depth.clamp(0, 1))\n",
        "\n",
        "        if len(sampled_depths) == 0:\n",
        "            return None\n",
        "\n",
        "        sampled_depths = torch.stack(sampled_depths)\n",
        "        target_depths = torch.stack(target_depths)\n",
        "        depth_loss = self.depth_criterion(sampled_depths, target_depths).mean()\n",
        "\n",
        "        return depth_loss\n",
        "\n",
        "    def _compute_depth_smoothness_loss(self, depth_pred):\n",
        "        \"\"\"Compute depth map smoothness loss\"\"\"\n",
        "        grad_x = torch.abs(depth_pred[:, :, :, :-1] - depth_pred[:, :, :, 1:])\n",
        "        grad_y = torch.abs(depth_pred[:, :, :-1, :] - depth_pred[:, :, 1:, :])\n",
        "        smoothness_loss = grad_x.mean() + grad_y.mean()\n",
        "        return smoothness_loss\n",
        "\n",
        "    def _assign_targets_to_anchors(self, anchors, gt_boxes, gt_classes):\n",
        "        \"\"\"Assign ground truth to anchors based on IoU\"\"\"\n",
        "        if len(gt_boxes) == 0:\n",
        "            return torch.tensor([], dtype=torch.long, device=self.device), \\\n",
        "                   torch.tensor([], dtype=torch.long, device=self.device), \\\n",
        "                   torch.tensor([], dtype=torch.long, device=self.device)\n",
        "\n",
        "        anchors_norm = anchors.clone()\n",
        "        anchors_norm[:, [0,2]] /= 1242\n",
        "        anchors_norm[:, [1,3]] /= 375\n",
        "        ious = self.bbox_iou(anchors_norm, gt_boxes)\n",
        "        max_ious, matched_gt_indices = torch.max(ious, dim=1)\n",
        "\n",
        "        pos_mask = max_ious >= self.pos_iou_threshold\n",
        "        neg_mask = max_ious < self.neg_iou_threshold\n",
        "\n",
        "        pos_indices = torch.where(pos_mask)[0]\n",
        "        neg_indices = torch.where(neg_mask)[0]\n",
        "        matched_gt_indices = matched_gt_indices[pos_indices]\n",
        "\n",
        "        return pos_indices, neg_indices, matched_gt_indices\n",
        "\n",
        "    def _encode_boxes(self, anchors, gt_boxes):\n",
        "        \"\"\"Box encoding - normalize koordinatlarda\"\"\"\n",
        "        anchor_widths = anchors[:, 2] - anchors[:, 0]\n",
        "        anchor_heights = anchors[:, 3] - anchors[:, 1]\n",
        "        anchor_ctr_x = anchors[:, 0] + 0.5 * anchor_widths\n",
        "        anchor_ctr_y = anchors[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "        gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "        gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "        gt_ctr_x = gt_boxes[:, 0] + 0.5 * gt_widths\n",
        "        gt_ctr_y = gt_boxes[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "        dx = (gt_ctr_x - anchor_ctr_x) / (anchor_widths + 1e-6)\n",
        "        dy = (gt_ctr_y - anchor_ctr_y) / (anchor_heights + 1e-6)\n",
        "        dw = torch.log(gt_widths / (anchor_widths + 1e-6))\n",
        "        dh = torch.log(gt_heights / (anchor_heights + 1e-6))\n",
        "\n",
        "        return torch.stack((dx, dy, dw, dh), dim=1)\n",
        "\n",
        "    def bbox_iou(self, box1, box2):\n",
        "        \"\"\"Compute IoU between two sets of boxes\"\"\"\n",
        "        area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
        "        area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
        "\n",
        "        lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "        rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "        union = area1[:, None] + area2 - inter\n",
        "        iou = inter / union\n",
        "        return iou\n",
        "\n",
        "    def focal_loss(self, inputs, targets, alpha=0.25, gamma=2.0):\n",
        "        \"\"\"Focal Loss implementation\"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "def test_model_corrected(model, test_loader, device='cuda', save_predictions=True):\n",
        "    \"\"\"DÃ¼zeltilmiÅŸ test fonksiyonu\"\"\"\n",
        "    model.eval()  # IMPORTANT: Model'i eval mode'a al\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, images in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # DÃœZELTME: mode=\"inference\" kullan\n",
        "            batch_results = model(images, mode=\"inference\")\n",
        "\n",
        "            for i, result in enumerate(batch_results):\n",
        "                image_result = {\n",
        "                    'image_idx': batch_idx * len(images) + i,\n",
        "                    'boxes': result['boxes'].cpu().numpy(),\n",
        "                    'scores': result['scores'].cpu().numpy(),\n",
        "                    'labels': result['labels'].cpu().numpy(),\n",
        "                    'depth': result['depth'] if result['depth'] is not None else None\n",
        "                }\n",
        "                results.append(image_result)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Processed {batch_idx}/{len(test_loader)} batches\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def visualize_single_prediction(image_path, prediction, show_depth=True, img_size=256):\n",
        "    \"\"\"\n",
        "    image_path: GÃ¶rsel dosya yolu\n",
        "    prediction: test_model_corrected Ã§Ä±ktÄ±sÄ±ndaki tek bir dict\n",
        "    img_size: GÃ¶rselin yeniden boyutlandÄ±rÄ±lacak boyutu (square)\n",
        "    \"\"\"\n",
        "    # GÃ¶rseli yÃ¼kle ve resize et\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    boxes = prediction[379]['boxes']\n",
        "    labels = prediction[379]['labels']\n",
        "    depth_values = prediction[379]['depth']\n",
        "\n",
        "    # EÄŸer boxes normalized ise img_size ile Ã§arp\n",
        "    if boxes.max() <= 1.0:\n",
        "        boxes = boxes * img_size\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "\n",
        "      x1, y1, x2, y2 = box\n",
        "      print(box)\n",
        "      x1 *= 256\n",
        "      x2 *= 256\n",
        "      y1 *= 256\n",
        "      y2 *= 256\n",
        "      rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                          fill=False, color='red', linewidth=2)\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "      label_str = kitti_class_dict[labels[i]]\n",
        "      text = f\"{label_str}\"\n",
        "      if show_depth and depth_values is not None:\n",
        "          text += f\", {depth_values[i]:.2f} metre\"\n",
        "      ax.text(x1, y1, text, color='yellow', fontsize=10)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "kitti_class_dict = {\n",
        "    0: \"Car\",\n",
        "    1: \"Van\",\n",
        "    2: \"Truck\",\n",
        "    3: \"Pedestrian\",\n",
        "    4: \"Person_sitting\",\n",
        "    5: \"Cyclist\",\n",
        "    6: \"Misc\"\n",
        "}"
      ],
      "metadata": {
        "id": "yWHY765bh6LH"
      },
      "id": "yWHY765bh6LH",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f04dbc5a",
      "metadata": {
        "id": "f04dbc5a"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/kitti2012\" #colab\n",
        "#data_path = \"C:/Users/Mehmet/Desktop/kitti2012\" #lcoal\n",
        "\n",
        "image_size=256\n",
        "batch_size=1\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_val_dataset = KITTI_Dataset(data_path=data_path ,transform=transform, mode='train')\n",
        "train_size = int(0.8 * len(train_val_dataset))  # ~155 sahne\n",
        "val_size = len(train_val_dataset) - train_size  # ~39 sahne\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset  = KITTI_Dataset(data_path,transform=transform,mode='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True, num_workers=0,collate_fn=test_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0,collate_fn=kitti_collate_fn)\n",
        "\n",
        "class_weights = torch.tensor([w / sum([1 / w for w in [1201,113,39,161,76,36,35]]) for w in [1 / w for w in [1201,113,39,161,76,36,35]]], dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rrO5j91dPxmQ",
      "metadata": {
        "collapsed": true,
        "id": "rrO5j91dPxmQ"
      },
      "outputs": [],
      "source": [
        "results = analyze_dataset(train_dataset,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDLujm0-svCb",
      "metadata": {
        "collapsed": true,
        "id": "TDLujm0-svCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "9cb0598e-d79f-46ff-eb68-973c0b58f16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-13 22:08:56,189] A new study created in memory with name: no-name-b2d5fc17-598c-40b9-9c81-4ec75abc669f\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmehmeteminuludag\u001b[0m (\u001b[33mmehmeteminuludag-kirikkale-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250813_220857-nd5v637k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k' target=\"_blank\">earthy-disco-342</a></strong> to <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi' target=\"_blank\">https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k' target=\"_blank\">https://wandb.ai/mehmeteminuludag-kirikkale-university/StajProjesi/runs/nd5v637k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/30 Train:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 122/310 [01:11<01:54,  1.64it/s, Acc=0.127, ClsLoss=0.127, F1=0.172, RMSE=0.159, mAP=0.339, TotalLoss=0.233]"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    task_weights = {\n",
        "          'classification': 1.0,\n",
        "          'regression': 2.0,\n",
        "          'depth': 0.3,\n",
        "          'depth_map': 0.3}\n",
        "\n",
        "    # IoU thresholds\n",
        "    p_iou_threshold = trial.suggest_categorical('p_iou_threshold', [0.35, 0.45])\n",
        "    n_iou_threshold = trial.suggest_categorical('n_iou_threshold', [0.2, 0.3])\n",
        "    model = CompleteMultiTaskModel(num_classes=7,max_detections=15, num_anchors=15,MAX_CANDIDATES=3000,confidence_threshold=0.25).to(device)\n",
        "    # Model oluÅŸtur ve BatchNorm ayarla\n",
        "\n",
        "    best_val_loss = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=30,  # Optuna iÃ§in kÄ±sa epoch\n",
        "        learning_rate=1e-3,\n",
        "        device=device,\n",
        "        save_path=f'weights_{trial.number}.pth',\n",
        "        class_weights=class_weights,\n",
        "        task_weights=task_weights,\n",
        "        scheduler_patience=3,\n",
        "        scheduler_factor=0.2,\n",
        "        p_iou_threshold=p_iou_threshold,\n",
        "        n_iou_threshold=n_iou_threshold,\n",
        "        early_stop_patience=5\n",
        "    )\n",
        "\n",
        "    return best_val_loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=2)\n",
        "\n",
        "print(\"En iyi parametreler:\", study.best_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PytTorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}